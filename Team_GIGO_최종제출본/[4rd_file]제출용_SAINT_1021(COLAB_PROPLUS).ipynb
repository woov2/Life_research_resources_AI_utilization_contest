{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHJQnh--dgT0"
   },
   "source": [
    "# 암환자 유전체 데이터 기반 암종 분류 AI 모델 개발\n",
    "\n",
    "\n",
    "- '2024 생명연구자원 AI활용 경진대회'는 바이오 데이터를 기반으로 한 AI 기술의 문제 해결 능력을 탐구하는 것을 목표로 합니다. <br>이 대회는 바이오 분야에서 AI 활용의 저변을 확대하고, 복잡한 바이오 데이터를 효율적으로 분석 및 해석할 수 있는 AI 알고리즘 개발에 초점을 맞추고 있습니다. <br><br>\n",
    "- 본 대회의 구체적인 과제는 암환자 유전체 데이터의 변이 정보를 활용하여 암종을 분류하는 AI 모델을 개발하는 것입니다. <br>참가자들은 제공된 학습 데이터셋(암환자 유전체 변이 정보)을 사용하여 특정 변이 정보를 바탕으로 암종을 정확하게 분류할 수 있는 AI 알고리즘을 개발해야 합니다. <br><br>\n",
    "- 이 대회의 궁극적인 목적은 바이오 데이터의 활용도를 높이고, 바이오 분야에서 AI 기술의 적용 가능성을 극대화하며, 인공지능 기술이 실제 바이오 의료 문제 해결에 어떻게 기여할 수 있는지 탐구하는 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M6Xs8OTCdgT1"
   },
   "source": [
    "# Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3821,
     "status": "ok",
     "timestamp": 1729569655543,
     "user": {
      "displayName": "장준보",
      "userId": "12796992458142872403"
     },
     "user_tz": -540
    },
    "id": "ifeDAkX1dieS",
    "outputId": "98048046-2db6-41e8-f7f2-7c572b6db050"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/', force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GimxHfWQhkzd"
   },
   "source": [
    "### -------------------------- Python & library version --------------------------\n",
    "### Python version: 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]\n",
    "### pandas version: 2.2.2\n",
    "### numpy version: 1.26.4\n",
    "### matplotlib version: 3.7.1\n",
    "### tqdm version: 4.66.5\n",
    "### scikit-learn version: 1.5.2\n",
    "### torch version: 2.4.1+cu121\n",
    "------------------------------------------------------------------------------\n",
    "\n",
    "USING Colab pro plus A100 Server(고용량 Ram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5013,
     "status": "ok",
     "timestamp": 1729569660554,
     "user": {
      "displayName": "장준보",
      "userId": "12796992458142872403"
     },
     "user_tz": -540
    },
    "id": "s-1-xSlYhJmO",
    "outputId": "4630e65e-9cb2-4bf9-9d0c-59d3e8306919"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------- Python & library version --------------------------\n",
      "Python version: 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]\n",
      "pandas version: 2.2.2\n",
      "numpy version: 1.26.4\n",
      "matplotlib version: 3.7.1\n",
      "tqdm version: 4.66.5\n",
      "scikit-learn version: 1.5.2\n",
      "torch version: 2.4.1+cu121\n",
      "------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import tqdm as tq\n",
    "import matplotlib\n",
    "import sklearn as skl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "print(\"-------------------------- Python & library version --------------------------\")\n",
    "print(\"Python version: {}\".format(sys.version))\n",
    "print(\"pandas version: {}\".format(pd.__version__))\n",
    "print(\"numpy version: {}\".format(np.__version__))\n",
    "print(\"matplotlib version: {}\".format(matplotlib.__version__))\n",
    "print(\"tqdm version: {}\".format(tq.__version__))\n",
    "print(\"scikit-learn version: {}\".format(skl.__version__))\n",
    "print(\"torch version: {}\".format(torch.__version__))\n",
    "\n",
    "print(\"------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 448,
     "status": "ok",
     "timestamp": 1729569661000,
     "user": {
      "displayName": "장준보",
      "userId": "12796992458142872403"
     },
     "user_tz": -540
    },
    "id": "s6zYrJITdgT2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm  # tqdm 임포트\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IxjSpnsUdgT2"
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1729569661000,
     "user": {
      "displayName": "장준보",
      "userId": "12796992458142872403"
     },
     "user_tz": -540
    },
    "id": "FCknUw5mdjwZ"
   },
   "outputs": [],
   "source": [
    "path = '/content/drive/MyDrive/유전체(과기부)/'\n",
    "import warnings\n",
    "\n",
    "# 경고 무시\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1729569661001,
     "user": {
      "displayName": "장준보",
      "userId": "12796992458142872403"
     },
     "user_tz": -540
    },
    "id": "CRLhqjopubBP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xpYX6GTTZOOc"
   },
   "source": [
    "### Starting with Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 35704,
     "status": "ok",
     "timestamp": 1729569696703,
     "user": {
      "displayName": "장준보",
      "userId": "12796992458142872403"
     },
     "user_tz": -540
    },
    "id": "LwveRVYIbL_l"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "train = pd.read_csv(path+'1019_final_tr_2.csv')\n",
    "test = pd.read_csv(path+'1019_final_te_2.csv')\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 3555,
     "status": "ok",
     "timestamp": 1729569700256,
     "user": {
      "displayName": "장준보",
      "userId": "12796992458142872403"
     },
     "user_tz": -540
    },
    "id": "ojf7gMgLsHlf"
   },
   "outputs": [],
   "source": [
    "train.loc[train['SUBCLASS']=='READ','SUBCLASS'] = np.nan\n",
    "train.loc[train['SUBCLASS']=='UVM','SUBCLASS'] = np.nan\n",
    "train.loc[train['SUBCLASS']=='UCS','SUBCLASS'] = np.nan\n",
    "train.loc[train['SUBCLASS']=='CHOL','SUBCLASS'] = np.nan\n",
    "train.loc[train['SUBCLASS']=='KIRP','SUBCLASS'] = 'KIPAN'\n",
    "train.loc[train['SUBCLASS']=='KICH','SUBCLASS'] = 'KIPAN'\n",
    "train.loc[train['SUBCLASS']=='KIRC','SUBCLASS'] = 'KIRC'\n",
    "\n",
    "train.loc[train['SUBCLASS']=='STAD','SUBCLASS'] = 'STES'\n",
    "train.loc[train['SUBCLASS']=='ESCA','SUBCLASS'] = 'STES'\n",
    "\n",
    "# train.loc[train['SUBCLASS']=='BLCA_2','SUBCLASS'] = 'BLCA'\n",
    "# train.loc[train['SUBCLASS']=='BRCA_2','SUBCLASS'] = np.nan\n",
    "\n",
    "# train.loc[train['SUBCLASS']=='CESC_2','SUBCLASS'] = np.nan\n",
    "# train.loc[train['SUBCLASS']=='COAD_2','SUBCLASS'] = np.nan\n",
    "# train.loc[train['SUBCLASS']=='COAD_3','SUBCLASS'] =np.nan\n",
    "\n",
    "# train.loc[train['SUBCLASS']=='HNSC_2','SUBCLASS'] = np.nan\n",
    "# train.loc[train['SUBCLASS']=='LAML_2','SUBCLASS'] = np.nan\n",
    "# train.loc[train['SUBCLASS']=='LIHC_2','SUBCLASS'] = np.nan\n",
    "\n",
    "# train.loc[train['SUBCLASS']=='OV_2','SUBCLASS'] = np.nan\n",
    "# train.loc[train['SUBCLASS']=='PRAD_2','SUBCLASS'] = np.nan\n",
    "# train.loc[train['SUBCLASS']=='SARC_2','SUBCLASS'] = np.nan\n",
    "\n",
    "# train.loc[train['SUBCLASS']=='THCA_2','SUBCLASS'] =np.nan\n",
    "# train.loc[train['SUBCLASS']=='UCEC_2','SUBCLASS'] =np.nan\n",
    "\n",
    "train = train.dropna(subset=['SUBCLASS'])\n",
    "\n",
    "train = train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 39086,
     "status": "ok",
     "timestamp": 1729569739340,
     "user": {
      "displayName": "장준보",
      "userId": "12796992458142872403"
     },
     "user_tz": -540
    },
    "id": "w7dBKH-Zp8Yc",
    "outputId": "fad45ef7-8579-44d9-e30f-ae9c7108de4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94\n"
     ]
    }
   ],
   "source": [
    "wt_indices = train.iloc[:, 2:].apply(lambda row: (row == 'WT').all(), axis=1)\n",
    "result_indices = train[wt_indices].index\n",
    "print(len(result_indices))\n",
    "train = train.drop(result_indices)\n",
    "train = train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 98682,
     "status": "ok",
     "timestamp": 1729569838020,
     "user": {
      "displayName": "장준보",
      "userId": "12796992458142872403"
     },
     "user_tz": -540
    },
    "id": "-YT7qDm3tRsY",
    "outputId": "51e777cf-f73f-4585-e140-f9bc70679b3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train TF-IDF shape: (39874, 28122)\n",
      "Test TF-IDF shape: (2546, 28122)\n",
      "원래 레이블: ACC, 변환된 숫자: 0\n",
      "원래 레이블: BLCA, 변환된 숫자: 1\n",
      "원래 레이블: BRCA, 변환된 숫자: 2\n",
      "원래 레이블: CESC, 변환된 숫자: 3\n",
      "원래 레이블: COAD, 변환된 숫자: 4\n",
      "원래 레이블: DLBC, 변환된 숫자: 5\n",
      "원래 레이블: GBMLGG, 변환된 숫자: 6\n",
      "원래 레이블: HNSC, 변환된 숫자: 7\n",
      "원래 레이블: KIPAN, 변환된 숫자: 8\n",
      "원래 레이블: KIRC, 변환된 숫자: 9\n",
      "원래 레이블: LAML, 변환된 숫자: 10\n",
      "원래 레이블: LGG, 변환된 숫자: 11\n",
      "원래 레이블: LIHC, 변환된 숫자: 12\n",
      "원래 레이블: LUAD, 변환된 숫자: 13\n",
      "원래 레이블: LUSC, 변환된 숫자: 14\n",
      "원래 레이블: OV, 변환된 숫자: 15\n",
      "원래 레이블: PAAD, 변환된 숫자: 16\n",
      "원래 레이블: PCPG, 변환된 숫자: 17\n",
      "원래 레이블: PRAD, 변환된 숫자: 18\n",
      "원래 레이블: SARC, 변환된 숫자: 19\n",
      "원래 레이블: SKCM, 변환된 숫자: 20\n",
      "원래 레이블: STES, 변환된 숫자: 21\n",
      "원래 레이블: TGCT, 변환된 숫자: 22\n",
      "원래 레이블: THCA, 변환된 숫자: 23\n",
      "원래 레이블: THYM, 변환된 숫자: 24\n",
      "원래 레이블: UCEC, 변환된 숫자: 25\n",
      "Train shape: (39874, 28122)\n",
      "Test shape: (2546, 28122)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 1. 아미노산 변환 사전 정의\n",
    "amino_acid_dict = {\n",
    "    'A': 'Alanine',\n",
    "    'R': 'Arginine',\n",
    "    'N': 'Asparagine',\n",
    "    'D': 'Aspartic acid',\n",
    "    'C': 'Cysteine',\n",
    "    'E': 'Glutamic acid',\n",
    "    'Q': 'Glutamine',\n",
    "    'G': 'Glycine',\n",
    "    'H': 'Histidine',\n",
    "    'I': 'Isoleucine',\n",
    "    'L': 'Leucine',\n",
    "    'K': 'Lysine',\n",
    "    'M': 'Methionine',\n",
    "    'F': 'Phenylalanine',\n",
    "    'P': 'Proline',\n",
    "    'S': 'Serine',\n",
    "    'T': 'Threonine',\n",
    "    'W': 'Tryptophan',\n",
    "    'Y': 'Tyrosine',\n",
    "    'V': 'Valine',\n",
    "    '*' :'Stop Codon',\n",
    "    'X' : 'UnKnown'\n",
    "}\n",
    "\n",
    "def translate_amino_acids(sequence, amino_acid_dict):\n",
    "    translated = [amino_acid_dict.get(aa, \"Unknown\") for aa in sequence]\n",
    "    return \", \".join(translated)\n",
    "\n",
    "# Function to interpret mutation notation\n",
    "def interpret_mutation_with_col_name(gene, mutation_code):\n",
    "    if mutation_code == \"WT\" or not isinstance(mutation_code, str) or len(mutation_code) < 2:\n",
    "        return None  # Skip wild-type entries, non-string entries, or too-short strings\n",
    "\n",
    "    # Split the mutation code by spaces to handle multiple mutations in one cell\n",
    "    mutations = mutation_code.split(' ')\n",
    "    descriptions = []\n",
    "\n",
    "    for mutation in mutations:\n",
    "        description = None  # Initialize description for each mutation\n",
    "        if '-' in mutation and 'fs' in mutation:\n",
    "            # Extract the position (e.g., -5026fs -> position 5026)\n",
    "            position = mutation.split('fs')[0]  # Split at 'fs' and take the first part\n",
    "            position = position.replace('-', '')  # Remove '-' to get the numeric position\n",
    "\n",
    "            # Create description for the frameshift mutation\n",
    "            description = f\" {gene},  frameshift  {position}\"\n",
    "\n",
    "        elif 'fs' in mutation:\n",
    "            # Handle frameshift mutations\n",
    "            if mutation[1].isalpha():  # 숫자가 아닐 때 (알파벳일 때)\n",
    "                original_aa_1 = mutation[0]\n",
    "                original_aa_1 = amino_acid_dict.get(original_aa_1, \"Unknown\")\n",
    "\n",
    "                original_aa_2 = mutation[1]\n",
    "                original_aa_2 = amino_acid_dict.get(original_aa_2, \"Unknown\")\n",
    "\n",
    "                position = mutation[2:-2]  # Exclude 'fs' from position\n",
    "                description = f\" {gene},  frameshift {original_aa_1}  {original_aa_2}  {position}\"\n",
    "            else:\n",
    "                original_aa = mutation[0]\n",
    "                original_aa = amino_acid_dict.get(original_aa, \"Unknown\")\n",
    "\n",
    "                position = mutation[1:-2]  # Exclude 'fs' from position\n",
    "                description = f\" {gene},  frameshift  {original_aa}  {position}\"\n",
    "\n",
    "        elif '*' in mutation:\n",
    "            if mutation[0]=='*' :\n",
    "                position = mutation[1:-1]\n",
    "                new_aa = mutation[-1]\n",
    "                new_aa = amino_acid_dict.get(new_aa, \"Unknown\")\n",
    "                description = f\"{gene}, stop codon {position} changes to {new_aa}\"\n",
    "\n",
    "            else:\n",
    "                # Handle stop codon mutations\n",
    "                original_aa = mutation[0]\n",
    "                original_aa = amino_acid_dict.get(original_aa, \"Unknown\")\n",
    "\n",
    "                position = mutation[1:-1]  # Position before the *\n",
    "                description = f\"{gene}, {original_aa} {position} changes to stop codon\"\n",
    "\n",
    "        elif 'delins' in mutation:\n",
    "            # Handle deletion-insertion mutations\n",
    "            match = re.match(r\"([A-Za-z]+)(\\d+)_([A-Za-z]+)(\\d+)delins([A-Za-z]+)\", mutation)\n",
    "            if match:\n",
    "                original_aa_start = match.group(1)\n",
    "                start_position = match.group(2)\n",
    "                original_aa_end = match.group(3)\n",
    "                end_position = match.group(4)\n",
    "                inserted_seq = match.group(5)\n",
    "\n",
    "                # Translate amino acids using dictionary\n",
    "                original_aa_start = amino_acid_dict.get(original_aa_start, \"Unknown\")\n",
    "                original_aa_end = amino_acid_dict.get(original_aa_end, \"Unknown\")\n",
    "                inserted_seq_translated = translate_amino_acids(inserted_seq, amino_acid_dict)\n",
    "\n",
    "                description = f\"{gene} {start_position} ({original_aa_start}) to {end_position} ({original_aa_end}), a del is followed by ins of {inserted_seq_translated}\"\n",
    "\n",
    "        elif 'ins' in mutation:\n",
    "            # Handle insertion mutations\n",
    "            match = re.match(r\"([A-Za-z]+)(\\d+)ins([A-Za-z]+)\", mutation)\n",
    "            if match:\n",
    "                original_aa = match.group(1)\n",
    "                position = match.group(2)\n",
    "                inserted_seq = match.group(3)\n",
    "\n",
    "                # Translate amino acids using dictionary\n",
    "                original_aa = amino_acid_dict.get(original_aa, \"Unknown\")\n",
    "                inserted_seq_translated = translate_amino_acids(inserted_seq, amino_acid_dict)\n",
    "\n",
    "                description = f\"{gene}, {inserted_seq_translated} ins {position} {original_aa}\"\n",
    "\n",
    "        elif 'del' in mutation:\n",
    "            # Handle deletion mutations (modified regex to capture both amino acids and positions)\n",
    "            match = re.match(r\"([A-Za-z]+)(\\d+)del\", mutation)\n",
    "            if match:\n",
    "                original_aa_start = match.group(1)\n",
    "                start_position = match.group(2)\n",
    "#                 original_aa_end = match.group(3)\n",
    "#                 end_position = match.group(4)\n",
    "\n",
    "                # Translate amino acids using dictionary\n",
    "                original_aa_start = amino_acid_dict.get(original_aa_start, \"Unknown\")\n",
    "#                 original_aa_end = amino_acid_dict.get(original_aa_end, \"Unknown\")\n",
    "\n",
    "                description = f\"{gene}, {start_position}, ({original_aa_start}) is del\"\n",
    "\n",
    "        elif 'dup' in mutation:\n",
    "            # Handle duplication mutations (modified regex to capture both amino acids and positions)\n",
    "            match = re.match(r\"([A-Za-z]+)(\\d+)dup\", mutation)\n",
    "            if match:\n",
    "                original_aa_start = match.group(1)\n",
    "                start_position = match.group(2)\n",
    "#                 original_aa_end = match.group(3)\n",
    "#                 end_position = match.group(4)\n",
    "\n",
    "                # Translate amino acids using dictionary\n",
    "                original_aa_start = amino_acid_dict.get(original_aa_start, \"Unknown\")\n",
    "#                 original_aa_end = amino_acid_dict.get(original_aa_end, \"Unknown\")\n",
    "\n",
    "                description = f\"{gene} {start_position} ({original_aa_start}) dup\"\n",
    "\n",
    "        elif len(mutation) >= 2 and mutation[0] == mutation[-1]:\n",
    "            # Handle mutations where the original and new amino acid are the same (e.g., S1866S)\n",
    "            original_aa = mutation[0]\n",
    "            original_aa = amino_acid_dict.get(original_aa, \"Unknown\")\n",
    "\n",
    "            position = mutation[1:-1]\n",
    "            description = f\"{gene}, {original_aa} {position} changes {original_aa}\"\n",
    "\n",
    "        elif len(mutation) >= 2 and mutation[0] != mutation[-1]:\n",
    "            # Handle mutations where the original and new amino acid are different\n",
    "            original_aa = mutation[0]\n",
    "            original_aa = amino_acid_dict.get(original_aa, \"Unknown\")\n",
    "\n",
    "            position = mutation[1:-1]\n",
    "            new_aa = mutation[-1]\n",
    "            new_aa = amino_acid_dict.get(new_aa, \"Unknown\")\n",
    "\n",
    "            description = f\" {gene}, {original_aa}  changes  {new_aa} {position}\"\n",
    "\n",
    "        else:\n",
    "            # Handle general mutations\n",
    "            match = re.match(r\"([A-Za-z]+)(\\d+)\", mutation)\n",
    "\n",
    "            if match:\n",
    "                original_aa = match.group(1)  # 'T'\n",
    "                position = match.group(2)     # '218'\n",
    "                original_aa = amino_acid_dict.get(original_aa, \"Unknown\")\n",
    "                description = f\"{gene}, {original_aa} is changes del or ins {position}\"\n",
    "\n",
    "        if description:\n",
    "            descriptions.append(description)\n",
    "\n",
    "    return \"; \".join(descriptions)\n",
    "\n",
    "# Train 데이터 돌연변이 설명 생성\n",
    "train['mutation_description'] = \"\"\n",
    "for index, row in train.iterrows():\n",
    "    mutation_descriptions = []\n",
    "    for gene, mutation in row.items():\n",
    "        if gene != 'SUBCLASS' and mutation != \"WT\" and gene != 'ID':\n",
    "            description = interpret_mutation_with_col_name(gene, mutation)\n",
    "            if description:\n",
    "                mutation_descriptions.append(description)\n",
    "    full_description = \"; \".join(mutation_descriptions)\n",
    "    train.at[index, 'mutation_description'] = full_description\n",
    "\n",
    "# Test 데이터 돌연변이 설명 생성\n",
    "test['mutation_description'] = \"\"\n",
    "for index, row in test.iterrows():\n",
    "    mutation_descriptions = []\n",
    "    for gene, mutation in row.items():\n",
    "        if gene != 'SUBCLASS' and mutation != \"WT\" and gene != 'ID':\n",
    "            description = interpret_mutation_with_col_name(gene, mutation)\n",
    "            if description:\n",
    "                mutation_descriptions.append(description)\n",
    "    full_description = \"; \".join(mutation_descriptions)\n",
    "    test.at[index, 'mutation_description'] = full_description\n",
    "\n",
    "df_test = test['mutation_description']\n",
    "df_train = train[[\"SUBCLASS\", \"mutation_description\"]]\n",
    "\n",
    "# TF-IDF Vectorizer 초기화 및 학습\n",
    "vectorizer = TfidfVectorizer(max_features=40000)\n",
    "\n",
    "# train 데이터를 이용해 TF-IDF 모델을 학습 (fit)\n",
    "vectorizer.fit(df_train['mutation_description'])\n",
    "\n",
    "# train 데이터를 변환 (transform)\n",
    "train_tfidf = vectorizer.transform(df_train['mutation_description'])\n",
    "\n",
    "# test 데이터를 변환 (transform)\n",
    "test_tfidf = vectorizer.transform(df_test)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"Train TF-IDF shape:\", train_tfidf.shape)\n",
    "print(\"Test TF-IDF shape:\", test_tfidf.shape)\n",
    "\n",
    "# 희소 행렬을 밀집 행렬로 변환\n",
    "train_data = pd.DataFrame(train_tfidf.toarray())\n",
    "test_data = pd.DataFrame(test_tfidf.toarray())\n",
    "\n",
    "# SUBCLASS 범주형 데이터를 숫자로 변환 (LabelEncoder 사용)\n",
    "train_data['SUBCLASS'] = df_train['SUBCLASS']\n",
    "le_subclass = LabelEncoder()\n",
    "train_data['SUBCLASS'] = le_subclass.fit_transform(train_data['SUBCLASS'])\n",
    "\n",
    "# 변환된 레이블 확인\n",
    "for i, label in enumerate(le_subclass.classes_):\n",
    "    print(f\"원래 레이블: {label}, 변환된 숫자: {i}\")\n",
    "\n",
    "# Feature와 Target 분리\n",
    "X = train_data.drop(columns=['SUBCLASS'])\n",
    "y = train_data['SUBCLASS']\n",
    "X_test = test_data\n",
    "\n",
    "# 데이터 준비 완료\n",
    "print(\"Train shape:\", X.shape)\n",
    "print(\"Test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 466,
     "status": "ok",
     "timestamp": 1729569838484,
     "user": {
      "displayName": "장준보",
      "userId": "12796992458142872403"
     },
     "user_tz": -540
    },
    "id": "Ziuig7c_NmqE"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "#DL 시드 고정\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # GPU 사용 시 추가\n",
    "    torch.backends.cudnn.deterministic = True  # Reproducibility를 위한 설정\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "# Convert data to PyTorch tensors\n",
    "\n",
    "X_train = torch.tensor(X.values, dtype=torch.float32)\n",
    "y_train = torch.tensor(y.values, dtype=torch.long)\n",
    "X_test = torch.tensor(X_test.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MwiC8GREvqxg",
    "outputId": "227be144-6bd2-4f5b-c586-39ad06535ea3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40, Train Loss: 2.8062, Val Loss: 2.1326, Val F1: 0.1696\n",
      "Epoch 2/40, Train Loss: 1.9528, Val Loss: 1.1650, Val F1: 0.4776\n",
      "Epoch 3/40, Train Loss: 1.0180, Val Loss: 0.6843, Val F1: 0.6144\n",
      "Epoch 4/40, Train Loss: 0.4324, Val Loss: 0.4075, Val F1: 0.7023\n",
      "Epoch 5/40, Train Loss: 0.1862, Val Loss: 0.3256, Val F1: 0.7418\n",
      "Epoch 6/40, Train Loss: 0.1041, Val Loss: 0.2786, Val F1: 0.7646\n",
      "Epoch 7/40, Train Loss: 0.0636, Val Loss: 0.2726, Val F1: 0.7775\n",
      "Epoch 8/40, Train Loss: 0.0425, Val Loss: 0.2591, Val F1: 0.7863\n",
      "Epoch 9/40, Train Loss: 0.0315, Val Loss: 0.2510, Val F1: 0.7934\n",
      "Epoch 10/40, Train Loss: 0.0236, Val Loss: 0.2479, Val F1: 0.7969\n",
      "Epoch 11/40, Train Loss: 0.0209, Val Loss: 0.2423, Val F1: 0.8017\n",
      "Epoch 12/40, Train Loss: 0.0161, Val Loss: 0.2460, Val F1: 0.8011\n",
      "Epoch 13/40, Train Loss: 0.0135, Val Loss: 0.2513, Val F1: 0.7992\n",
      "Epoch 14/40, Train Loss: 0.0119, Val Loss: 0.2549, Val F1: 0.7979\n",
      "Epoch 15/40, Train Loss: 0.0096, Val Loss: 0.2511, Val F1: 0.8022\n",
      "Epoch 16/40, Train Loss: 0.0088, Val Loss: 0.2578, Val F1: 0.8005\n",
      "Epoch 17/40, Train Loss: 0.0077, Val Loss: 0.2567, Val F1: 0.8008\n",
      "Epoch 18/40, Train Loss: 0.0071, Val Loss: 0.2613, Val F1: 0.7993\n",
      "Epoch 19/40, Train Loss: 0.0063, Val Loss: 0.2606, Val F1: 0.8010\n",
      "Epoch 20/40, Train Loss: 0.0060, Val Loss: 0.2620, Val F1: 0.8018\n",
      "Epoch 21/40, Train Loss: 0.0056, Val Loss: 0.2643, Val F1: 0.8013\n",
      "Epoch 22/40, Train Loss: 0.0055, Val Loss: 0.2662, Val F1: 0.8026\n",
      "Epoch 23/40, Train Loss: 0.0053, Val Loss: 0.2680, Val F1: 0.8002\n",
      "Epoch 24/40, Train Loss: 0.0052, Val Loss: 0.2661, Val F1: 0.8021\n",
      "Epoch 25/40, Train Loss: 0.0047, Val Loss: 0.2664, Val F1: 0.8032\n",
      "Epoch 26/40, Train Loss: 0.0046, Val Loss: 0.2677, Val F1: 0.8032\n",
      "Epoch 27/40, Train Loss: 0.0045, Val Loss: 0.2678, Val F1: 0.8036\n",
      "Epoch 28/40, Train Loss: 0.0044, Val Loss: 0.2700, Val F1: 0.8034\n",
      "Epoch 29/40, Train Loss: 0.0049, Val Loss: 0.2679, Val F1: 0.8019\n",
      "Epoch 30/40, Train Loss: 0.0041, Val Loss: 0.2707, Val F1: 0.8023\n",
      "Epoch 31/40, Train Loss: 0.0040, Val Loss: 0.2703, Val F1: 0.8028\n",
      "Epoch 32/40, Train Loss: 0.0040, Val Loss: 0.2695, Val F1: 0.8021\n",
      "Epoch 33/40, Train Loss: 0.0040, Val Loss: 0.2707, Val F1: 0.8021\n",
      "Epoch 34/40, Train Loss: 0.0040, Val Loss: 0.2704, Val F1: 0.8023\n",
      "Epoch 35/40, Train Loss: 0.0039, Val Loss: 0.2714, Val F1: 0.8034\n",
      "Epoch 36/40, Train Loss: 0.0037, Val Loss: 0.2716, Val F1: 0.8029\n",
      "Epoch 37/40, Train Loss: 0.0038, Val Loss: 0.2716, Val F1: 0.8034\n",
      "Epoch 38/40, Train Loss: 0.0040, Val Loss: 0.2721, Val F1: 0.8026\n",
      "Epoch 39/40, Train Loss: 0.0036, Val Loss: 0.2719, Val F1: 0.8036\n",
      "Epoch 40/40, Train Loss: 0.0036, Val Loss: 0.2717, Val F1: 0.8027\n",
      "Fold 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40, Train Loss: 2.8822, Val Loss: 2.1648, Val F1: 0.1292\n",
      "Epoch 2/40, Train Loss: 2.0372, Val Loss: 1.2113, Val F1: 0.4335\n",
      "Epoch 3/40, Train Loss: 1.0446, Val Loss: 0.6429, Val F1: 0.6151\n",
      "Epoch 4/40, Train Loss: 0.4352, Val Loss: 0.4130, Val F1: 0.7048\n",
      "Epoch 5/40, Train Loss: 0.1890, Val Loss: 0.3295, Val F1: 0.7320\n",
      "Epoch 6/40, Train Loss: 0.0980, Val Loss: 0.2912, Val F1: 0.7559\n",
      "Epoch 7/40, Train Loss: 0.0606, Val Loss: 0.2677, Val F1: 0.7732\n",
      "Epoch 8/40, Train Loss: 0.0426, Val Loss: 0.2613, Val F1: 0.7813\n",
      "Epoch 9/40, Train Loss: 0.0305, Val Loss: 0.2522, Val F1: 0.7854\n",
      "Epoch 10/40, Train Loss: 0.0231, Val Loss: 0.2444, Val F1: 0.7967\n",
      "Epoch 11/40, Train Loss: 0.0189, Val Loss: 0.2407, Val F1: 0.7969\n",
      "Epoch 12/40, Train Loss: 0.0150, Val Loss: 0.2438, Val F1: 0.8004\n",
      "Epoch 13/40, Train Loss: 0.0128, Val Loss: 0.2428, Val F1: 0.8011\n",
      "Epoch 14/40, Train Loss: 0.0111, Val Loss: 0.2458, Val F1: 0.8040\n",
      "Epoch 15/40, Train Loss: 0.0094, Val Loss: 0.2442, Val F1: 0.8039\n",
      "Epoch 16/40, Train Loss: 0.0083, Val Loss: 0.2478, Val F1: 0.8039\n",
      "Epoch 17/40, Train Loss: 0.0074, Val Loss: 0.2511, Val F1: 0.8038\n",
      "Epoch 18/40, Train Loss: 0.0064, Val Loss: 0.2516, Val F1: 0.8044\n",
      "Epoch 19/40, Train Loss: 0.0061, Val Loss: 0.2505, Val F1: 0.8045\n",
      "Epoch 20/40, Train Loss: 0.0060, Val Loss: 0.2533, Val F1: 0.8027\n",
      "Epoch 21/40, Train Loss: 0.0057, Val Loss: 0.2548, Val F1: 0.8029\n",
      "Epoch 22/40, Train Loss: 0.0053, Val Loss: 0.2571, Val F1: 0.8031\n",
      "Epoch 23/40, Train Loss: 0.0050, Val Loss: 0.2581, Val F1: 0.8035\n",
      "Epoch 24/40, Train Loss: 0.0046, Val Loss: 0.2575, Val F1: 0.8037\n",
      "Epoch 25/40, Train Loss: 0.0045, Val Loss: 0.2563, Val F1: 0.8043\n",
      "Epoch 26/40, Train Loss: 0.0043, Val Loss: 0.2578, Val F1: 0.8051\n",
      "Epoch 27/40, Train Loss: 0.0044, Val Loss: 0.2592, Val F1: 0.8034\n",
      "Epoch 28/40, Train Loss: 0.0045, Val Loss: 0.2587, Val F1: 0.8041\n",
      "Epoch 29/40, Train Loss: 0.0041, Val Loss: 0.2598, Val F1: 0.8041\n",
      "Epoch 30/40, Train Loss: 0.0039, Val Loss: 0.2606, Val F1: 0.8045\n",
      "Epoch 31/40, Train Loss: 0.0041, Val Loss: 0.2603, Val F1: 0.8040\n",
      "Epoch 32/40, Train Loss: 0.0038, Val Loss: 0.2603, Val F1: 0.8041\n",
      "Epoch 33/40, Train Loss: 0.0044, Val Loss: 0.2603, Val F1: 0.8054\n",
      "Epoch 34/40, Train Loss: 0.0038, Val Loss: 0.2608, Val F1: 0.8051\n",
      "Epoch 35/40, Train Loss: 0.0037, Val Loss: 0.2609, Val F1: 0.8056\n",
      "Epoch 36/40, Train Loss: 0.0038, Val Loss: 0.2609, Val F1: 0.8050\n",
      "Epoch 37/40, Train Loss: 0.0036, Val Loss: 0.2615, Val F1: 0.8053\n",
      "Epoch 38/40, Train Loss: 0.0036, Val Loss: 0.2622, Val F1: 0.8053\n",
      "Epoch 39/40, Train Loss: 0.0035, Val Loss: 0.2620, Val F1: 0.8056\n",
      "Epoch 40/40, Train Loss: 0.0037, Val Loss: 0.2619, Val F1: 0.8053\n",
      "Fold 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40, Train Loss: 2.8050, Val Loss: 2.0614, Val F1: 0.2335\n",
      "Epoch 2/40, Train Loss: 1.8943, Val Loss: 1.1436, Val F1: 0.4458\n",
      "Epoch 3/40, Train Loss: 0.9669, Val Loss: 0.6758, Val F1: 0.5904\n",
      "Epoch 4/40, Train Loss: 0.4243, Val Loss: 0.4533, Val F1: 0.6756\n",
      "Epoch 5/40, Train Loss: 0.1918, Val Loss: 0.3646, Val F1: 0.7238\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Custom Focal Loss implementation\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = nn.CrossEntropyLoss(reduction='mean')(inputs, targets)\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n",
    "        # F_loss = BCE_loss\n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean(F_loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            return torch.sum(F_loss)\n",
    "        else:\n",
    "            return F_loss\n",
    "\n",
    "# SAINT Model definition\n",
    "class SAINTModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, d_model=512, nhead=8, num_encoder_layers=2, dim_feedforward=2048, dropout=0.1):\n",
    "        super(SAINTModel, self).__init__()\n",
    "\n",
    "        # Feature-wise embedding\n",
    "        self.input_fc = nn.Linear(input_dim, d_model)\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # Transformer encoder for self-attention over features\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout)\n",
    "        self.feature_transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "\n",
    "        # Transformer encoder for intersample attention (between rows)\n",
    "        self.intersample_transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "\n",
    "        # Output layer for classification\n",
    "        self.output_fc = nn.Linear(d_model, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Feature embedding and normalization\n",
    "        x = self.input_fc(x)\n",
    "        x = self.layer_norm1(x)\n",
    "\n",
    "        # Apply self-attention over features (feature-wise)\n",
    "        x = x.unsqueeze(1)  # Add sequence dimension (batch, seq_len=1, d_model)\n",
    "        x = self.feature_transformer(x)\n",
    "        x = x.squeeze(1)  # Remove sequence dimension\n",
    "\n",
    "        # Apply intersample attention (row-wise)\n",
    "        x = x.unsqueeze(0)  # Add sequence dimension for intersample attention (seq_len=batch_size, batch=1, d_model)\n",
    "        x = self.intersample_transformer(x)\n",
    "        x = x.squeeze(0)  # Remove sequence dimension\n",
    "\n",
    "        # Output normalization and classification\n",
    "        x = self.layer_norm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.output_fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Training function\n",
    "def train_model(model, criterion, optimizer, scheduler, train_loader, val_loader, device, epochs=60):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        val_loss, val_f1 = evaluate_model(model, criterion, val_loader, device)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss:.4f}, Val F1: {val_f1:.4f}')\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, criterion, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    return val_loss, val_f1\n",
    "\n",
    "# Initialize model parameters\n",
    "d_model = 512\n",
    "nhead = 8\n",
    "num_encoder_layers = 4\n",
    "dim_feedforward = 2048\n",
    "dropout = 0.4\n",
    "learning_rate = 5e-5\n",
    "weight_decay = 1e-5\n",
    "\n",
    "# Initialize variables for k-fold cross-validation\n",
    "n_splits = 10\n",
    "# skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "input_dim = X_train.shape[1]\n",
    "num_classes = len(y_train.unique())\n",
    "\n",
    "oof_predictions = np.zeros((X_train.shape[0], num_classes))\n",
    "test_predictions = np.zeros((X_test.shape[0], num_classes))\n",
    "\n",
    "# Cross-validation loop\n",
    "for state in [41,43,1004]:\n",
    "  skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=state)\n",
    "\n",
    "  for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train)):\n",
    "      print(f'Fold {fold + 1}/{n_splits}')\n",
    "\n",
    "      X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]\n",
    "      y_fold_train, y_fold_val = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "      fold_train_dataset = TensorDataset(X_fold_train, y_fold_train)\n",
    "      fold_val_dataset = TensorDataset(X_fold_val, y_fold_val)\n",
    "\n",
    "      fold_train_loader = DataLoader(fold_train_dataset, batch_size=1024, shuffle=True)\n",
    "      fold_val_loader = DataLoader(fold_val_dataset, batch_size=1024, shuffle=False)\n",
    "\n",
    "      model = SAINTModel(input_dim=input_dim, num_classes=num_classes, d_model=d_model, nhead=nhead, num_encoder_layers=num_encoder_layers, dim_feedforward=dim_feedforward, dropout=dropout)\n",
    "      model = model.to(device)\n",
    "\n",
    "      optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "      scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "      criterion = FocalLoss(alpha=1, gamma=2)\n",
    "      # criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "      train_model(model, criterion, optimizer, scheduler, fold_train_loader, fold_val_loader, device, epochs=40)\n",
    "\n",
    "      # Make predictions for the validation set of this fold\n",
    "      model.eval()\n",
    "      with torch.no_grad():\n",
    "          val_outputs = []\n",
    "          for X_batch, _ in fold_val_loader:\n",
    "              X_batch = X_batch.to(device)\n",
    "              outputs = model(X_batch)\n",
    "              val_outputs.append(outputs.cpu().numpy())\n",
    "\n",
    "          oof_predictions[val_idx] = np.concatenate(val_outputs, axis=0)\n",
    "\n",
    "      test_fold_predictions = []\n",
    "      test_loader = DataLoader(TensorDataset(X_test), batch_size=1024, shuffle=False)\n",
    "      with torch.no_grad():\n",
    "          for X_batch in test_loader:\n",
    "              X_batch = X_batch[0].to(device)\n",
    "              outputs = model(X_batch)\n",
    "              test_fold_predictions.append(outputs.cpu().numpy())\n",
    "\n",
    "      test_predictions += np.concatenate(test_fold_predictions, axis=0) / n_splits\n",
    "\n",
    "# Calculate final OOF F1 score\n",
    "oof_pred_labels = np.argmax(oof_predictions, axis=1)\n",
    "oof_f1 = f1_score(y_train.numpy(), oof_pred_labels, average='weighted')\n",
    "\n",
    "print(f'Out-of-Fold F1 Score: {oof_f1:.4f}')\n",
    "\n",
    "# Convert test predictions to class labels\n",
    "test_pred_labels = np.argmax(test_predictions, axis=1)\n",
    "\n",
    "# Assuming le_subclass is the label encoder used for encoding the SUBCLASS labels\n",
    "predicted_labels = le_subclass.inverse_transform(test_pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AB5BZaXn651H"
   },
   "outputs": [],
   "source": [
    "# Prepare the submission file\n",
    "\n",
    "submission_dl = pd.read_csv(path+'sample_submission.csv')\n",
    "submission_dl['SUBCLASS'] = predicted_labels\n",
    "pd.DataFrame(test_predictions).to_csv(path + 'final_saint_proba.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dNUe1GqFHmVk"
   },
   "outputs": [],
   "source": [
    "## 여기까지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tgU1VurQHoQk"
   },
   "source": [
    "# 여기까지"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
