{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHJQnh--dgT0"
   },
   "source": [
    "# 암환자 유전체 데이터 기반 암종 분류 AI 모델 개발\n",
    "\n",
    "\n",
    "- '2024 생명연구자원 AI활용 경진대회'는 바이오 데이터를 기반으로 한 AI 기술의 문제 해결 능력을 탐구하는 것을 목표로 합니다. <br>이 대회는 바이오 분야에서 AI 활용의 저변을 확대하고, 복잡한 바이오 데이터를 효율적으로 분석 및 해석할 수 있는 AI 알고리즘 개발에 초점을 맞추고 있습니다. <br><br>\n",
    "- 본 대회의 구체적인 과제는 암환자 유전체 데이터의 변이 정보를 활용하여 암종을 분류하는 AI 모델을 개발하는 것입니다. <br>참가자들은 제공된 학습 데이터셋(암환자 유전체 변이 정보)을 사용하여 특정 변이 정보를 바탕으로 암종을 정확하게 분류할 수 있는 AI 알고리즘을 개발해야 합니다. <br><br>\n",
    "- 이 대회의 궁극적인 목적은 바이오 데이터의 활용도를 높이고, 바이오 분야에서 AI 기술의 적용 가능성을 극대화하며, 인공지능 기술이 실제 바이오 의료 문제 해결에 어떻게 기여할 수 있는지 탐구하는 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M6Xs8OTCdgT1"
   },
   "source": [
    "# Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ifeDAkX1dieS",
    "outputId": "0eef95f6-1abd-4c6c-90a6-dc4320cc2acc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/', force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GimxHfWQhkzd"
   },
   "source": [
    "### -------------------------- Python & library version --------------------------\n",
    "### Python version: 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]\n",
    "### pandas version: 2.2.2\n",
    "### numpy version: 1.26.4\n",
    "### matplotlib version: 3.7.1\n",
    "### tqdm version: 4.66.5\n",
    "### scikit-learn version: 1.5.2\n",
    "### torch version: 2.4.1+cu121\n",
    "------------------------------------------------------------------------------\n",
    "\n",
    "USING Colab pro plus A100 Server(고용량 Ram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s-1-xSlYhJmO",
    "outputId": "9bd7c9c7-4619-428e-934c-eee52a4df066"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------- Python & library version --------------------------\n",
      "Python version: 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]\n",
      "pandas version: 2.2.2\n",
      "numpy version: 1.26.4\n",
      "matplotlib version: 3.7.1\n",
      "tqdm version: 4.66.5\n",
      "scikit-learn version: 1.5.2\n",
      "torch version: 2.4.1+cu121\n",
      "------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import tqdm as tq\n",
    "import matplotlib\n",
    "import sklearn as skl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "print(\"-------------------------- Python & library version --------------------------\")\n",
    "print(\"Python version: {}\".format(sys.version))\n",
    "print(\"pandas version: {}\".format(pd.__version__))\n",
    "print(\"numpy version: {}\".format(np.__version__))\n",
    "print(\"matplotlib version: {}\".format(matplotlib.__version__))\n",
    "print(\"tqdm version: {}\".format(tq.__version__))\n",
    "print(\"scikit-learn version: {}\".format(skl.__version__))\n",
    "print(\"torch version: {}\".format(torch.__version__))\n",
    "\n",
    "print(\"------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "s6zYrJITdgT2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm  # tqdm 임포트\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IxjSpnsUdgT2"
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "FCknUw5mdjwZ"
   },
   "outputs": [],
   "source": [
    "path = '/content/drive/MyDrive/유전체(과기부)/'\n",
    "import warnings\n",
    "\n",
    "# 경고 무시\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xpYX6GTTZOOc"
   },
   "source": [
    "### Starting with Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "LwveRVYIbL_l"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "train = pd.read_csv(path+'1019_final_tr_2.csv')\n",
    "test = pd.read_csv(path+'1019_final_te_2.csv')\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ojf7gMgLsHlf"
   },
   "outputs": [],
   "source": [
    "train.loc[train['SUBCLASS']=='READ','SUBCLASS'] = np.nan\n",
    "train.loc[train['SUBCLASS']=='UVM','SUBCLASS'] = np.nan\n",
    "train.loc[train['SUBCLASS']=='UCS','SUBCLASS'] = np.nan\n",
    "train.loc[train['SUBCLASS']=='CHOL','SUBCLASS'] = np.nan\n",
    "train.loc[train['SUBCLASS']=='KIRP','SUBCLASS'] = 'KIPAN'\n",
    "train.loc[train['SUBCLASS']=='KICH','SUBCLASS'] = 'KIPAN'\n",
    "train.loc[train['SUBCLASS']=='KIRC','SUBCLASS'] = 'KIRC'\n",
    "\n",
    "train.loc[train['SUBCLASS']=='STAD','SUBCLASS'] = 'STES'\n",
    "train.loc[train['SUBCLASS']=='ESCA','SUBCLASS'] = 'STES'\n",
    "\n",
    "# train.loc[train['SUBCLASS']=='BLCA_2','SUBCLASS'] = 'BLCA'\n",
    "# train.loc[train['SUBCLASS']=='BRCA_2','SUBCLASS'] = np.nan\n",
    "\n",
    "# train.loc[train['SUBCLASS']=='CESC_2','SUBCLASS'] = np.nan\n",
    "# train.loc[train['SUBCLASS']=='COAD_2','SUBCLASS'] = np.nan\n",
    "# train.loc[train['SUBCLASS']=='COAD_3','SUBCLASS'] =np.nan\n",
    "\n",
    "# train.loc[train['SUBCLASS']=='HNSC_2','SUBCLASS'] = np.nan\n",
    "# train.loc[train['SUBCLASS']=='LAML_2','SUBCLASS'] = np.nan\n",
    "# train.loc[train['SUBCLASS']=='LIHC_2','SUBCLASS'] = np.nan\n",
    "\n",
    "# train.loc[train['SUBCLASS']=='OV_2','SUBCLASS'] = np.nan\n",
    "# train.loc[train['SUBCLASS']=='PRAD_2','SUBCLASS'] = np.nan\n",
    "# train.loc[train['SUBCLASS']=='SARC_2','SUBCLASS'] = np.nan\n",
    "\n",
    "# train.loc[train['SUBCLASS']=='THCA_2','SUBCLASS'] =np.nan\n",
    "# train.loc[train['SUBCLASS']=='UCEC_2','SUBCLASS'] =np.nan\n",
    "\n",
    "train = train.dropna(subset=['SUBCLASS'])\n",
    "\n",
    "train = train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w7dBKH-Zp8Yc",
    "outputId": "8156340b-6853-46ec-9ddb-4db98f9a4098"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94\n"
     ]
    }
   ],
   "source": [
    "wt_indices = train.iloc[:, 2:].apply(lambda row: (row == 'WT').all(), axis=1)\n",
    "result_indices = train[wt_indices].index\n",
    "print(len(result_indices))\n",
    "train = train.drop(result_indices)\n",
    "train = train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-YT7qDm3tRsY",
    "outputId": "c7ba67c5-e317-47c5-d6e4-355228f87ce0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train TF-IDF shape: (39874, 28122)\n",
      "Test TF-IDF shape: (2546, 28122)\n",
      "원래 레이블: ACC, 변환된 숫자: 0\n",
      "원래 레이블: BLCA, 변환된 숫자: 1\n",
      "원래 레이블: BRCA, 변환된 숫자: 2\n",
      "원래 레이블: CESC, 변환된 숫자: 3\n",
      "원래 레이블: COAD, 변환된 숫자: 4\n",
      "원래 레이블: DLBC, 변환된 숫자: 5\n",
      "원래 레이블: GBMLGG, 변환된 숫자: 6\n",
      "원래 레이블: HNSC, 변환된 숫자: 7\n",
      "원래 레이블: KIPAN, 변환된 숫자: 8\n",
      "원래 레이블: KIRC, 변환된 숫자: 9\n",
      "원래 레이블: LAML, 변환된 숫자: 10\n",
      "원래 레이블: LGG, 변환된 숫자: 11\n",
      "원래 레이블: LIHC, 변환된 숫자: 12\n",
      "원래 레이블: LUAD, 변환된 숫자: 13\n",
      "원래 레이블: LUSC, 변환된 숫자: 14\n",
      "원래 레이블: OV, 변환된 숫자: 15\n",
      "원래 레이블: PAAD, 변환된 숫자: 16\n",
      "원래 레이블: PCPG, 변환된 숫자: 17\n",
      "원래 레이블: PRAD, 변환된 숫자: 18\n",
      "원래 레이블: SARC, 변환된 숫자: 19\n",
      "원래 레이블: SKCM, 변환된 숫자: 20\n",
      "원래 레이블: STES, 변환된 숫자: 21\n",
      "원래 레이블: TGCT, 변환된 숫자: 22\n",
      "원래 레이블: THCA, 변환된 숫자: 23\n",
      "원래 레이블: THYM, 변환된 숫자: 24\n",
      "원래 레이블: UCEC, 변환된 숫자: 25\n",
      "Train shape: (39874, 28122)\n",
      "Test shape: (2546, 28122)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 1. 아미노산 변환 사전 정의\n",
    "amino_acid_dict = {\n",
    "    'A': 'Alanine',\n",
    "    'R': 'Arginine',\n",
    "    'N': 'Asparagine',\n",
    "    'D': 'Aspartic acid',\n",
    "    'C': 'Cysteine',\n",
    "    'E': 'Glutamic acid',\n",
    "    'Q': 'Glutamine',\n",
    "    'G': 'Glycine',\n",
    "    'H': 'Histidine',\n",
    "    'I': 'Isoleucine',\n",
    "    'L': 'Leucine',\n",
    "    'K': 'Lysine',\n",
    "    'M': 'Methionine',\n",
    "    'F': 'Phenylalanine',\n",
    "    'P': 'Proline',\n",
    "    'S': 'Serine',\n",
    "    'T': 'Threonine',\n",
    "    'W': 'Tryptophan',\n",
    "    'Y': 'Tyrosine',\n",
    "    'V': 'Valine',\n",
    "    '*' :'Stop Codon',\n",
    "    'X' : 'UnKnown'\n",
    "}\n",
    "\n",
    "def translate_amino_acids(sequence, amino_acid_dict):\n",
    "    translated = [amino_acid_dict.get(aa, \"Unknown\") for aa in sequence]\n",
    "    return \", \".join(translated)\n",
    "\n",
    "# Function to interpret mutation notation\n",
    "def interpret_mutation_with_col_name(gene, mutation_code):\n",
    "    if mutation_code == \"WT\" or not isinstance(mutation_code, str) or len(mutation_code) < 2:\n",
    "        return None  # Skip wild-type entries, non-string entries, or too-short strings\n",
    "\n",
    "    # Split the mutation code by spaces to handle multiple mutations in one cell\n",
    "    mutations = mutation_code.split(' ')\n",
    "    descriptions = []\n",
    "\n",
    "    for mutation in mutations:\n",
    "        description = None  # Initialize description for each mutation\n",
    "        if '-' in mutation and 'fs' in mutation:\n",
    "            # Extract the position (e.g., -5026fs -> position 5026)\n",
    "            position = mutation.split('fs')[0]  # Split at 'fs' and take the first part\n",
    "            position = position.replace('-', '')  # Remove '-' to get the numeric position\n",
    "\n",
    "            # Create description for the frameshift mutation\n",
    "            description = f\" {gene},  frameshift  {position}\"\n",
    "\n",
    "        elif 'fs' in mutation:\n",
    "            # Handle frameshift mutations\n",
    "            if mutation[1].isalpha():  # 숫자가 아닐 때 (알파벳일 때)\n",
    "                original_aa_1 = mutation[0]\n",
    "                original_aa_1 = amino_acid_dict.get(original_aa_1, \"Unknown\")\n",
    "\n",
    "                original_aa_2 = mutation[1]\n",
    "                original_aa_2 = amino_acid_dict.get(original_aa_2, \"Unknown\")\n",
    "\n",
    "                position = mutation[2:-2]  # Exclude 'fs' from position\n",
    "                description = f\" {gene},  frameshift {original_aa_1}  {original_aa_2}  {position}\"\n",
    "            else:\n",
    "                original_aa = mutation[0]\n",
    "                original_aa = amino_acid_dict.get(original_aa, \"Unknown\")\n",
    "\n",
    "                position = mutation[1:-2]  # Exclude 'fs' from position\n",
    "                description = f\" {gene},  frameshift  {original_aa}  {position}\"\n",
    "\n",
    "        elif '*' in mutation:\n",
    "            if mutation[0]=='*' :\n",
    "                position = mutation[1:-1]\n",
    "                new_aa = mutation[-1]\n",
    "                new_aa = amino_acid_dict.get(new_aa, \"Unknown\")\n",
    "                description = f\"{gene}, stop codon {position} changes to {new_aa}\"\n",
    "\n",
    "            else:\n",
    "                # Handle stop codon mutations\n",
    "                original_aa = mutation[0]\n",
    "                original_aa = amino_acid_dict.get(original_aa, \"Unknown\")\n",
    "\n",
    "                position = mutation[1:-1]  # Position before the *\n",
    "                description = f\"{gene}, {original_aa} {position} changes to stop codon\"\n",
    "\n",
    "        elif 'delins' in mutation:\n",
    "            # Handle deletion-insertion mutations\n",
    "            match = re.match(r\"([A-Za-z]+)(\\d+)_([A-Za-z]+)(\\d+)delins([A-Za-z]+)\", mutation)\n",
    "            if match:\n",
    "                original_aa_start = match.group(1)\n",
    "                start_position = match.group(2)\n",
    "                original_aa_end = match.group(3)\n",
    "                end_position = match.group(4)\n",
    "                inserted_seq = match.group(5)\n",
    "\n",
    "                # Translate amino acids using dictionary\n",
    "                original_aa_start = amino_acid_dict.get(original_aa_start, \"Unknown\")\n",
    "                original_aa_end = amino_acid_dict.get(original_aa_end, \"Unknown\")\n",
    "                inserted_seq_translated = translate_amino_acids(inserted_seq, amino_acid_dict)\n",
    "\n",
    "                description = f\"{gene} {start_position} ({original_aa_start}) to {end_position} ({original_aa_end}), a del is followed by ins of {inserted_seq_translated}\"\n",
    "\n",
    "        elif 'ins' in mutation:\n",
    "            # Handle insertion mutations\n",
    "            match = re.match(r\"([A-Za-z]+)(\\d+)ins([A-Za-z]+)\", mutation)\n",
    "            if match:\n",
    "                original_aa = match.group(1)\n",
    "                position = match.group(2)\n",
    "                inserted_seq = match.group(3)\n",
    "\n",
    "                # Translate amino acids using dictionary\n",
    "                original_aa = amino_acid_dict.get(original_aa, \"Unknown\")\n",
    "                inserted_seq_translated = translate_amino_acids(inserted_seq, amino_acid_dict)\n",
    "\n",
    "                description = f\"{gene}, {inserted_seq_translated} ins {position} {original_aa}\"\n",
    "\n",
    "        elif 'del' in mutation:\n",
    "            # Handle deletion mutations (modified regex to capture both amino acids and positions)\n",
    "            match = re.match(r\"([A-Za-z]+)(\\d+)del\", mutation)\n",
    "            if match:\n",
    "                original_aa_start = match.group(1)\n",
    "                start_position = match.group(2)\n",
    "#                 original_aa_end = match.group(3)\n",
    "#                 end_position = match.group(4)\n",
    "\n",
    "                # Translate amino acids using dictionary\n",
    "                original_aa_start = amino_acid_dict.get(original_aa_start, \"Unknown\")\n",
    "#                 original_aa_end = amino_acid_dict.get(original_aa_end, \"Unknown\")\n",
    "\n",
    "                description = f\"{gene}, {start_position}, ({original_aa_start}) is del\"\n",
    "\n",
    "        elif 'dup' in mutation:\n",
    "            # Handle duplication mutations (modified regex to capture both amino acids and positions)\n",
    "            match = re.match(r\"([A-Za-z]+)(\\d+)dup\", mutation)\n",
    "            if match:\n",
    "                original_aa_start = match.group(1)\n",
    "                start_position = match.group(2)\n",
    "#                 original_aa_end = match.group(3)\n",
    "#                 end_position = match.group(4)\n",
    "\n",
    "                # Translate amino acids using dictionary\n",
    "                original_aa_start = amino_acid_dict.get(original_aa_start, \"Unknown\")\n",
    "#                 original_aa_end = amino_acid_dict.get(original_aa_end, \"Unknown\")\n",
    "\n",
    "                description = f\"{gene} {start_position} ({original_aa_start}) dup\"\n",
    "\n",
    "        elif len(mutation) >= 2 and mutation[0] == mutation[-1]:\n",
    "            # Handle mutations where the original and new amino acid are the same (e.g., S1866S)\n",
    "            original_aa = mutation[0]\n",
    "            original_aa = amino_acid_dict.get(original_aa, \"Unknown\")\n",
    "\n",
    "            position = mutation[1:-1]\n",
    "            description = f\"{gene}, {original_aa} {position} changes {original_aa}\"\n",
    "\n",
    "        elif len(mutation) >= 2 and mutation[0] != mutation[-1]:\n",
    "            # Handle mutations where the original and new amino acid are different\n",
    "            original_aa = mutation[0]\n",
    "            original_aa = amino_acid_dict.get(original_aa, \"Unknown\")\n",
    "\n",
    "            position = mutation[1:-1]\n",
    "            new_aa = mutation[-1]\n",
    "            new_aa = amino_acid_dict.get(new_aa, \"Unknown\")\n",
    "\n",
    "            description = f\" {gene}, {original_aa}  changes  {new_aa} {position}\"\n",
    "\n",
    "        else:\n",
    "            # Handle general mutations\n",
    "            match = re.match(r\"([A-Za-z]+)(\\d+)\", mutation)\n",
    "\n",
    "            if match:\n",
    "                original_aa = match.group(1)  # 'T'\n",
    "                position = match.group(2)     # '218'\n",
    "                original_aa = amino_acid_dict.get(original_aa, \"Unknown\")\n",
    "                description = f\"{gene}, {original_aa} is changes del or ins {position}\"\n",
    "\n",
    "        if description:\n",
    "            descriptions.append(description)\n",
    "\n",
    "    return \"; \".join(descriptions)\n",
    "\n",
    "# Train 데이터 돌연변이 설명 생성\n",
    "train['mutation_description'] = \"\"\n",
    "for index, row in train.iterrows():\n",
    "    mutation_descriptions = []\n",
    "    for gene, mutation in row.items():\n",
    "        if gene != 'SUBCLASS' and mutation != \"WT\" and gene != 'ID':\n",
    "            description = interpret_mutation_with_col_name(gene, mutation)\n",
    "            if description:\n",
    "                mutation_descriptions.append(description)\n",
    "    full_description = \"; \".join(mutation_descriptions)\n",
    "    train.at[index, 'mutation_description'] = full_description\n",
    "\n",
    "# Test 데이터 돌연변이 설명 생성\n",
    "test['mutation_description'] = \"\"\n",
    "for index, row in test.iterrows():\n",
    "    mutation_descriptions = []\n",
    "    for gene, mutation in row.items():\n",
    "        if gene != 'SUBCLASS' and mutation != \"WT\" and gene != 'ID':\n",
    "            description = interpret_mutation_with_col_name(gene, mutation)\n",
    "            if description:\n",
    "                mutation_descriptions.append(description)\n",
    "    full_description = \"; \".join(mutation_descriptions)\n",
    "    test.at[index, 'mutation_description'] = full_description\n",
    "\n",
    "df_test = test['mutation_description']\n",
    "df_train = train[[\"SUBCLASS\", \"mutation_description\"]]\n",
    "\n",
    "# TF-IDF Vectorizer 초기화 및 학습\n",
    "vectorizer = TfidfVectorizer(max_features=40000)\n",
    "\n",
    "# train 데이터를 이용해 TF-IDF 모델을 학습 (fit)\n",
    "vectorizer.fit(df_train['mutation_description'])\n",
    "\n",
    "# train 데이터를 변환 (transform)\n",
    "train_tfidf = vectorizer.transform(df_train['mutation_description'])\n",
    "\n",
    "# test 데이터를 변환 (transform)\n",
    "test_tfidf = vectorizer.transform(df_test)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"Train TF-IDF shape:\", train_tfidf.shape)\n",
    "print(\"Test TF-IDF shape:\", test_tfidf.shape)\n",
    "\n",
    "# 희소 행렬을 밀집 행렬로 변환\n",
    "train_data = pd.DataFrame(train_tfidf.toarray())\n",
    "test_data = pd.DataFrame(test_tfidf.toarray())\n",
    "\n",
    "# SUBCLASS 범주형 데이터를 숫자로 변환 (LabelEncoder 사용)\n",
    "train_data['SUBCLASS'] = df_train['SUBCLASS']\n",
    "le_subclass = LabelEncoder()\n",
    "train_data['SUBCLASS'] = le_subclass.fit_transform(train_data['SUBCLASS'])\n",
    "\n",
    "# 변환된 레이블 확인\n",
    "for i, label in enumerate(le_subclass.classes_):\n",
    "    print(f\"원래 레이블: {label}, 변환된 숫자: {i}\")\n",
    "\n",
    "# Feature와 Target 분리\n",
    "X = train_data.drop(columns=['SUBCLASS'])\n",
    "y = train_data['SUBCLASS']\n",
    "X_test = test_data\n",
    "\n",
    "# 데이터 준비 완료\n",
    "print(\"Train shape:\", X.shape)\n",
    "print(\"Test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "Ziuig7c_NmqE"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "#DL 시드 고정\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # GPU 사용 시 추가\n",
    "    torch.backends.cudnn.deterministic = True  # Reproducibility를 위한 설정\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "# Convert data to PyTorch tensors\n",
    "\n",
    "X_train = torch.tensor(X.values, dtype=torch.float32)\n",
    "y_train = torch.tensor(y.values, dtype=torch.long)\n",
    "X_test = torch.tensor(X_test.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 881
    },
    "id": "MwiC8GREvqxg",
    "outputId": "c0040824-d554-4f83-90ef-29ef7a4000db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40, Train Loss: 2.6165, Val Loss: 1.7386, Val F1: 0.3360\n",
      "Epoch 2/40, Train Loss: 1.2813, Val Loss: 0.8985, Val F1: 0.5734\n",
      "Epoch 3/40, Train Loss: 0.4804, Val Loss: 0.5581, Val F1: 0.6742\n",
      "Epoch 4/40, Train Loss: 0.1898, Val Loss: 0.4208, Val F1: 0.7213\n",
      "Epoch 5/40, Train Loss: 0.0959, Val Loss: 0.3521, Val F1: 0.7500\n",
      "Epoch 6/40, Train Loss: 0.0598, Val Loss: 0.3152, Val F1: 0.7639\n",
      "Epoch 7/40, Train Loss: 0.0412, Val Loss: 0.2983, Val F1: 0.7655\n",
      "Epoch 8/40, Train Loss: 0.0312, Val Loss: 0.2836, Val F1: 0.7711\n",
      "Epoch 9/40, Train Loss: 0.0227, Val Loss: 0.2778, Val F1: 0.7795\n",
      "Epoch 10/40, Train Loss: 0.0183, Val Loss: 0.2702, Val F1: 0.7854\n",
      "Epoch 11/40, Train Loss: 0.0144, Val Loss: 0.2664, Val F1: 0.7862\n",
      "Epoch 12/40, Train Loss: 0.0122, Val Loss: 0.2630, Val F1: 0.7860\n",
      "Epoch 13/40, Train Loss: 0.0104, Val Loss: 0.2606, Val F1: 0.7874\n",
      "Epoch 14/40, Train Loss: 0.0090, Val Loss: 0.2592, Val F1: 0.7875\n",
      "Epoch 15/40, Train Loss: 0.0079, Val Loss: 0.2594, Val F1: 0.7897\n",
      "Epoch 16/40, Train Loss: 0.0067, Val Loss: 0.2596, Val F1: 0.7892\n",
      "Epoch 17/40, Train Loss: 0.0060, Val Loss: 0.2588, Val F1: 0.7892\n",
      "Epoch 18/40, Train Loss: 0.0057, Val Loss: 0.2598, Val F1: 0.7903\n",
      "Epoch 19/40, Train Loss: 0.0056, Val Loss: 0.2592, Val F1: 0.7917\n",
      "Epoch 20/40, Train Loss: 0.0052, Val Loss: 0.2642, Val F1: 0.7915\n",
      "Epoch 21/40, Train Loss: 0.0043, Val Loss: 0.2650, Val F1: 0.7923\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-f259e192ce62>\u001b[0m in \u001b[0;36m<cell line: 129>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    150\u001b[0m       \u001b[0;31m# criterion = nn.BCEWithLogitsLoss()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m       \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold_train_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold_val_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m       \u001b[0;31m# Make predictions for the validation set of this fold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-f259e192ce62>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, train_loader, val_loader, device, epochs)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    519\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m             )\n\u001b[0;32m--> 521\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 769\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    770\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Custom Focal Loss implementation\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = nn.CrossEntropyLoss(reduction='mean')(inputs, targets)\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n",
    "        # F_loss = BCE_loss\n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean(F_loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            return torch.sum(F_loss)\n",
    "        else:\n",
    "            return F_loss\n",
    "\n",
    "# X-Transformer Model definition\n",
    "class XTransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, d_model=512, nhead=8, num_encoder_layers=2, dim_feedforward=2048, dropout=0.1):\n",
    "        super(XTransformerModel, self).__init__()\n",
    "\n",
    "        # Embedding for Categorical and Numerical Features\n",
    "        self.input_fc = nn.Linear(input_dim, d_model)\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # Transformer Encoder for feature interactions\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "\n",
    "        # Final output classification layer\n",
    "        self.output_fc = nn.Linear(d_model, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Feature embedding transformation\n",
    "        x = self.input_fc(x)\n",
    "        x = self.layer_norm1(x)\n",
    "\n",
    "        # Apply transformer encoder for feature-wise attention\n",
    "        x = x.unsqueeze(1)  # Add sequence dimension\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.squeeze(1)  # Remove sequence dimension\n",
    "\n",
    "        # Output normalization and classification\n",
    "        x = self.layer_norm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.output_fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Training function\n",
    "def train_model(model, criterion, optimizer, scheduler, train_loader, val_loader, device, epochs=60):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        val_loss, val_f1 = evaluate_model(model, criterion, val_loader, device)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss:.4f}, Val F1: {val_f1:.4f}')\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, criterion, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    return val_loss, val_f1\n",
    "\n",
    "# Initialize model parameters\n",
    "d_model = 512\n",
    "nhead = 8\n",
    "num_encoder_layers = 4\n",
    "dim_feedforward = 2048\n",
    "dropout = 0.4\n",
    "learning_rate = 5e-5\n",
    "weight_decay = 1e-5\n",
    "\n",
    "# Initialize variables for k-fold cross-validation\n",
    "n_splits = 10\n",
    "# skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "input_dim = X_train.shape[1]\n",
    "num_classes = len(y_train.unique())\n",
    "\n",
    "oof_predictions = np.zeros((X_train.shape[0], num_classes))\n",
    "test_predictions = np.zeros((X_test.shape[0], num_classes))\n",
    "\n",
    "# Cross-validation loop\n",
    "for state in [0,42,100]:\n",
    "  skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=state)\n",
    "\n",
    "  for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train)):\n",
    "      print(f'Fold {fold + 1}/{n_splits}')\n",
    "\n",
    "      X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]\n",
    "      y_fold_train, y_fold_val = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "      fold_train_dataset = TensorDataset(X_fold_train, y_fold_train)\n",
    "      fold_val_dataset = TensorDataset(X_fold_val, y_fold_val)\n",
    "\n",
    "      fold_train_loader = DataLoader(fold_train_dataset, batch_size=1024, shuffle=True)\n",
    "      fold_val_loader = DataLoader(fold_val_dataset, batch_size=1024, shuffle=False)\n",
    "\n",
    "      model = XTransformerModel(input_dim=input_dim, num_classes=num_classes, d_model=d_model, nhead=nhead, num_encoder_layers=num_encoder_layers, dim_feedforward=dim_feedforward, dropout=dropout)\n",
    "      model = model.to(device)\n",
    "\n",
    "      optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "      scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "      criterion = FocalLoss(alpha=1, gamma=2)\n",
    "      # criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "      train_model(model, criterion, optimizer, scheduler, fold_train_loader, fold_val_loader, device, epochs=40)\n",
    "\n",
    "      # Make predictions for the validation set of this fold\n",
    "      model.eval()\n",
    "      with torch.no_grad():\n",
    "          val_outputs = []\n",
    "          for X_batch, _ in fold_val_loader:\n",
    "              X_batch = X_batch.to(device)\n",
    "              outputs = model(X_batch)\n",
    "              val_outputs.append(outputs.cpu().numpy())\n",
    "\n",
    "          oof_predictions[val_idx] = np.concatenate(val_outputs, axis=0)\n",
    "\n",
    "      test_fold_predictions = []\n",
    "      test_loader = DataLoader(TensorDataset(X_test), batch_size=1024, shuffle=False)\n",
    "      with torch.no_grad():\n",
    "          for X_batch in test_loader:\n",
    "              X_batch = X_batch[0].to(device)\n",
    "              outputs = model(X_batch)\n",
    "              test_fold_predictions.append(outputs.cpu().numpy())\n",
    "\n",
    "      test_predictions += np.concatenate(test_fold_predictions, axis=0) / n_splits\n",
    "\n",
    "# Calculate final OOF F1 score\n",
    "oof_pred_labels = np.argmax(oof_predictions, axis=1)\n",
    "oof_f1 = f1_score(y_train.numpy(), oof_pred_labels, average='weighted')\n",
    "\n",
    "print(f'Out-of-Fold F1 Score: {oof_f1:.4f}')\n",
    "\n",
    "# Convert test predictions to class labels\n",
    "test_pred_labels = np.argmax(test_predictions, axis=1)\n",
    "\n",
    "# Assuming le_subclass is the label encoder used for encoding the SUBCLASS labels\n",
    "predicted_labels = le_subclass.inverse_transform(test_pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 931
    },
    "id": "AB5BZaXn651H",
    "outputId": "a1e6820f-74a7-4206-ff39-89726d89be96"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SUBCLASS</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>COAD</th>\n",
       "      <td>540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STES</th>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BRCA</th>\n",
       "      <td>229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KIRC</th>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KIPAN</th>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRAD</th>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LUAD</th>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SARC</th>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>THCA</th>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GBMLGG</th>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SKCM</th>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGG</th>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LIHC</th>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HNSC</th>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OV</th>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UCEC</th>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CESC</th>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAAD</th>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BLCA</th>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TGCT</th>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LAML</th>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCPG</th>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACC</th>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>THYM</th>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LUSC</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DLBC</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div><br><label><b>dtype:</b> int64</label>"
      ],
      "text/plain": [
       "SUBCLASS\n",
       "COAD      540\n",
       "STES      232\n",
       "BRCA      229\n",
       "KIRC      188\n",
       "KIPAN     131\n",
       "PRAD      120\n",
       "LUAD      114\n",
       "SARC       93\n",
       "THCA       88\n",
       "GBMLGG     88\n",
       "SKCM       85\n",
       "LGG        80\n",
       "LIHC       77\n",
       "HNSC       75\n",
       "OV         63\n",
       "UCEC       55\n",
       "CESC       41\n",
       "PAAD       39\n",
       "BLCA       39\n",
       "TGCT       38\n",
       "LAML       35\n",
       "PCPG       34\n",
       "ACC        21\n",
       "THYM       17\n",
       "LUSC       15\n",
       "DLBC        9\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare the submission file\n",
    "\n",
    "submission_dl = pd.read_csv(path+'sample_submission.csv')\n",
    "submission_dl['SUBCLASS'] = predicted_labels\n",
    "pd.DataFrame(test_predictions).to_csv(path + 'final_xtf_proba.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dNUe1GqFHmVk"
   },
   "outputs": [],
   "source": [
    "## 여기까지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tgU1VurQHoQk"
   },
   "source": [
    "# 여기까지"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
