{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tm9KV33HZJyL"
   },
   "source": [
    "# 암환자 유전체 데이터 기반 암종 분류 AI 모델 개발\n",
    "\n",
    "\n",
    "- '2024 생명연구자원 AI활용 경진대회'는 바이오 데이터를 기반으로 한 AI 기술의 문제 해결 능력을 탐구하는 것을 목표로 합니다. <br>이 대회는 바이오 분야에서 AI 활용의 저변을 확대하고, 복잡한 바이오 데이터를 효율적으로 분석 및 해석할 수 있는 AI 알고리즘 개발에 초점을 맞추고 있습니다. <br><br>\n",
    "- 본 대회의 구체적인 과제는 암환자 유전체 데이터의 변이 정보를 활용하여 암종을 분류하는 AI 모델을 개발하는 것입니다. <br>참가자들은 제공된 학습 데이터셋(암환자 유전체 변이 정보)을 사용하여 특정 변이 정보를 바탕으로 암종을 정확하게 분류할 수 있는 AI 알고리즘을 개발해야 합니다. <br><br>\n",
    "- 이 대회의 궁극적인 목적은 바이오 데이터의 활용도를 높이고, 바이오 분야에서 AI 기술의 적용 가능성을 극대화하며, 인공지능 기술이 실제 바이오 의료 문제 해결에 어떻게 기여할 수 있는지 탐구하는 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oA0H0as2ZJyN"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VERSIONS #\n",
    "### GPU 0 : NVIDIA GeForce RTX 3080 Ti Laptop GPU\n",
    "### GPU 1 : AMD Radeon(TM) Graphics\n",
    "### CPU : AMD Ryzen 9 6900HX with Radeon Graphics\n",
    "\n",
    "### -------------------------- Python & library version --------------------------\n",
    "### Python version: 3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]\n",
    "### pandas version: 2.0.3\n",
    "### numpy version: 1.21.5\n",
    "### matplotlib version: 3.5.2\n",
    "### tqdm version: 4.64.1\n",
    "### xgboost version: 1.7.2\n",
    "### lightgbm version: 3.3.3\n",
    "### catboost version: 1.1.1\n",
    "### scikit-learn version: 1.0.2\n",
    "------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "4cG6WDWfZJyN"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import sklearn  # scikit-learn 임포트\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm  # tqdm 임포트\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# PerformanceWarning을 무시하도록 설정\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1UhTBst4ZJyO"
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "n-PXYwGHtQQ8"
   },
   "outputs": [],
   "source": [
    "# KIPAN, STES 빼고 24개 증강"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "Q-D5WiF7ZJyO"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def convert_format(data):\n",
    "    # 패턴 수정: 마지막 그룹에서 변이 문자를 처리하기 위해 \\w* 사용\n",
    "    pattern = r\"(\\d+)_(\\d+)(\\w+)\\>(\\w*)\"\n",
    "    \n",
    "    # 결과를 저장할 리스트\n",
    "    result = []\n",
    "    \n",
    "    # 데이터를 공백 기준으로 분리 (변이가 여러 개일 수 있으므로)\n",
    "    parts = data.split()\n",
    "\n",
    "    for part in parts:\n",
    "        # 정규 표현식으로 변이를 추출\n",
    "        match = re.match(pattern, part)\n",
    "        if match:\n",
    "            # 각각의 위치와 아미노산 정보 추출\n",
    "            pos1, pos2, original_aa, new_aa = match.groups()\n",
    "            \n",
    "            # 변이 형식으로 변환\n",
    "            aa1 = f\"{original_aa[0]}{pos1}{new_aa[0] if new_aa else original_aa[0]}\"\n",
    "            aa2 = f\"{original_aa[1]}{pos2}{new_aa[1] if new_aa and len(new_aa) > 1 else original_aa[1]}\"\n",
    "            \n",
    "            # 변환된 두 아미노산을 결과에 추가\n",
    "            result.append(f\"{aa1} {aa2}\")\n",
    "        else:\n",
    "            # F1917F 같은 형식은 그대로 추가\n",
    "            result.append(part)\n",
    "    \n",
    "    # 변환된 데이터 반환 (공백으로 다시 결합)\n",
    "    return ' '.join(result)\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "def convert_deletion_format(data):\n",
    "    # 패턴: I255_I258del 같은 형식을 찾는 정규식\n",
    "    pattern = r\"([A-Z])(\\d+)_([A-Z])(\\d+)del\"\n",
    "\n",
    "    # 결과를 저장할 리스트\n",
    "    result = []\n",
    "    \n",
    "    # 데이터를 공백 기준으로 분리 (여러 변이가 있을 수 있으므로)\n",
    "    parts = data.split()\n",
    "\n",
    "    for part in parts:\n",
    "        # 정규 표현식으로 변이를 추출\n",
    "        match = re.match(pattern, part)\n",
    "        if match:\n",
    "            # 아미노산과 위치 정보 추출\n",
    "            aa1, pos1, aa2, pos2 = match.groups()\n",
    "            \n",
    "            # 각 변이를 단일 형식으로 변환\n",
    "            result.append(f\"{aa1}{pos1}del\")\n",
    "            result.append(f\"{aa2}{pos2}del\")\n",
    "        else:\n",
    "            # 단일 형식 (예: I410del)인 경우 그대로 추가\n",
    "            result.append(part)\n",
    "    \n",
    "    # 변환된 데이터 반환 (공백으로 결합)\n",
    "    return ' '.join(result)\n",
    "# 테스트 예시\n",
    "# data = '1099_1100PL>PL F1917F'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\3946466048.py:9: DtypeWarning: Columns (74,78,114,130) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_TCGA = pd.read_csv(f, sep='\\t', comment='#')  # 주석(#) 무시하고 탭으로 구분\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "\n",
    "# .maf.gz 파일을 불러오기\n",
    "file_path_1 = 'additional_data/ACC.gz'\n",
    "\n",
    "# 파일을 pandas의 DataFrame으로 읽기\n",
    "with gzip.open(file_path_1, 'rt') as f:\n",
    "    df_TCGA = pd.read_csv(f, sep='\\t', comment='#')  # 주석(#) 무시하고 탭으로 구분\n",
    "\n",
    "df_TCGA = df_TCGA.dropna(subset='HGVSp_Short')\n",
    "# 'frameshift_variant'이고, 'HGVSp_Short' 열에 'fs'가 없는 값들을 'fs'를 추가하여 수정합니다.\n",
    "df_TCGA.loc[(df_TCGA['One_Consequence'] == 'frameshift_variant') & ~df_TCGA['HGVSp_Short'].str.contains('fs'), 'HGVSp_Short'] = \\\n",
    "    df_TCGA.loc[(df_TCGA['One_Consequence'] == 'frameshift_variant') & ~df_TCGA['HGVSp_Short'].str.contains('fs'), 'HGVSp_Short'].str.replace(r'(\\*)$', r'fs\\1', regex=True)\n",
    "\n",
    "# 'Variant_Classification'이 \"Silent\"인 행의 'HGVSp_Short' 열 값을 수정합니다.\n",
    "df_TCGA.loc[df_TCGA['One_Consequence'] == \"synonymous_variant\", 'HGVSp_Short'] = \\\n",
    "    df_TCGA.loc[df_TCGA['One_Consequence'] == \"synonymous_variant\", 'HGVSp_Short'].str.replace(r'(\\w)(\\d+)=', r'\\1\\2\\1', regex=True)\n",
    "\n",
    "df_TCGA.loc[df_TCGA['One_Consequence'] == 'frameshift_variant', 'HGVSp_Short'] = df_TCGA.loc[df_TCGA['One_Consequence'] == 'frameshift_variant', 'HGVSp_Short'].str.split('*').str[0]\n",
    "df_TCGA = df_TCGA[df_TCGA['One_Consequence'].isin(['missense_variant', 'synonymous_variant', 'stop_gained', 'frameshift_variant','inframe_deletion','inframe_insertion'])]\n",
    "df_TCGA_0 = df_TCGA.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "# tar.gz 파일 열기\n",
    "with tarfile.open('additional_data_2/ACC/acc_tcga.tar.gz', 'r:gz') as tar:\n",
    "    # 압축 파일 내부의 파일 목록 보기\n",
    "#     tar.list()\n",
    "\n",
    "    # 원하는 파일을 선택하여 읽기\n",
    "    file = tar.extractfile('acc_tcga/data_mutations.txt')\n",
    "    df_TCGA_1 = pd.read_csv(file, sep='\\t')  # 탭으로 구분된 텍스트 파일 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "# tar.gz 파일 열기\n",
    "with tarfile.open('additional_data_2/ACC/acc_tcga_gdc.tar.gz', 'r:gz') as tar:\n",
    "    # 압축 파일 내부의 파일 목록 보기\n",
    "#     tar.list()\n",
    "\n",
    "    # 원하는 파일을 선택하여 읽기\n",
    "    file = tar.extractfile('acc_tcga_gdc/data_mutations.txt')\n",
    "    df_TCGA_2 = pd.read_csv(file, sep='\\t', skiprows=2)  # 탭으로 구분된 텍스트 파일 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "# tar.gz 파일 열기\n",
    "with tarfile.open('additional_data_2/ACC/acc_tcga_pan_can_atlas_2018.tar.gz', 'r:gz') as tar:\n",
    "    # 압축 파일 내부의 파일 목록 보기\n",
    "#     tar.list()\n",
    "\n",
    "    # 원하는 파일을 선택하여 읽기\n",
    "    file = tar.extractfile('acc_tcga_pan_can_atlas_2018/data_mutations.txt')\n",
    "    df_TCGA_3 = pd.read_csv(file, sep='\\t', skiprows=0)  # 탭으로 구분된 텍스트 파일 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TCGA_1 = df_TCGA_1[['Tumor_Sample_Barcode','Hugo_Symbol','Consequence','HGVSp_Short']]\n",
    "df_TCGA_2 = df_TCGA_2[['Tumor_Sample_Barcode','Hugo_Symbol','Consequence','HGVSp_Short']]\n",
    "df_TCGA_3 = df_TCGA_3[['Tumor_Sample_Barcode','Hugo_Symbol','Consequence','HGVSp_Short']]\n",
    "df_TCGA = pd.concat([df_TCGA_1,df_TCGA_2,df_TCGA_3,df_TCGA_0],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 265/265 [00:58<00:00,  4.52it/s]\n"
     ]
    }
   ],
   "source": [
    "df_TCGA = df_TCGA.dropna(subset='HGVSp_Short')\n",
    "# 'frameshift_variant'이고, 'HGVSp_Short' 열에 'fs'가 없는 값들을 'fs'를 추가하여 수정합니다.\n",
    "df_TCGA.loc[(df_TCGA['Consequence'] == 'frameshift_variant') & ~df_TCGA['HGVSp_Short'].str.contains('fs'), 'HGVSp_Short'] = \\\n",
    "    df_TCGA.loc[(df_TCGA['Consequence'] == 'frameshift_variant') & ~df_TCGA['HGVSp_Short'].str.contains('fs'), 'HGVSp_Short'].str.replace(r'(\\*)$', r'fs\\1', regex=True)\n",
    "\n",
    "# 'Variant_Classification'이 \"Silent\"인 행의 'HGVSp_Short' 열 값을 수정합니다.\n",
    "df_TCGA.loc[df_TCGA['Consequence'] == \"synonymous_variant\", 'HGVSp_Short'] = \\\n",
    "    df_TCGA.loc[df_TCGA['Consequence'] == \"synonymous_variant\", 'HGVSp_Short'].str.replace(r'(\\w)(\\d+)=', r'\\1\\2\\1', regex=True)\n",
    "\n",
    "df_TCGA.loc[df_TCGA['Consequence'] == 'frameshift_variant', 'HGVSp_Short'] = df_TCGA.loc[df_TCGA['Consequence'] == 'frameshift_variant', 'HGVSp_Short'].str.split('*').str[0]\n",
    "df_TCGA = df_TCGA[df_TCGA['Consequence'].isin(['missense_variant', 'synonymous_variant', 'stop_gained', 'frameshift_variant','inframe_deletion','inframe_insertion'])]\n",
    "df_TCGA = df_TCGA.drop_duplicates()\n",
    "\n",
    "lst = train.drop(columns=['ID','SUBCLASS']).columns\n",
    "train_col_list = list(lst)\n",
    "TCGA = df_TCGA[df_TCGA['Hugo_Symbol'].isin(train_col_list)]\n",
    "\n",
    "tcga_data = TCGA[['Tumor_Sample_Barcode','Hugo_Symbol','HGVSp_Short']].dropna()\n",
    "\n",
    "tcga_data['ID'] = tcga_data['Tumor_Sample_Barcode']\n",
    "tcga_data['HGVSp_Short'] = tcga_data['HGVSp_Short'].str[2:]\n",
    "tcga_data['SUBCLASS'] = 'ACC'\n",
    "tcga_data = tcga_data.drop(columns=['Tumor_Sample_Barcode'])\n",
    "\n",
    "ID_unique = tcga_data['ID'].unique()\n",
    "\n",
    "for idx in tqdm(ID_unique):\n",
    "    data = tcga_data[tcga_data['ID']==idx].groupby('Hugo_Symbol')['HGVSp_Short'].agg(lambda x: ' '.join(x)).reset_index()\n",
    "    data['HGVSp_Short'] = data['HGVSp_Short'].apply(lambda x: ' '.join(sorted(set(x.split()), key=x.split().index)))\n",
    "\n",
    "    new_row = pd.Series(['WT'] * train.shape[1], index=train.columns)\n",
    "    new_row['ID'] = 'ID'\n",
    "    new_row['SUBCLASS'] = 'ACC'\n",
    "\n",
    "        # Hugo_Symbol과 base_series의 인덱스 이름이 같으면 WT 값을 HGVSp_Short로 변경\n",
    "    for index, row in data.iterrows():\n",
    "        hugo_symbol = row['Hugo_Symbol']\n",
    "        mutation = row['HGVSp_Short']\n",
    "\n",
    "        # new_row의 인덱스 중 Hugo_Symbol과 일치하는 항목을 찾고 'WT'를 변이 값으로 교체\n",
    "        if hugo_symbol in new_row.index:\n",
    "            new_row[hugo_symbol] = mutation\n",
    "\n",
    "    train = pd.concat([train, pd.DataFrame(new_row).T], axis=0)\n",
    "\n",
    "train = train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dJmJwjbltQQ9"
   },
   "source": [
    "# BLCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\20855946.py:9: DtypeWarning: Columns (120) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_TCGA = pd.read_csv(f, sep='\\t', comment='#')  # 주석(#) 무시하고 탭으로 구분\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "\n",
    "# .maf.gz 파일을 불러오기\n",
    "file_path_1 = 'additional_data/BLCA.gz'\n",
    "\n",
    "# 파일을 pandas의 DataFrame으로 읽기\n",
    "with gzip.open(file_path_1, 'rt') as f:\n",
    "    df_TCGA = pd.read_csv(f, sep='\\t', comment='#')  # 주석(#) 무시하고 탭으로 구분\n",
    "\n",
    "# df_TCGA = pd.concat([df_TCGA_1,df_TCGA_2,df_TCGA_3,df_TCGA_4],axis=0)#     .drop_duplicates()\n",
    "\n",
    "df_TCGA = df_TCGA.dropna(subset='HGVSp_Short')\n",
    "# 'frameshift_variant'이고, 'HGVSp_Short' 열에 'fs'가 없는 값들을 'fs'를 추가하여 수정합니다.\n",
    "df_TCGA.loc[(df_TCGA['One_Consequence'] == 'frameshift_variant') & ~df_TCGA['HGVSp_Short'].str.contains('fs'), 'HGVSp_Short'] = \\\n",
    "    df_TCGA.loc[(df_TCGA['One_Consequence'] == 'frameshift_variant') & ~df_TCGA['HGVSp_Short'].str.contains('fs'), 'HGVSp_Short'].str.replace(r'(\\*)$', r'fs\\1', regex=True)\n",
    "\n",
    "# 'Variant_Classification'이 \"Silent\"인 행의 'HGVSp_Short' 열 값을 수정합니다.\n",
    "df_TCGA.loc[df_TCGA['One_Consequence'] == \"synonymous_variant\", 'HGVSp_Short'] = \\\n",
    "    df_TCGA.loc[df_TCGA['One_Consequence'] == \"synonymous_variant\", 'HGVSp_Short'].str.replace(r'(\\w)(\\d+)=', r'\\1\\2\\1', regex=True)\n",
    "\n",
    "df_TCGA.loc[df_TCGA['One_Consequence'] == 'frameshift_variant', 'HGVSp_Short'] = df_TCGA.loc[df_TCGA['One_Consequence'] == 'frameshift_variant', 'HGVSp_Short'].str.split('*').str[0]\n",
    "df_TCGA = df_TCGA[df_TCGA['One_Consequence'].isin(['missense_variant', 'synonymous_variant', 'stop_gained', 'frameshift_variant','inframe_deletion','inframe_insertion'])]\n",
    "df_TCGA_0 = df_TCGA.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "# tar.gz 파일 열기\n",
    "with tarfile.open('additional_data_2/BLCA/blca_bgi.tar.gz', 'r:gz') as tar:\n",
    "    # 압축 파일 내부의 파일 목록 보기\n",
    "#     tar.list()\n",
    "\n",
    "    # 원하는 파일을 선택하여 읽기\n",
    "    file = tar.extractfile('blca_bgi/data_mutations.txt')\n",
    "    df_TCGA_1 = pd.read_csv(file, sep='\\t')  # 탭으로 구분된 텍스트 파일 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "# tar.gz 파일 열기\n",
    "with tarfile.open('additional_data_2/BLCA/blca_dfarber_mskcc_2014.tar.gz', 'r:gz') as tar:\n",
    "    # 압축 파일 내부의 파일 목록 보기\n",
    "#     tar.list()\n",
    "\n",
    "    # 원하는 파일을 선택하여 읽기\n",
    "    file = tar.extractfile('blca_dfarber_mskcc_2014/data_mutations.txt')\n",
    "    df_TCGA_2 = pd.read_csv(file, sep='\\t',skiprows=1)  # 탭으로 구분된 텍스트 파일 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\2966922366.py:10: DtypeWarning: Columns (50,58,61,71,77,94) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_TCGA_3 = pd.read_csv(file, sep='\\t',skiprows=0)  # 탭으로 구분된 텍스트 파일 읽기\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "\n",
    "# tar.gz 파일 열기\n",
    "with tarfile.open('additional_data_2/BLCA/blca_tcga.tar.gz', 'r:gz') as tar:\n",
    "    # 압축 파일 내부의 파일 목록 보기\n",
    "#     tar.list()\n",
    "\n",
    "    # 원하는 파일을 선택하여 읽기\n",
    "    file = tar.extractfile('blca_tcga/data_mutations.txt')\n",
    "    df_TCGA_3 = pd.read_csv(file, sep='\\t',skiprows=0)  # 탭으로 구분된 텍스트 파일 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hugo_Symbol</th>\n",
       "      <th>Entrez_Gene_Id</th>\n",
       "      <th>Center</th>\n",
       "      <th>NCBI_Build</th>\n",
       "      <th>Chromosome</th>\n",
       "      <th>Start_Position</th>\n",
       "      <th>End_Position</th>\n",
       "      <th>Strand</th>\n",
       "      <th>Consequence</th>\n",
       "      <th>Variant_Classification</th>\n",
       "      <th>...</th>\n",
       "      <th>HGVSc</th>\n",
       "      <th>HGVSp</th>\n",
       "      <th>HGVSp_Short</th>\n",
       "      <th>Transcript_ID</th>\n",
       "      <th>RefSeq</th>\n",
       "      <th>Protein_position</th>\n",
       "      <th>Codons</th>\n",
       "      <th>Exon_Number</th>\n",
       "      <th>genomic_location_explanation</th>\n",
       "      <th>Annotation_Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UTS2</td>\n",
       "      <td>10911.0</td>\n",
       "      <td>BI</td>\n",
       "      <td>GRCh38</td>\n",
       "      <td>1</td>\n",
       "      <td>7847808</td>\n",
       "      <td>7847808</td>\n",
       "      <td>+</td>\n",
       "      <td>missense_variant</td>\n",
       "      <td>Missense_Mutation</td>\n",
       "      <td>...</td>\n",
       "      <td>ENST00000361696.10:c.333G&gt;C</td>\n",
       "      <td>p.Lys111Asn</td>\n",
       "      <td>p.K111N</td>\n",
       "      <td>ENST00000361696</td>\n",
       "      <td>NM_006786.4</td>\n",
       "      <td>111.0</td>\n",
       "      <td>aaG/aaC</td>\n",
       "      <td>4/4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SUCCESS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Hugo_Symbol  Entrez_Gene_Id Center NCBI_Build Chromosome  Start_Position  \\\n",
       "0        UTS2         10911.0     BI     GRCh38          1         7847808   \n",
       "\n",
       "   End_Position Strand       Consequence Variant_Classification  ...  \\\n",
       "0       7847808      +  missense_variant      Missense_Mutation  ...   \n",
       "\n",
       "                         HGVSc        HGVSp HGVSp_Short    Transcript_ID  \\\n",
       "0  ENST00000361696.10:c.333G>C  p.Lys111Asn     p.K111N  ENST00000361696   \n",
       "\n",
       "        RefSeq  Protein_position   Codons Exon_Number  \\\n",
       "0  NM_006786.4             111.0  aaG/aaC         4/4   \n",
       "\n",
       "   genomic_location_explanation  Annotation_Status  \n",
       "0                           NaN            SUCCESS  \n",
       "\n",
       "[1 rows x 47 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tarfile\n",
    "\n",
    "# tar.gz 파일 열기\n",
    "with tarfile.open('additional_data_2/BLCA/blca_tcga_gdc.tar.gz', 'r:gz') as tar:\n",
    "    # 압축 파일 내부의 파일 목록 보기\n",
    "#     tar.list()\n",
    "\n",
    "    # 원하는 파일을 선택하여 읽기\n",
    "    file = tar.extractfile('blca_tcga_gdc/data_mutations.txt')\n",
    "    df_TCGA_4 = pd.read_csv(file, sep='\\t',skiprows=2)  # 탭으로 구분된 텍스트 파일 읽기\n",
    "df_TCGA_4.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hugo_Symbol</th>\n",
       "      <th>Entrez_Gene_Id</th>\n",
       "      <th>Center</th>\n",
       "      <th>NCBI_Build</th>\n",
       "      <th>Chromosome</th>\n",
       "      <th>Start_Position</th>\n",
       "      <th>End_Position</th>\n",
       "      <th>Strand</th>\n",
       "      <th>Consequence</th>\n",
       "      <th>Variant_Classification</th>\n",
       "      <th>...</th>\n",
       "      <th>SYMBOL_SOURCE</th>\n",
       "      <th>TREMBL</th>\n",
       "      <th>TSL</th>\n",
       "      <th>UNIPARC</th>\n",
       "      <th>VARIANT_CLASS</th>\n",
       "      <th>all_effects</th>\n",
       "      <th>cDNA_position</th>\n",
       "      <th>n_depth</th>\n",
       "      <th>t_depth</th>\n",
       "      <th>Annotation_Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DNMBP</td>\n",
       "      <td>23268.0</td>\n",
       "      <td>.</td>\n",
       "      <td>GRCh37</td>\n",
       "      <td>10</td>\n",
       "      <td>101715548</td>\n",
       "      <td>101715548</td>\n",
       "      <td>+</td>\n",
       "      <td>synonymous_variant</td>\n",
       "      <td>Silent</td>\n",
       "      <td>...</td>\n",
       "      <td>HGNC</td>\n",
       "      <td>B4E0Q3_HUMAN</td>\n",
       "      <td>.</td>\n",
       "      <td>UPI000013D6C9</td>\n",
       "      <td>SNV</td>\n",
       "      <td>DNMBP,synonymous_variant,p.%3D,ENST00000324109...</td>\n",
       "      <td>1775</td>\n",
       "      <td>88</td>\n",
       "      <td>96</td>\n",
       "      <td>SUCCESS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 112 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Hugo_Symbol  Entrez_Gene_Id Center NCBI_Build Chromosome  Start_Position  \\\n",
       "0       DNMBP         23268.0      .     GRCh37         10       101715548   \n",
       "\n",
       "   End_Position Strand         Consequence Variant_Classification  ...  \\\n",
       "0     101715548      +  synonymous_variant                 Silent  ...   \n",
       "\n",
       "  SYMBOL_SOURCE        TREMBL TSL        UNIPARC VARIANT_CLASS  \\\n",
       "0          HGNC  B4E0Q3_HUMAN   .  UPI000013D6C9           SNV   \n",
       "\n",
       "                                         all_effects cDNA_position n_depth  \\\n",
       "0  DNMBP,synonymous_variant,p.%3D,ENST00000324109...          1775      88   \n",
       "\n",
       "  t_depth Annotation_Status  \n",
       "0      96           SUCCESS  \n",
       "\n",
       "[1 rows x 112 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tarfile\n",
    "\n",
    "# tar.gz 파일 열기\n",
    "with tarfile.open('additional_data_2/BLCA/blca_tcga_pan_can_atlas_2018.tar.gz', 'r:gz') as tar:\n",
    "    # 압축 파일 내부의 파일 목록 보기\n",
    "#     tar.list()\n",
    "\n",
    "    # 원하는 파일을 선택하여 읽기\n",
    "    file = tar.extractfile('blca_tcga_pan_can_atlas_2018/data_mutations.txt')\n",
    "    df_TCGA_5 = pd.read_csv(file, sep='\\t',skiprows=0)  # 탭으로 구분된 텍스트 파일 읽기\n",
    "df_TCGA_5.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hugo_Symbol</th>\n",
       "      <th>Entrez_Gene_Id</th>\n",
       "      <th>Center</th>\n",
       "      <th>NCBI_Build</th>\n",
       "      <th>Chromosome</th>\n",
       "      <th>Start_Position</th>\n",
       "      <th>End_Position</th>\n",
       "      <th>Strand</th>\n",
       "      <th>Consequence</th>\n",
       "      <th>Variant_Classification</th>\n",
       "      <th>...</th>\n",
       "      <th>n_ref_count</th>\n",
       "      <th>n_alt_count</th>\n",
       "      <th>HGVSc</th>\n",
       "      <th>HGVSp</th>\n",
       "      <th>HGVSp_Short</th>\n",
       "      <th>Transcript_ID</th>\n",
       "      <th>RefSeq</th>\n",
       "      <th>Protein_position</th>\n",
       "      <th>Codons</th>\n",
       "      <th>Hotspot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EMC1</td>\n",
       "      <td>0</td>\n",
       "      <td>broad.mit.edu</td>\n",
       "      <td>GRCh37</td>\n",
       "      <td>1</td>\n",
       "      <td>19549914</td>\n",
       "      <td>19549914</td>\n",
       "      <td>+</td>\n",
       "      <td>synonymous_variant</td>\n",
       "      <td>Silent</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ENST00000477853.1:c.2352C&gt;T</td>\n",
       "      <td>p.Ile784=</td>\n",
       "      <td>p.I784=</td>\n",
       "      <td>ENST00000477853</td>\n",
       "      <td>NM_001271427.1</td>\n",
       "      <td>784.0</td>\n",
       "      <td>atC/atT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Hugo_Symbol  Entrez_Gene_Id         Center NCBI_Build Chromosome  \\\n",
       "0        EMC1               0  broad.mit.edu     GRCh37          1   \n",
       "\n",
       "   Start_Position  End_Position Strand         Consequence  \\\n",
       "0        19549914      19549914      +  synonymous_variant   \n",
       "\n",
       "  Variant_Classification  ... n_ref_count n_alt_count  \\\n",
       "0                 Silent  ...         NaN         NaN   \n",
       "\n",
       "                         HGVSc      HGVSp HGVSp_Short    Transcript_ID  \\\n",
       "0  ENST00000477853.1:c.2352C>T  p.Ile784=     p.I784=  ENST00000477853   \n",
       "\n",
       "           RefSeq Protein_position   Codons Hotspot  \n",
       "0  NM_001271427.1            784.0  atC/atT       0  \n",
       "\n",
       "[1 rows x 45 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tarfile\n",
    "\n",
    "# tar.gz 파일 열기\n",
    "with tarfile.open('additional_data_2/BLCA/blca_tcga_pub.tar.gz', 'r:gz') as tar:\n",
    "    # 압축 파일 내부의 파일 목록 보기\n",
    "#     tar.list()\n",
    "\n",
    "    # 원하는 파일을 선택하여 읽기\n",
    "    file = tar.extractfile('blca_tcga_pub/data_mutations.txt')\n",
    "    df_TCGA_6 = pd.read_csv(file, sep='\\t',skiprows=0)  # 탭으로 구분된 텍스트 파일 읽기\n",
    "df_TCGA_6.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TCGA_1 = df_TCGA_1[['Tumor_Sample_Barcode','Hugo_Symbol','Consequence','HGVSp_Short']]\n",
    "df_TCGA_2 = df_TCGA_2[['Tumor_Sample_Barcode','Hugo_Symbol','Consequence','HGVSp_Short']]\n",
    "df_TCGA_3 = df_TCGA_3[['Tumor_Sample_Barcode','Hugo_Symbol','Consequence','HGVSp_Short']]\n",
    "df_TCGA_4 = df_TCGA_4[['Tumor_Sample_Barcode','Hugo_Symbol','Consequence','HGVSp_Short']]\n",
    "df_TCGA_5 = df_TCGA_5[['Tumor_Sample_Barcode','Hugo_Symbol','Consequence','HGVSp_Short']]\n",
    "df_TCGA_6 = df_TCGA_6[['Tumor_Sample_Barcode','Hugo_Symbol','Consequence','HGVSp_Short']]\n",
    "df_TCGA = pd.concat([df_TCGA_1,df_TCGA_2,df_TCGA_3,df_TCGA_4,df_TCGA_5,df_TCGA_6,df_TCGA_0],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1379/1379 [05:39<00:00,  4.06it/s]\n"
     ]
    }
   ],
   "source": [
    "df_TCGA = df_TCGA.dropna(subset='HGVSp_Short')\n",
    "# 'frameshift_variant'이고, 'HGVSp_Short' 열에 'fs'가 없는 값들을 'fs'를 추가하여 수정합니다.\n",
    "df_TCGA.loc[(df_TCGA['Consequence'] == 'frameshift_variant') & ~df_TCGA['HGVSp_Short'].str.contains('fs'), 'HGVSp_Short'] = \\\n",
    "    df_TCGA.loc[(df_TCGA['Consequence'] == 'frameshift_variant') & ~df_TCGA['HGVSp_Short'].str.contains('fs'), 'HGVSp_Short'].str.replace(r'(\\*)$', r'fs\\1', regex=True)\n",
    "\n",
    "# 'Variant_Classification'이 \"Silent\"인 행의 'HGVSp_Short' 열 값을 수정합니다.\n",
    "df_TCGA.loc[df_TCGA['Consequence'] == \"synonymous_variant\", 'HGVSp_Short'] = \\\n",
    "    df_TCGA.loc[df_TCGA['Consequence'] == \"synonymous_variant\", 'HGVSp_Short'].str.replace(r'(\\w)(\\d+)=', r'\\1\\2\\1', regex=True)\n",
    "\n",
    "df_TCGA.loc[df_TCGA['Consequence'] == 'frameshift_variant', 'HGVSp_Short'] = df_TCGA.loc[df_TCGA['Consequence'] == 'frameshift_variant', 'HGVSp_Short'].str.split('*').str[0]\n",
    "df_TCGA = df_TCGA[df_TCGA['Consequence'].isin(['missense_variant', 'synonymous_variant', 'stop_gained', 'frameshift_variant','inframe_deletion','inframe_insertion'])]\n",
    "df_TCGA = df_TCGA.drop_duplicates()\n",
    "\n",
    "lst = train.drop(columns=['ID','SUBCLASS']).columns\n",
    "train_col_list = list(lst)\n",
    "TCGA = df_TCGA[df_TCGA['Hugo_Symbol'].isin(train_col_list)]\n",
    "\n",
    "tcga_data = TCGA[['Tumor_Sample_Barcode','Hugo_Symbol','HGVSp_Short']].dropna()\n",
    "\n",
    "tcga_data['ID'] = tcga_data['Tumor_Sample_Barcode']\n",
    "tcga_data['HGVSp_Short'] = tcga_data['HGVSp_Short'].str[2:]\n",
    "tcga_data['SUBCLASS'] = 'BLCA'\n",
    "tcga_data = tcga_data.drop(columns=['Tumor_Sample_Barcode'])\n",
    "\n",
    "ID_unique = tcga_data['ID'].unique()\n",
    "\n",
    "for idx in tqdm(ID_unique):\n",
    "    data = tcga_data[tcga_data['ID']==idx].groupby('Hugo_Symbol')['HGVSp_Short'].agg(lambda x: ' '.join(x)).reset_index()\n",
    "    data['HGVSp_Short'] = data['HGVSp_Short'].apply(lambda x: ' '.join(sorted(set(x.split()), key=x.split().index)))\n",
    "\n",
    "    new_row = pd.Series(['WT'] * train.shape[1], index=train.columns)\n",
    "    new_row['ID'] = 'ID'\n",
    "    new_row['SUBCLASS'] = 'BLCA'\n",
    "\n",
    "        # Hugo_Symbol과 base_series의 인덱스 이름이 같으면 WT 값을 HGVSp_Short로 변경\n",
    "    for index, row in data.iterrows():\n",
    "        hugo_symbol = row['Hugo_Symbol']\n",
    "        mutation = row['HGVSp_Short']\n",
    "\n",
    "        # new_row의 인덱스 중 Hugo_Symbol과 일치하는 항목을 찾고 'WT'를 변이 값으로 교체\n",
    "        if hugo_symbol in new_row.index:\n",
    "            new_row[hugo_symbol] = mutation\n",
    "\n",
    "    train = pd.concat([train, pd.DataFrame(new_row).T], axis=0)\n",
    "\n",
    "train = train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wt_MDT5atQQ9"
   },
   "source": [
    "# additional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>SUBCLASS</th>\n",
       "      <th>A2M</th>\n",
       "      <th>AAAS</th>\n",
       "      <th>AADAT</th>\n",
       "      <th>AARS1</th>\n",
       "      <th>ABAT</th>\n",
       "      <th>ABCA1</th>\n",
       "      <th>ABCA2</th>\n",
       "      <th>ABCA3</th>\n",
       "      <th>...</th>\n",
       "      <th>ZNF292</th>\n",
       "      <th>ZNF365</th>\n",
       "      <th>ZNF639</th>\n",
       "      <th>ZNF707</th>\n",
       "      <th>ZNFX1</th>\n",
       "      <th>ZNRF4</th>\n",
       "      <th>ZPBP</th>\n",
       "      <th>ZW10</th>\n",
       "      <th>ZWINT</th>\n",
       "      <th>ZYX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>KIPAN</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>SARC</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0002</td>\n",
       "      <td>SKCM</td>\n",
       "      <td>R895R</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0003</td>\n",
       "      <td>KIRC</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>GBMLGG</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7840</th>\n",
       "      <td>ID</td>\n",
       "      <td>BLCA</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7841</th>\n",
       "      <td>ID</td>\n",
       "      <td>BLCA</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7842</th>\n",
       "      <td>ID</td>\n",
       "      <td>BLCA</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7843</th>\n",
       "      <td>ID</td>\n",
       "      <td>BLCA</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>I180T</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>S276F</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7844</th>\n",
       "      <td>ID</td>\n",
       "      <td>BLCA</td>\n",
       "      <td>E184D</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7845 rows × 4386 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID SUBCLASS    A2M AAAS AADAT AARS1 ABAT ABCA1 ABCA2 ABCA3  ...  \\\n",
       "0     TRAIN_0000    KIPAN     WT   WT    WT    WT   WT    WT    WT    WT  ...   \n",
       "1     TRAIN_0001     SARC     WT   WT    WT    WT   WT    WT    WT    WT  ...   \n",
       "2     TRAIN_0002     SKCM  R895R   WT    WT    WT   WT    WT    WT    WT  ...   \n",
       "3     TRAIN_0003     KIRC     WT   WT    WT    WT   WT    WT    WT    WT  ...   \n",
       "4     TRAIN_0004   GBMLGG     WT   WT    WT    WT   WT    WT    WT    WT  ...   \n",
       "...          ...      ...    ...  ...   ...   ...  ...   ...   ...   ...  ...   \n",
       "7840          ID     BLCA     WT   WT    WT    WT   WT    WT    WT    WT  ...   \n",
       "7841          ID     BLCA     WT   WT    WT    WT   WT    WT    WT    WT  ...   \n",
       "7842          ID     BLCA     WT   WT    WT    WT   WT    WT    WT    WT  ...   \n",
       "7843          ID     BLCA     WT   WT    WT    WT   WT    WT    WT    WT  ...   \n",
       "7844          ID     BLCA  E184D   WT    WT    WT   WT    WT    WT    WT  ...   \n",
       "\n",
       "     ZNF292 ZNF365 ZNF639 ZNF707  ZNFX1 ZNRF4 ZPBP ZW10 ZWINT ZYX  \n",
       "0        WT     WT     WT     WT     WT    WT   WT   WT    WT  WT  \n",
       "1        WT     WT     WT     WT     WT    WT   WT   WT    WT  WT  \n",
       "2        WT     WT     WT     WT     WT    WT   WT   WT    WT  WT  \n",
       "3        WT     WT     WT     WT     WT    WT   WT   WT    WT  WT  \n",
       "4        WT     WT     WT     WT     WT    WT   WT   WT    WT  WT  \n",
       "...     ...    ...    ...    ...    ...   ...  ...  ...   ...  ..  \n",
       "7840     WT     WT     WT     WT     WT    WT   WT   WT    WT  WT  \n",
       "7841     WT     WT     WT     WT     WT    WT   WT   WT    WT  WT  \n",
       "7842     WT     WT     WT     WT     WT    WT   WT   WT    WT  WT  \n",
       "7843     WT  I180T     WT     WT  S276F    WT   WT   WT    WT  WT  \n",
       "7844     WT     WT     WT     WT     WT    WT   WT   WT    WT  WT  \n",
       "\n",
       "[7845 rows x 4386 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "re30qP30tQQ9"
   },
   "source": [
    "# BRCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'#version' detected with skiprows=0, retrying with skiprows=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\820952279.py:18: DtypeWarning: Columns (1,5,6,14,15,20,21,22,23,24,27,28,30,31,32,42,44,45,46,48,49,50,52,53,55,56,57,58,59,60,61,63,67,70,71,72,73,75,77,82,83,84,88,91,93,94,96,97,99,101,105,107,110,113,114,117,120,121,122,125,126,127,128,129,130,132,135,138,140,142,145,147,149,151,152) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'#version' detected with skiprows=0, retrying with skiprows=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\820952279.py:18: DtypeWarning: Columns (38,39) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n",
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\820952279.py:18: DtypeWarning: Columns (143,144) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n",
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\820952279.py:60: DtypeWarning: Columns (120) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_TCGA_2 = pd.read_csv(f, sep='\\t', comment='#')  # 주석(#) 무시하고 탭으로 구분\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "\n",
    "# BRCA 폴더에 있는 모든 .tar.gz 파일을 처리\n",
    "folder_path = 'additional_data_2/BRCA'\n",
    "gz_files = [f for f in os.listdir(folder_path) if f.endswith('.tar.gz')]\n",
    "\n",
    "# 데이터를 저장할 리스트\n",
    "df_list = []\n",
    "\n",
    "# DataFrame을 로드하는 함수 (skiprows 조정 기능 포함)\n",
    "def load_dataframe(file, sep='\\t'):\n",
    "    skiprows = 0  # 처음에는 skiprows=0으로 시도\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n",
    "            # 첫 번째 열 이름이 '#version'으로 시작하는지 확인\n",
    "            if df.columns[0].startswith('#version'):\n",
    "                print(f\"'#version' detected with skiprows={skiprows}, retrying with skiprows={skiprows+1}\")\n",
    "                skiprows += 1  # skiprows 값을 1 증가시키고 다시 시도\n",
    "                file.seek(0)  # 파일 포인터를 다시 시작점으로 이동\n",
    "            else:\n",
    "                break  # '#version'이 없으면 루프 탈출\n",
    "        except pd.errors.ParserError as e:\n",
    "            print(f\"ParserError encountered: {e}\")\n",
    "            return None  # 에러가 발생하면 None 반환\n",
    "    return df\n",
    "\n",
    "# 각 tar.gz 파일을 순차적으로 처리\n",
    "for gz_file in gz_files:\n",
    "    file_path = os.path.join(folder_path, gz_file)\n",
    "    \n",
    "    with tarfile.open(file_path, 'r:gz') as tar:\n",
    "        # 압축 파일 내부의 파일 목록을 확인\n",
    "        file_names = tar.getnames()\n",
    "        \n",
    "        # 우리가 필요한 파일 경로를 찾아서 추출\n",
    "        for file_name in file_names:\n",
    "            if 'data_mutations.txt' in file_name:  # 필요한 파일의 경로를 찾음\n",
    "                file = tar.extractfile(file_name)\n",
    "                \n",
    "                # DataFrame을 로드하는 함수 호출\n",
    "                df = load_dataframe(file, sep='\\t')\n",
    "                df = df[['Tumor_Sample_Barcode','Hugo_Symbol','Consequence','HGVSp_Short']]\n",
    "                # df가 제대로 로드되었는지 확인 후 리스트에 추가\n",
    "                if df is not None:\n",
    "                    df_list.append(df)\n",
    "                break  # 필요한 파일만 처리하므로 루프 탈출\n",
    "                \n",
    "df_TCGA = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "###################################\n",
    "# .maf.gz 파일을 불러오기\n",
    "file_path_1 = 'additional_data/BRCA.gz'\n",
    "\n",
    "# 파일을 pandas의 DataFrame으로 읽기\n",
    "with gzip.open(file_path_1, 'rt') as f:\n",
    "    df_TCGA_2 = pd.read_csv(f, sep='\\t', comment='#')  # 주석(#) 무시하고 탭으로 구분\n",
    "    \n",
    "df_TCGA_2= df_TCGA_2[['Tumor_Sample_Barcode','Hugo_Symbol','Consequence','HGVSp_Short']]\n",
    "df_TCGA = pd.concat([df_TCGA,df_TCGA_2])\n",
    "#####################################\n",
    "\n",
    "df_TCGA = df_TCGA.dropna(subset='HGVSp_Short')\n",
    "# 'frameshift_variant'이고, 'HGVSp_Short' 열에 'fs'가 없는 값들을 'fs'를 추가하여 수정합니다.\n",
    "df_TCGA.loc[(df_TCGA['Consequence'] == 'frameshift_variant') & ~df_TCGA['HGVSp_Short'].str.contains('fs'), 'HGVSp_Short'] = \\\n",
    "    df_TCGA.loc[(df_TCGA['Consequence'] == 'frameshift_variant') & ~df_TCGA['HGVSp_Short'].str.contains('fs'), 'HGVSp_Short'].str.replace(r'(\\*)$', r'fs\\1', regex=True)\n",
    "\n",
    "# 'Variant_Classification'이 \"Silent\"인 행의 'HGVSp_Short' 열 값을 수정합니다.\n",
    "df_TCGA.loc[df_TCGA['Consequence'] == \"synonymous_variant\", 'HGVSp_Short'] = \\\n",
    "    df_TCGA.loc[df_TCGA['Consequence'] == \"synonymous_variant\", 'HGVSp_Short'].str.replace(r'(\\w)(\\d+)=', r'\\1\\2\\1', regex=True)\n",
    "\n",
    "df_TCGA.loc[df_TCGA['Consequence'] == 'frameshift_variant', 'HGVSp_Short'] = df_TCGA.loc[df_TCGA['Consequence'] == 'frameshift_variant', 'HGVSp_Short'].str.split('*').str[0]\n",
    "df_TCGA = df_TCGA[df_TCGA['Consequence'].isin(['missense_variant', 'synonymous_variant', 'stop_gained', 'frameshift_variant','inframe_deletion','inframe_insertion'])]\n",
    "df_TCGA = df_TCGA.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tumor_Sample_Barcode\n",
       "TCGA-AN-A046-01                 7023\n",
       "TCGA-AC-A23H-01                 5844\n",
       "TCGA-AN-A046-01A-21W-A050-09    5678\n",
       "TCGA-AC-A23H-01A-11D-A159-09    4745\n",
       "TCGA-EW-A2FV-01                 3751\n",
       "                                ... \n",
       "TCGA-EW-A1P1-01A-31D-A14G-09       2\n",
       "TCGA-AO-A1KO-01A-31D-A188-09       2\n",
       "TCGA-A7-A0DC-01                    1\n",
       "SA085                              1\n",
       "SA063                              1\n",
       "Name: count, Length: 2346, dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_TCGA['Tumor_Sample_Barcode'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "Xy2bHCnbtQQ-",
    "outputId": "75983770-7043-4776-e98c-b643eba73599",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2336/2336 [10:59<00:00,  3.54it/s]\n"
     ]
    }
   ],
   "source": [
    "lst = train.drop(columns=['ID','SUBCLASS']).columns\n",
    "train_col_list = list(lst)\n",
    "TCGA = df_TCGA[df_TCGA['Hugo_Symbol'].isin(train_col_list)]\n",
    "tcga_data = TCGA[['Tumor_Sample_Barcode','Hugo_Symbol','HGVSp_Short']].dropna()\n",
    "\n",
    "tcga_data['ID'] = tcga_data['Tumor_Sample_Barcode']\n",
    "tcga_data['HGVSp_Short'] = tcga_data['HGVSp_Short'].str[2:]\n",
    "tcga_data['SUBCLASS'] = 'BRCA'\n",
    "tcga_data = tcga_data.drop(columns=['Tumor_Sample_Barcode'])\n",
    "\n",
    "ID_unique = tcga_data['ID'].unique()\n",
    "\n",
    "for idx in tqdm(ID_unique):\n",
    "    data = tcga_data[tcga_data['ID']==idx].groupby('Hugo_Symbol')['HGVSp_Short'].agg(lambda x: ' '.join(x)).reset_index()\n",
    "    data['HGVSp_Short'] = data['HGVSp_Short'].apply(lambda x: ' '.join(sorted(set(x.split()), key=x.split().index)))\n",
    "\n",
    "    new_row = pd.Series(['WT'] * train.shape[1], index=train.columns)\n",
    "    new_row['ID'] = 'ID'\n",
    "    new_row['SUBCLASS'] = 'BRCA'\n",
    "\n",
    "        # Hugo_Symbol과 base_series의 인덱스 이름이 같으면 WT 값을 HGVSp_Short로 변경\n",
    "    for index, row in data.iterrows():\n",
    "        hugo_symbol = row['Hugo_Symbol']\n",
    "        mutation = row['HGVSp_Short']\n",
    "\n",
    "        # new_row의 인덱스 중 Hugo_Symbol과 일치하는 항목을 찾고 'WT'를 변이 값으로 교체\n",
    "        if hugo_symbol in new_row.index:\n",
    "            new_row[hugo_symbol] = mutation\n",
    "\n",
    "    train = pd.concat([train, pd.DataFrame(new_row).T], axis=0)\n",
    "\n",
    "train = train.reset_index(drop=True)\n",
    "\n",
    "#980개 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFeUu1f6tQQ-"
   },
   "source": [
    "# additional data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBCp671CtQQ-"
   },
   "source": [
    "# CESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "7Rd4n8yStQQ-",
    "outputId": "f047b02f-7933-4ea0-9258-92d8be00b01c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\1968593471.py:18: DtypeWarning: Columns (52,53,56,72,77,81,95,100,101,103) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParserError encountered with skiprows=0, retrying with skiprows=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\1968593471.py:18: DtypeWarning: Columns (4,38,39,43) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "\n",
    "# BRCA 폴더에 있는 모든 .tar.gz 파일을 처리\n",
    "folder_path = 'additional_data_2/CESC'\n",
    "gz_files = [f for f in os.listdir(folder_path) if f.endswith('.tar.gz')]\n",
    "\n",
    "# 데이터를 저장할 리스트\n",
    "df_list = []\n",
    "\n",
    "# DataFrame을 로드하는 함수 (skiprows 조정 기능 포함)\n",
    "def load_dataframe(file, sep='\\t'):\n",
    "    skiprows = 0  # 처음에는 skiprows=0으로 시도\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n",
    "            # 첫 번째 열 이름이 '#version'으로 시작하는지 확인\n",
    "            if df.columns[0].startswith('#version'):\n",
    "                print(f\"'#version' detected with skiprows={skiprows}, retrying with skiprows={skiprows+1}\")\n",
    "                skiprows += 1  # skiprows 값을 1 증가시키고 다시 시도\n",
    "                file.seek(0)  # 파일 포인터를 다시 시작점으로 이동\n",
    "            else:\n",
    "                break  # '#version'이 없으면 루프 탈출\n",
    "        except pd.errors.ParserError:\n",
    "            if skiprows == 0:\n",
    "                print(f\"ParserError encountered with skiprows=0, retrying with skiprows=2\")\n",
    "                skiprows = 2  # skiprows를 2로 설정하고 다시 시도\n",
    "                file.seek(0)  # 파일 포인터를 다시 시작점으로 이동\n",
    "            else:\n",
    "                print(f\"ParserError encountered again with skiprows={skiprows}. Skipping file.\")\n",
    "                return None  # 에러가 발생하면 None 반환\n",
    "    return df\n",
    "\n",
    "# 각 tar.gz 파일을 순차적으로 처리\n",
    "for gz_file in gz_files:\n",
    "    file_path = os.path.join(folder_path, gz_file)\n",
    "    \n",
    "    with tarfile.open(file_path, 'r:gz') as tar:\n",
    "        # 압축 파일 내부의 파일 목록을 확인\n",
    "        file_names = tar.getnames()\n",
    "        \n",
    "        # 우리가 필요한 파일 경로를 찾아서 추출\n",
    "        for file_name in file_names:\n",
    "            if 'data_mutations.txt' in file_name:  # 필요한 파일의 경로를 찾음\n",
    "                file = tar.extractfile(file_name)\n",
    "                \n",
    "                # DataFrame을 로드하는 함수 호출\n",
    "                df = load_dataframe(file, sep='\\t')\n",
    "                df = df[['Tumor_Sample_Barcode','Hugo_Symbol','Consequence','HGVSp_Short']]\n",
    "                # df가 제대로 로드되었는지 확인 후 리스트에 추가\n",
    "                if df is not None:\n",
    "                    df_list.append(df)\n",
    "                break  # 필요한 파일만 처리하므로 루프 탈출\n",
    "                \n",
    "df_TCGA = pd.concat(df_list, ignore_index=True)\n",
    "###################################\n",
    "# .maf.gz 파일을 불러오기\n",
    "file_path_1 = 'additional_data/CESC.gz'\n",
    "\n",
    "# 파일을 pandas의 DataFrame으로 읽기\n",
    "with gzip.open(file_path_1, 'rt') as f:\n",
    "    df_TCGA_2 = pd.read_csv(f, sep='\\t', comment='#')  # 주석(#) 무시하고 탭으로 구분\n",
    "    \n",
    "df_TCGA_2= df_TCGA_2[['Tumor_Sample_Barcode','Hugo_Symbol','Consequence','HGVSp_Short']]\n",
    "df_TCGA = pd.concat([df_TCGA,df_TCGA_2])\n",
    "#####################################\n",
    "df_TCGA = df_TCGA.dropna(subset='HGVSp_Short')\n",
    "# 'frameshift_variant'이고, 'HGVSp_Short' 열에 'fs'가 없는 값들을 'fs'를 추가하여 수정합니다.\n",
    "df_TCGA.loc[(df_TCGA['Consequence'] == 'frameshift_variant') & ~df_TCGA['HGVSp_Short'].str.contains('fs'), 'HGVSp_Short'] = \\\n",
    "    df_TCGA.loc[(df_TCGA['Consequence'] == 'frameshift_variant') & ~df_TCGA['HGVSp_Short'].str.contains('fs'), 'HGVSp_Short'].str.replace(r'(\\*)$', r'fs\\1', regex=True)\n",
    "\n",
    "# 'Variant_Classification'이 \"Silent\"인 행의 'HGVSp_Short' 열 값을 수정합니다.\n",
    "df_TCGA.loc[df_TCGA['Consequence'] == \"synonymous_variant\", 'HGVSp_Short'] = \\\n",
    "    df_TCGA.loc[df_TCGA['Consequence'] == \"synonymous_variant\", 'HGVSp_Short'].str.replace(r'(\\w)(\\d+)=', r'\\1\\2\\1', regex=True)\n",
    "\n",
    "df_TCGA.loc[df_TCGA['Consequence'] == 'frameshift_variant', 'HGVSp_Short'] = df_TCGA.loc[df_TCGA['Consequence'] == 'frameshift_variant', 'HGVSp_Short'].str.split('*').str[0]\n",
    "df_TCGA = df_TCGA[df_TCGA['Consequence'].isin(['missense_variant', 'synonymous_variant', 'stop_gained', 'frameshift_variant','inframe_deletion','inframe_insertion'])]\n",
    "df_TCGA = df_TCGA.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "SIhOw4XItQQ-",
    "outputId": "f6403919-9dbf-4445-bd6c-a70277bcefa1",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 873/873 [04:45<00:00,  3.06it/s]\n"
     ]
    }
   ],
   "source": [
    "lst = train.drop(columns=['ID','SUBCLASS']).columns\n",
    "train_col_list = list(lst)\n",
    "TCGA = df_TCGA[df_TCGA['Hugo_Symbol'].isin(train_col_list)]\n",
    "tcga_data = TCGA[['Tumor_Sample_Barcode','Hugo_Symbol','HGVSp_Short']].dropna()\n",
    "\n",
    "tcga_data['ID'] = tcga_data['Tumor_Sample_Barcode']\n",
    "tcga_data['HGVSp_Short'] = tcga_data['HGVSp_Short'].str[2:]\n",
    "tcga_data['SUBCLASS'] = 'CESC'\n",
    "tcga_data = tcga_data.drop(columns=['Tumor_Sample_Barcode'])\n",
    "\n",
    "ID_unique = tcga_data['ID'].unique()\n",
    "\n",
    "for idx in tqdm(ID_unique):\n",
    "    data = tcga_data[tcga_data['ID']==idx].groupby('Hugo_Symbol')['HGVSp_Short'].agg(lambda x: ' '.join(x)).reset_index()\n",
    "    data['HGVSp_Short'] = data['HGVSp_Short'].apply(lambda x: ' '.join(sorted(set(x.split()), key=x.split().index)))\n",
    "\n",
    "    new_row = pd.Series(['WT'] * train.shape[1], index=train.columns)\n",
    "    new_row['ID'] = 'ID'\n",
    "    new_row['SUBCLASS'] = 'CESC'\n",
    "\n",
    "        # Hugo_Symbol과 base_series의 인덱스 이름이 같으면 WT 값을 HGVSp_Short로 변경\n",
    "    for index, row in data.iterrows():\n",
    "        hugo_symbol = row['Hugo_Symbol']\n",
    "        mutation = row['HGVSp_Short']\n",
    "\n",
    "        # new_row의 인덱스 중 Hugo_Symbol과 일치하는 항목을 찾고 'WT'를 변이 값으로 교체\n",
    "        if hugo_symbol in new_row.index:\n",
    "            new_row[hugo_symbol] = mutation\n",
    "\n",
    "    train = pd.concat([train, pd.DataFrame(new_row).T], axis=0)\n",
    "\n",
    "train = train.reset_index(drop=True)\n",
    "\n",
    "# 288개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>SUBCLASS</th>\n",
       "      <th>A2M</th>\n",
       "      <th>AAAS</th>\n",
       "      <th>AADAT</th>\n",
       "      <th>AARS1</th>\n",
       "      <th>ABAT</th>\n",
       "      <th>ABCA1</th>\n",
       "      <th>ABCA2</th>\n",
       "      <th>ABCA3</th>\n",
       "      <th>...</th>\n",
       "      <th>ZNF292</th>\n",
       "      <th>ZNF365</th>\n",
       "      <th>ZNF639</th>\n",
       "      <th>ZNF707</th>\n",
       "      <th>ZNFX1</th>\n",
       "      <th>ZNRF4</th>\n",
       "      <th>ZPBP</th>\n",
       "      <th>ZW10</th>\n",
       "      <th>ZWINT</th>\n",
       "      <th>ZYX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>KIPAN</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>SARC</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0002</td>\n",
       "      <td>SKCM</td>\n",
       "      <td>R895R</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0003</td>\n",
       "      <td>KIRC</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>GBMLGG</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11049</th>\n",
       "      <td>ID</td>\n",
       "      <td>CESC</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11050</th>\n",
       "      <td>ID</td>\n",
       "      <td>CESC</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11051</th>\n",
       "      <td>ID</td>\n",
       "      <td>CESC</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11052</th>\n",
       "      <td>ID</td>\n",
       "      <td>CESC</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11053</th>\n",
       "      <td>ID</td>\n",
       "      <td>CESC</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11054 rows × 4386 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               ID SUBCLASS    A2M AAAS AADAT AARS1 ABAT ABCA1 ABCA2 ABCA3  \\\n",
       "0      TRAIN_0000    KIPAN     WT   WT    WT    WT   WT    WT    WT    WT   \n",
       "1      TRAIN_0001     SARC     WT   WT    WT    WT   WT    WT    WT    WT   \n",
       "2      TRAIN_0002     SKCM  R895R   WT    WT    WT   WT    WT    WT    WT   \n",
       "3      TRAIN_0003     KIRC     WT   WT    WT    WT   WT    WT    WT    WT   \n",
       "4      TRAIN_0004   GBMLGG     WT   WT    WT    WT   WT    WT    WT    WT   \n",
       "...           ...      ...    ...  ...   ...   ...  ...   ...   ...   ...   \n",
       "11049          ID     CESC     WT   WT    WT    WT   WT    WT    WT    WT   \n",
       "11050          ID     CESC     WT   WT    WT    WT   WT    WT    WT    WT   \n",
       "11051          ID     CESC     WT   WT    WT    WT   WT    WT    WT    WT   \n",
       "11052          ID     CESC     WT   WT    WT    WT   WT    WT    WT    WT   \n",
       "11053          ID     CESC     WT   WT    WT    WT   WT    WT    WT    WT   \n",
       "\n",
       "       ... ZNF292 ZNF365 ZNF639 ZNF707 ZNFX1 ZNRF4 ZPBP ZW10 ZWINT ZYX  \n",
       "0      ...     WT     WT     WT     WT    WT    WT   WT   WT    WT  WT  \n",
       "1      ...     WT     WT     WT     WT    WT    WT   WT   WT    WT  WT  \n",
       "2      ...     WT     WT     WT     WT    WT    WT   WT   WT    WT  WT  \n",
       "3      ...     WT     WT     WT     WT    WT    WT   WT   WT    WT  WT  \n",
       "4      ...     WT     WT     WT     WT    WT    WT   WT   WT    WT  WT  \n",
       "...    ...    ...    ...    ...    ...   ...   ...  ...  ...   ...  ..  \n",
       "11049  ...     WT     WT     WT     WT    WT    WT   WT   WT    WT  WT  \n",
       "11050  ...     WT     WT     WT     WT    WT    WT   WT   WT    WT  WT  \n",
       "11051  ...     WT     WT     WT     WT    WT    WT   WT   WT    WT  WT  \n",
       "11052  ...     WT     WT     WT     WT    WT    WT   WT   WT    WT  WT  \n",
       "11053  ...     WT     WT     WT     WT    WT    WT   WT   WT    WT  WT  \n",
       "\n",
       "[11054 rows x 4386 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U5giPHdbtQQ-"
   },
   "source": [
    "# additional data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZekOVdNOtQQ-"
   },
   "source": [
    "# COAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "ZcZz7NSAtQQ-",
    "outputId": "82112f96-a31a-47cd-cd4a-6f854fa9bf7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParserError encountered with skiprows=0, retrying with skiprows=2\n",
      "ParserError encountered with skiprows=0, retrying with skiprows=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\2044667432.py:64: DtypeWarning: Columns (120) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_TCGA_2 = pd.read_csv(f, sep='\\t', comment='#')  # 주석(#) 무시하고 탭으로 구분\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "\n",
    "# BRCA 폴더에 있는 모든 .tar.gz 파일을 처리\n",
    "folder_path = 'additional_data_2/COAD'\n",
    "gz_files = [f for f in os.listdir(folder_path) if f.endswith('.tar.gz')]\n",
    "\n",
    "# 데이터를 저장할 리스트\n",
    "df_list = []\n",
    "\n",
    "# DataFrame을 로드하는 함수 (skiprows 조정 기능 포함)\n",
    "def load_dataframe(file, sep='\\t'):\n",
    "    skiprows = 0  # 처음에는 skiprows=0으로 시도\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n",
    "            # 첫 번째 열 이름이 '#version'으로 시작하는지 확인\n",
    "            if df.columns[0].startswith('#version'):\n",
    "                print(f\"'#version' detected with skiprows={skiprows}, retrying with skiprows={skiprows+1}\")\n",
    "                skiprows += 1  # skiprows 값을 1 증가시키고 다시 시도\n",
    "                file.seek(0)  # 파일 포인터를 다시 시작점으로 이동\n",
    "            else:\n",
    "                break  # '#version'이 없으면 루프 탈출\n",
    "        except pd.errors.ParserError:\n",
    "            if skiprows == 0:\n",
    "                print(f\"ParserError encountered with skiprows=0, retrying with skiprows=2\")\n",
    "                skiprows = 2  # skiprows를 2로 설정하고 다시 시도\n",
    "                file.seek(0)  # 파일 포인터를 다시 시작점으로 이동\n",
    "            else:\n",
    "                print(f\"ParserError encountered again with skiprows={skiprows}. Skipping file.\")\n",
    "                return None  # 에러가 발생하면 None 반환\n",
    "    return df\n",
    "\n",
    "# 각 tar.gz 파일을 순차적으로 처리\n",
    "for gz_file in gz_files:\n",
    "    file_path = os.path.join(folder_path, gz_file)\n",
    "    \n",
    "    with tarfile.open(file_path, 'r:gz') as tar:\n",
    "        # 압축 파일 내부의 파일 목록을 확인\n",
    "        file_names = tar.getnames()\n",
    "        \n",
    "        # 우리가 필요한 파일 경로를 찾아서 추출\n",
    "        for file_name in file_names:\n",
    "            if 'data_mutations.txt' in file_name:  # 필요한 파일의 경로를 찾음\n",
    "                file = tar.extractfile(file_name)\n",
    "                \n",
    "                # DataFrame을 로드하는 함수 호출\n",
    "                df = load_dataframe(file, sep='\\t')\n",
    "                df = df[['Tumor_Sample_Barcode','Hugo_Symbol','Consequence','HGVSp_Short']]\n",
    "                # df가 제대로 로드되었는지 확인 후 리스트에 추가\n",
    "                if df is not None:\n",
    "                    df_list.append(df)\n",
    "                break  # 필요한 파일만 처리하므로 루프 탈출\n",
    "                \n",
    "df_TCGA = pd.concat(df_list, ignore_index=True)\n",
    "###################################\n",
    "# .maf.gz 파일을 불러오기\n",
    "file_path_1 = 'additional_data/COAD.gz'\n",
    "\n",
    "# 파일을 pandas의 DataFrame으로 읽기\n",
    "with gzip.open(file_path_1, 'rt') as f:\n",
    "    df_TCGA_2 = pd.read_csv(f, sep='\\t', comment='#')  # 주석(#) 무시하고 탭으로 구분\n",
    "    \n",
    "df_TCGA_2= df_TCGA_2[['Tumor_Sample_Barcode','Hugo_Symbol','Consequence','HGVSp_Short']]\n",
    "df_TCGA = pd.concat([df_TCGA,df_TCGA_2])\n",
    "#####################################\n",
    "df_TCGA = df_TCGA.dropna(subset='HGVSp_Short')\n",
    "# 'frameshift_variant'이고, 'HGVSp_Short' 열에 'fs'가 없는 값들을 'fs'를 추가하여 수정합니다.\n",
    "df_TCGA.loc[(df_TCGA['Consequence'] == 'frameshift_variant') & ~df_TCGA['HGVSp_Short'].str.contains('fs'), 'HGVSp_Short'] = \\\n",
    "    df_TCGA.loc[(df_TCGA['Consequence'] == 'frameshift_variant') & ~df_TCGA['HGVSp_Short'].str.contains('fs'), 'HGVSp_Short'].str.replace(r'(\\*)$', r'fs\\1', regex=True)\n",
    "\n",
    "# 'Variant_Classification'이 \"Silent\"인 행의 'HGVSp_Short' 열 값을 수정합니다.\n",
    "df_TCGA.loc[df_TCGA['Consequence'] == \"synonymous_variant\", 'HGVSp_Short'] = \\\n",
    "    df_TCGA.loc[df_TCGA['Consequence'] == \"synonymous_variant\", 'HGVSp_Short'].str.replace(r'(\\w)(\\d+)=', r'\\1\\2\\1', regex=True)\n",
    "\n",
    "df_TCGA.loc[df_TCGA['Consequence'] == 'frameshift_variant', 'HGVSp_Short'] = df_TCGA.loc[df_TCGA['Consequence'] == 'frameshift_variant', 'HGVSp_Short'].str.split('*').str[0]\n",
    "df_TCGA = df_TCGA[df_TCGA['Consequence'].isin(['missense_variant', 'synonymous_variant', 'stop_gained', 'frameshift_variant','inframe_deletion','inframe_insertion'])]\n",
    "df_TCGA = df_TCGA.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "o4fZruU5tQQ-",
    "outputId": "61f837e9-76cb-4394-902f-ba8f6f028a1a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1013/1013 [06:00<00:00,  2.81it/s]\n"
     ]
    }
   ],
   "source": [
    "lst = train.drop(columns=['ID','SUBCLASS']).columns\n",
    "train_col_list = list(lst)\n",
    "TCGA = df_TCGA[df_TCGA['Hugo_Symbol'].isin(train_col_list)]\n",
    "tcga_data = TCGA[['Tumor_Sample_Barcode','Hugo_Symbol','HGVSp_Short']].dropna()\n",
    "\n",
    "tcga_data['ID'] = tcga_data['Tumor_Sample_Barcode']\n",
    "tcga_data['HGVSp_Short'] = tcga_data['HGVSp_Short'].str[2:]\n",
    "tcga_data['SUBCLASS'] = 'COAD'\n",
    "tcga_data = tcga_data.drop(columns=['Tumor_Sample_Barcode'])\n",
    "\n",
    "ID_unique = tcga_data['ID'].unique()\n",
    "\n",
    "for idx in tqdm(ID_unique):\n",
    "    data = tcga_data[tcga_data['ID']==idx].groupby('Hugo_Symbol')['HGVSp_Short'].agg(lambda x: ' '.join(x)).reset_index()\n",
    "    data['HGVSp_Short'] = data['HGVSp_Short'].apply(lambda x: ' '.join(sorted(set(x.split()), key=x.split().index)))\n",
    "\n",
    "    new_row = pd.Series(['WT'] * train.shape[1], index=train.columns)\n",
    "    new_row['ID'] = 'ID'\n",
    "    new_row['SUBCLASS'] = 'COAD'\n",
    "\n",
    "        # Hugo_Symbol과 base_series의 인덱스 이름이 같으면 WT 값을 HGVSp_Short로 변경\n",
    "    for index, row in data.iterrows():\n",
    "        hugo_symbol = row['Hugo_Symbol']\n",
    "        mutation = row['HGVSp_Short']\n",
    "\n",
    "        # new_row의 인덱스 중 Hugo_Symbol과 일치하는 항목을 찾고 'WT'를 변이 값으로 교체\n",
    "        if hugo_symbol in new_row.index:\n",
    "            new_row[hugo_symbol] = mutation\n",
    "\n",
    "    train = pd.concat([train, pd.DataFrame(new_row).T], axis=0)\n",
    "\n",
    "train = train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0tsqfcRStQQ-"
   },
   "source": [
    "# additional data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5b8Lj7_tQQ-"
   },
   "source": [
    "# DLBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "S3eqOEAktQQ-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "\n",
    "# BRCA 폴더에 있는 모든 .tar.gz 파일을 처리\n",
    "folder_path = 'additional_data_2/DLBC'\n",
    "gz_files = [f for f in os.listdir(folder_path) if f.endswith('.tar.gz')]\n",
    "\n",
    "# 데이터를 저장할 리스트\n",
    "df_list = []\n",
    "\n",
    "# DataFrame을 로드하는 함수 (skiprows 조정 기능 포함)\n",
    "def load_dataframe(file, sep='\\t'):\n",
    "    skiprows = 0  # 처음에는 skiprows=0으로 시도\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n",
    "            # 첫 번째 열 이름이 '#version'으로 시작하는지 확인\n",
    "            if df.columns[0].startswith('#version'):\n",
    "                print(f\"'#version' detected with skiprows={skiprows}, retrying with skiprows={skiprows+1}\")\n",
    "                skiprows += 1  # skiprows 값을 1 증가시키고 다시 시도\n",
    "                file.seek(0)  # 파일 포인터를 다시 시작점으로 이동\n",
    "            else:\n",
    "                break  # '#version'이 없으면 루프 탈출\n",
    "        except pd.errors.ParserError:\n",
    "            if skiprows == 0:\n",
    "                print(f\"ParserError encountered with skiprows=0, retrying with skiprows=2\")\n",
    "                skiprows = 2  # skiprows를 2로 설정하고 다시 시도\n",
    "                file.seek(0)  # 파일 포인터를 다시 시작점으로 이동\n",
    "            else:\n",
    "                print(f\"ParserError encountered again with skiprows={skiprows}. Skipping file.\")\n",
    "                return None  # 에러가 발생하면 None 반환\n",
    "    return df\n",
    "\n",
    "# 각 tar.gz 파일을 순차적으로 처리\n",
    "for gz_file in gz_files:\n",
    "    file_path = os.path.join(folder_path, gz_file)\n",
    "    \n",
    "    with tarfile.open(file_path, 'r:gz') as tar:\n",
    "        # 압축 파일 내부의 파일 목록을 확인\n",
    "        file_names = tar.getnames()\n",
    "        \n",
    "        # 우리가 필요한 파일 경로를 찾아서 추출\n",
    "        for file_name in file_names:\n",
    "            if 'data_mutations.txt' in file_name:  # 필요한 파일의 경로를 찾음\n",
    "                file = tar.extractfile(file_name)\n",
    "                \n",
    "                # DataFrame을 로드하는 함수 호출\n",
    "                df = load_dataframe(file, sep='\\t')\n",
    "                df = df[['Tumor_Sample_Barcode','Hugo_Symbol','Consequence','HGVSp_Short']]\n",
    "                # df가 제대로 로드되었는지 확인 후 리스트에 추가\n",
    "                if df is not None:\n",
    "                    df_list.append(df)\n",
    "                break  # 필요한 파일만 처리하므로 루프 탈출\n",
    "                \n",
    "df_TCGA = pd.concat(df_list, ignore_index=True)\n",
    "###################################\n",
    "# .maf.gz 파일을 불러오기\n",
    "file_path_1 = 'additional_data/DLBC.gz'\n",
    "\n",
    "# 파일을 pandas의 DataFrame으로 읽기\n",
    "with gzip.open(file_path_1, 'rt') as f:\n",
    "    df_TCGA_2 = pd.read_csv(f, sep='\\t', comment='#')  # 주석(#) 무시하고 탭으로 구분\n",
    "    \n",
    "df_TCGA_2= df_TCGA_2[['Tumor_Sample_Barcode','Hugo_Symbol','Consequence','HGVSp_Short']]\n",
    "df_TCGA = pd.concat([df_TCGA,df_TCGA_2])\n",
    "#####################################\n",
    "df_TCGA = df_TCGA.dropna(subset='HGVSp_Short')\n",
    "# 'frameshift_variant'이고, 'HGVSp_Short' 열에 'fs'가 없는 값들을 'fs'를 추가하여 수정합니다.\n",
    "df_TCGA.loc[(df_TCGA['Consequence'] == 'frameshift_variant') & ~df_TCGA['HGVSp_Short'].str.contains('fs'), 'HGVSp_Short'] = \\\n",
    "    df_TCGA.loc[(df_TCGA['Consequence'] == 'frameshift_variant') & ~df_TCGA['HGVSp_Short'].str.contains('fs'), 'HGVSp_Short'].str.replace(r'(\\*)$', r'fs\\1', regex=True)\n",
    "\n",
    "# 'Variant_Classification'이 \"Silent\"인 행의 'HGVSp_Short' 열 값을 수정합니다.\n",
    "df_TCGA.loc[df_TCGA['Consequence'] == \"synonymous_variant\", 'HGVSp_Short'] = \\\n",
    "    df_TCGA.loc[df_TCGA['Consequence'] == \"synonymous_variant\", 'HGVSp_Short'].str.replace(r'(\\w)(\\d+)=', r'\\1\\2\\1', regex=True)\n",
    "\n",
    "df_TCGA.loc[df_TCGA['Consequence'] == 'frameshift_variant', 'HGVSp_Short'] = df_TCGA.loc[df_TCGA['Consequence'] == 'frameshift_variant', 'HGVSp_Short'].str.split('*').str[0]\n",
    "df_TCGA = df_TCGA[df_TCGA['Consequence'].isin(['missense_variant', 'synonymous_variant', 'stop_gained', 'frameshift_variant','inframe_deletion','inframe_insertion'])]\n",
    "df_TCGA = df_TCGA.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tumor_Sample_Barcode</th>\n",
       "      <th>Hugo_Symbol</th>\n",
       "      <th>Consequence</th>\n",
       "      <th>HGVSp_Short</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TCGA-RQ-A6JB-01</td>\n",
       "      <td>C10orf71</td>\n",
       "      <td>inframe_deletion</td>\n",
       "      <td>p.F1299del</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TCGA-RQ-A6JB-01</td>\n",
       "      <td>STAT3</td>\n",
       "      <td>inframe_deletion</td>\n",
       "      <td>p.E616del</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TCGA-RQ-A6JB-01</td>\n",
       "      <td>KRTAP4-5</td>\n",
       "      <td>inframe_insertion</td>\n",
       "      <td>p.C81_Q82insHPSCC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TCGA-RQ-A6JB-01</td>\n",
       "      <td>DMKN</td>\n",
       "      <td>inframe_insertion</td>\n",
       "      <td>p.G270_G271insSSS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TCGA-RQ-A6JB-01</td>\n",
       "      <td>B2M</td>\n",
       "      <td>missense_variant</td>\n",
       "      <td>p.D54V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6624</th>\n",
       "      <td>TCGA-FA-A4XK-01A-11D-A31X-10</td>\n",
       "      <td>IGLV4-69</td>\n",
       "      <td>missense_variant</td>\n",
       "      <td>p.S72I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6625</th>\n",
       "      <td>TCGA-FA-A4XK-01A-11D-A31X-10</td>\n",
       "      <td>IGLC2</td>\n",
       "      <td>inframe_deletion</td>\n",
       "      <td>p.S62_Y66delinsN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6626</th>\n",
       "      <td>TCGA-FA-A4XK-01A-11D-A31X-10</td>\n",
       "      <td>IGLC2</td>\n",
       "      <td>synonymous_variant</td>\n",
       "      <td>p.T90T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6627</th>\n",
       "      <td>TCGA-FA-A4XK-01A-11D-A31X-10</td>\n",
       "      <td>IGLC3</td>\n",
       "      <td>inframe_deletion</td>\n",
       "      <td>p.S62_Y66delinsN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6628</th>\n",
       "      <td>TCGA-FA-A4XK-01A-11D-A31X-10</td>\n",
       "      <td>FAAH2</td>\n",
       "      <td>synonymous_variant</td>\n",
       "      <td>p.A96A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22099 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Tumor_Sample_Barcode Hugo_Symbol         Consequence  \\\n",
       "0                  TCGA-RQ-A6JB-01    C10orf71    inframe_deletion   \n",
       "1                  TCGA-RQ-A6JB-01       STAT3    inframe_deletion   \n",
       "2                  TCGA-RQ-A6JB-01    KRTAP4-5   inframe_insertion   \n",
       "3                  TCGA-RQ-A6JB-01        DMKN   inframe_insertion   \n",
       "4                  TCGA-RQ-A6JB-01         B2M    missense_variant   \n",
       "...                            ...         ...                 ...   \n",
       "6624  TCGA-FA-A4XK-01A-11D-A31X-10    IGLV4-69    missense_variant   \n",
       "6625  TCGA-FA-A4XK-01A-11D-A31X-10       IGLC2    inframe_deletion   \n",
       "6626  TCGA-FA-A4XK-01A-11D-A31X-10       IGLC2  synonymous_variant   \n",
       "6627  TCGA-FA-A4XK-01A-11D-A31X-10       IGLC3    inframe_deletion   \n",
       "6628  TCGA-FA-A4XK-01A-11D-A31X-10       FAAH2  synonymous_variant   \n",
       "\n",
       "            HGVSp_Short  \n",
       "0            p.F1299del  \n",
       "1             p.E616del  \n",
       "2     p.C81_Q82insHPSCC  \n",
       "3     p.G270_G271insSSS  \n",
       "4                p.D54V  \n",
       "...                 ...  \n",
       "6624             p.S72I  \n",
       "6625   p.S62_Y66delinsN  \n",
       "6626             p.T90T  \n",
       "6627   p.S62_Y66delinsN  \n",
       "6628             p.A96A  \n",
       "\n",
       "[22099 rows x 4 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_TCGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "8s-4lstvtQQ-",
    "outputId": "21236f1c-fa97-4531-d56d-967d695445d7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 94/94 [00:35<00:00,  2.64it/s]\n"
     ]
    }
   ],
   "source": [
    "lst = train.drop(columns=['ID','SUBCLASS']).columns\n",
    "train_col_list = list(lst)\n",
    "TCGA = df_TCGA[df_TCGA['Hugo_Symbol'].isin(train_col_list)]\n",
    "tcga_data = TCGA[['Tumor_Sample_Barcode','Hugo_Symbol','HGVSp_Short']].dropna()\n",
    "\n",
    "tcga_data['ID'] = tcga_data['Tumor_Sample_Barcode']\n",
    "tcga_data['HGVSp_Short'] = tcga_data['HGVSp_Short'].str[2:]\n",
    "tcga_data['SUBCLASS'] = 'DLBC'\n",
    "tcga_data = tcga_data.drop(columns=['Tumor_Sample_Barcode'])\n",
    "\n",
    "ID_unique = tcga_data['ID'].unique()\n",
    "\n",
    "for idx in tqdm(ID_unique):\n",
    "    data = tcga_data[tcga_data['ID']==idx].groupby('Hugo_Symbol')['HGVSp_Short'].agg(lambda x: ' '.join(x)).reset_index()\n",
    "    data['HGVSp_Short'] = data['HGVSp_Short'].apply(lambda x: ' '.join(sorted(set(x.split()), key=x.split().index)))\n",
    "\n",
    "    new_row = pd.Series(['WT'] * train.shape[1], index=train.columns)\n",
    "    new_row['ID'] = 'ID'\n",
    "    new_row['SUBCLASS'] = 'DLBC'\n",
    "\n",
    "        # Hugo_Symbol과 base_series의 인덱스 이름이 같으면 WT 값을 HGVSp_Short로 변경\n",
    "    for index, row in data.iterrows():\n",
    "        hugo_symbol = row['Hugo_Symbol']\n",
    "        mutation = row['HGVSp_Short']\n",
    "\n",
    "        # new_row의 인덱스 중 Hugo_Symbol과 일치하는 항목을 찾고 'WT'를 변이 값으로 교체\n",
    "        if hugo_symbol in new_row.index:\n",
    "            new_row[hugo_symbol] = mutation\n",
    "\n",
    "    train = pd.concat([train, pd.DataFrame(new_row).T], axis=0)\n",
    "\n",
    "train = train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDL5HkUNtQQ-"
   },
   "source": [
    "# additional data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jCOaetk-tQQ_"
   },
   "source": [
    "# GBMLGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "# tar.gz 파일 열기\n",
    "with tarfile.open('additional_data_2/GBMLGG/glioma_msk_2018.tar.gz', 'r:gz') as tar:\n",
    "    # 압축 파일 내부의 파일 목록 보기\n",
    "#     tar.list()\n",
    "\n",
    "    # 원하는 파일을 선택하여 읽기\n",
    "    file = tar.extractfile('glioma_msk_2018/data_mutations.txt')\n",
    "    df_TCGA_1 = pd.read_csv(file, sep='\\t',skiprows=0)  # 탭으로 구분된 텍스트 파일 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TCGA_1 = df_TCGA_1[['Tumor_Sample_Barcode','Hugo_Symbol','Consequence','Amino_Acid_Change']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "# tar.gz 파일 열기\n",
    "with tarfile.open('additional_data_2/GBMLGG/glioma_mskcc_2019.tar.gz', 'r:gz') as tar:\n",
    "    # 압축 파일 내부의 파일 목록 보기\n",
    "#     tar.list()\n",
    "\n",
    "    # 원하는 파일을 선택하여 읽기\n",
    "    file = tar.extractfile('glioma_mskcc_2019/data_mutations.txt')\n",
    "    df_TCGA_2 = pd.read_csv(file, sep='\\t',skiprows=2)  # 탭으로 구분된 텍스트 파일 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TCGA_2 = df_TCGA_2[['Tumor_Sample_Barcode','Hugo_Symbol','Consequence','HGVSp_Short']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TCGA_1.columns = ['Tumor_Sample_Barcode','Hugo_Symbol','Consequence','HGVSp_Short']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TCGA = pd.concat([df_TCGA_1,df_TCGA_2])\n",
    "\n",
    "###################################\n",
    "# .maf.gz 파일을 불러오기\n",
    "file_path_1 = 'additional_data/GBM.gz'\n",
    "\n",
    "# 파일을 pandas의 DataFrame으로 읽기\n",
    "with gzip.open(file_path_1, 'rt') as f:\n",
    "    df_TCGA_2 = pd.read_csv(f, sep='\\t', comment='#')  # 주석(#) 무시하고 탭으로 구분\n",
    "    \n",
    "df_TCGA_2= df_TCGA_2[['Tumor_Sample_Barcode','Hugo_Symbol','Consequence','HGVSp_Short']]\n",
    "df_TCGA = pd.concat([df_TCGA,df_TCGA_2])\n",
    "#####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TCGA = df_TCGA.dropna(subset='HGVSp_Short')\n",
    "# 'frameshift_variant'이고, 'HGVSp_Short' 열에 'fs'가 없는 값들을 'fs'를 추가하여 수정합니다.\n",
    "df_TCGA.loc[(df_TCGA['Consequence'] == 'frameshift_variant') & ~df_TCGA['HGVSp_Short'].str.contains('fs'), 'HGVSp_Short'] = \\\n",
    "    df_TCGA.loc[(df_TCGA['Consequence'] == 'frameshift_variant') & ~df_TCGA['HGVSp_Short'].str.contains('fs'), 'HGVSp_Short'].str.replace(r'(\\*)$', r'fs\\1', regex=True)\n",
    "\n",
    "# 'Variant_Classification'이 \"Silent\"인 행의 'HGVSp_Short' 열 값을 수정합니다.\n",
    "df_TCGA.loc[df_TCGA['Consequence'] == \"synonymous_variant\", 'HGVSp_Short'] = \\\n",
    "    df_TCGA.loc[df_TCGA['Consequence'] == \"synonymous_variant\", 'HGVSp_Short'].str.replace(r'(\\w)(\\d+)=', r'\\1\\2\\1', regex=True)\n",
    "\n",
    "df_TCGA.loc[df_TCGA['Consequence'] == 'frameshift_variant', 'HGVSp_Short'] = df_TCGA.loc[df_TCGA['Consequence'] == 'frameshift_variant', 'HGVSp_Short'].str.split('*').str[0]\n",
    "df_TCGA = df_TCGA[df_TCGA['Consequence'].isin(['missense_variant', 'synonymous_variant', 'stop_gained', 'frameshift_variant','inframe_deletion','inframe_insertion'])]\n",
    "df_TCGA = df_TCGA.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "vCDYSw7LtQQ_",
    "outputId": "d8c70539-9325-40ee-e0d9-05ebb3c004f0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1470/1470 [09:11<00:00,  2.67it/s]\n"
     ]
    }
   ],
   "source": [
    "lst = train.drop(columns=['ID','SUBCLASS']).columns\n",
    "train_col_list = list(lst)\n",
    "TCGA = df_TCGA[df_TCGA['Hugo_Symbol'].isin(train_col_list)]\n",
    "tcga_data = TCGA[['Tumor_Sample_Barcode','Hugo_Symbol','HGVSp_Short']].dropna()\n",
    "\n",
    "tcga_data['ID'] = tcga_data['Tumor_Sample_Barcode']\n",
    "tcga_data['HGVSp_Short'] = tcga_data['HGVSp_Short'].str[2:]\n",
    "tcga_data['SUBCLASS'] = 'GBMLGG'\n",
    "tcga_data = tcga_data.drop(columns=['Tumor_Sample_Barcode'])\n",
    "\n",
    "ID_unique = tcga_data['ID'].unique()\n",
    "\n",
    "for idx in tqdm(ID_unique):\n",
    "    data = tcga_data[tcga_data['ID']==idx].groupby('Hugo_Symbol')['HGVSp_Short'].agg(lambda x: ' '.join(x)).reset_index()\n",
    "    data['HGVSp_Short'] = data['HGVSp_Short'].apply(lambda x: ' '.join(sorted(set(x.split()), key=x.split().index)))\n",
    "\n",
    "    new_row = pd.Series(['WT'] * train.shape[1], index=train.columns)\n",
    "    new_row['ID'] = 'ID'\n",
    "    new_row['SUBCLASS'] = 'GBMLGG'\n",
    "\n",
    "        # Hugo_Symbol과 base_series의 인덱스 이름이 같으면 WT 값을 HGVSp_Short로 변경\n",
    "    for index, row in data.iterrows():\n",
    "        hugo_symbol = row['Hugo_Symbol']\n",
    "        mutation = row['HGVSp_Short']\n",
    "\n",
    "        # new_row의 인덱스 중 Hugo_Symbol과 일치하는 항목을 찾고 'WT'를 변이 값으로 교체\n",
    "        if hugo_symbol in new_row.index:\n",
    "            new_row[hugo_symbol] = mutation\n",
    "\n",
    "    train = pd.concat([train, pd.DataFrame(new_row).T], axis=0)\n",
    "\n",
    "train = train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p6foD_9ZtQQ_"
   },
   "source": [
    "# additional data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wctofXawtQQ_"
   },
   "source": [
    "# HNSC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "SbUr45KjtQQ_",
    "outputId": "1b651637-0260-493d-dbdd-e7f2a32956f4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\1262893719.py:18: DtypeWarning: Columns (1,5,6,20,21,22,23,24,30,31,42,44,45,46,48,53,54,55,57,60,64,68,69,70,71,81,88,99,102,103,106,116,119,125,129,133) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'#version' detected with skiprows=0, retrying with skiprows=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\1262893719.py:18: DtypeWarning: Columns (24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n",
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\1262893719.py:18: DtypeWarning: Columns (67) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParserError encountered with skiprows=0, retrying with skiprows=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\1262893719.py:18: DtypeWarning: Columns (38,39) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n",
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\1262893719.py:18: DtypeWarning: Columns (123) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "\n",
    "# BRCA 폴더에 있는 모든 .tar.gz 파일을 처리\n",
    "folder_path = 'additional_data_2/HNSC'\n",
    "gz_files = [f for f in os.listdir(folder_path) if f.endswith('.tar.gz')]\n",
    "\n",
    "# 데이터를 저장할 리스트\n",
    "df_list = []\n",
    "\n",
    "# DataFrame을 로드하는 함수 (skiprows 조정 기능 포함)\n",
    "def load_dataframe(file, sep='\\t'):\n",
    "    skiprows = 0  # 처음에는 skiprows=0으로 시도\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n",
    "            # 첫 번째 열 이름이 '#version'으로 시작하는지 확인\n",
    "            if df.columns[0].startswith('#version'):\n",
    "                print(f\"'#version' detected with skiprows={skiprows}, retrying with skiprows={skiprows+1}\")\n",
    "                skiprows += 1  # skiprows 값을 1 증가시키고 다시 시도\n",
    "                file.seek(0)  # 파일 포인터를 다시 시작점으로 이동\n",
    "            else:\n",
    "                break  # '#version'이 없으면 루프 탈출\n",
    "        except pd.errors.ParserError:\n",
    "            if skiprows == 0:\n",
    "                print(f\"ParserError encountered with skiprows=0, retrying with skiprows=2\")\n",
    "                skiprows = 2  # skiprows를 2로 설정하고 다시 시도\n",
    "                file.seek(0)  # 파일 포인터를 다시 시작점으로 이동\n",
    "            else:\n",
    "                print(f\"ParserError encountered again with skiprows={skiprows}. Skipping file.\")\n",
    "                return None  # 에러가 발생하면 None 반환\n",
    "    return df\n",
    "\n",
    "# 각 tar.gz 파일을 순차적으로 처리\n",
    "for gz_file in gz_files:\n",
    "    file_path = os.path.join(folder_path, gz_file)\n",
    "    \n",
    "    with tarfile.open(file_path, 'r:gz') as tar:\n",
    "        # 압축 파일 내부의 파일 목록을 확인\n",
    "        file_names = tar.getnames()\n",
    "        \n",
    "        # 우리가 필요한 파일 경로를 찾아서 추출\n",
    "        for file_name in file_names:\n",
    "            if 'data_mutations.txt' in file_name:  # 필요한 파일의 경로를 찾음\n",
    "                file = tar.extractfile(file_name)\n",
    "                \n",
    "                # DataFrame을 로드하는 함수 호출\n",
    "                df = load_dataframe(file, sep='\\t')\n",
    "                df = df[['Tumor_Sample_Barcode','Hugo_Symbol','Consequence','HGVSp_Short']]\n",
    "                # df가 제대로 로드되었는지 확인 후 리스트에 추가\n",
    "                if df is not None:\n",
    "                    df_list.append(df)\n",
    "                break  # 필요한 파일만 처리하므로 루프 탈출\n",
    "                \n",
    "df_TCGA = pd.concat(df_list, ignore_index=True)\n",
    "###################################\n",
    "# .maf.gz 파일을 불러오기\n",
    "file_path_1 = 'additional_data/HNSC.gz'\n",
    "\n",
    "# 파일을 pandas의 DataFrame으로 읽기\n",
    "with gzip.open(file_path_1, 'rt') as f:\n",
    "    df_TCGA_2 = pd.read_csv(f, sep='\\t', comment='#')  # 주석(#) 무시하고 탭으로 구분\n",
    "    \n",
    "df_TCGA_2= df_TCGA_2[['Tumor_Sample_Barcode','Hugo_Symbol','Consequence','HGVSp_Short']]\n",
    "df_TCGA = pd.concat([df_TCGA,df_TCGA_2])\n",
    "#####################################\n",
    "df_TCGA = df_TCGA.dropna(subset='HGVSp_Short')\n",
    "# 'frameshift_variant'이고, 'HGVSp_Short' 열에 'fs'가 없는 값들을 'fs'를 추가하여 수정합니다.\n",
    "df_TCGA.loc[(df_TCGA['Consequence'] == 'frameshift_variant') & ~df_TCGA['HGVSp_Short'].str.contains('fs'), 'HGVSp_Short'] = \\\n",
    "    df_TCGA.loc[(df_TCGA['Consequence'] == 'frameshift_variant') & ~df_TCGA['HGVSp_Short'].str.contains('fs'), 'HGVSp_Short'].str.replace(r'(\\*)$', r'fs\\1', regex=True)\n",
    "\n",
    "# 'Variant_Classification'이 \"Silent\"인 행의 'HGVSp_Short' 열 값을 수정합니다.\n",
    "df_TCGA.loc[df_TCGA['Consequence'] == \"synonymous_variant\", 'HGVSp_Short'] = \\\n",
    "    df_TCGA.loc[df_TCGA['Consequence'] == \"synonymous_variant\", 'HGVSp_Short'].str.replace(r'(\\w)(\\d+)=', r'\\1\\2\\1', regex=True)\n",
    "\n",
    "df_TCGA.loc[df_TCGA['Consequence'] == 'frameshift_variant', 'HGVSp_Short'] = df_TCGA.loc[df_TCGA['Consequence'] == 'frameshift_variant', 'HGVSp_Short'].str.split('*').str[0]\n",
    "df_TCGA = df_TCGA[df_TCGA['Consequence'].isin(['missense_variant', 'synonymous_variant', 'stop_gained', 'frameshift_variant','inframe_deletion','inframe_insertion'])]\n",
    "df_TCGA = df_TCGA.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tumor_Sample_Barcode</th>\n",
       "      <th>Hugo_Symbol</th>\n",
       "      <th>Consequence</th>\n",
       "      <th>HGVSp_Short</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HN_0-046</td>\n",
       "      <td>ASIC2</td>\n",
       "      <td>synonymous_variant</td>\n",
       "      <td>p.A402A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HN_0-046</td>\n",
       "      <td>ADAMTS18</td>\n",
       "      <td>missense_variant</td>\n",
       "      <td>p.L568S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HN_0-046</td>\n",
       "      <td>ADAMTS9</td>\n",
       "      <td>stop_gained</td>\n",
       "      <td>p.G379*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HN_0-046</td>\n",
       "      <td>AMFR</td>\n",
       "      <td>missense_variant</td>\n",
       "      <td>p.R414C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HN_0-046</td>\n",
       "      <td>ANKAR</td>\n",
       "      <td>missense_variant</td>\n",
       "      <td>p.K882T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87962</th>\n",
       "      <td>TCGA-D6-A74Q-01A-11D-A34J-08</td>\n",
       "      <td>RNF128</td>\n",
       "      <td>missense_variant</td>\n",
       "      <td>p.N74I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87963</th>\n",
       "      <td>TCGA-D6-A74Q-01A-11D-A34J-08</td>\n",
       "      <td>TRPC5</td>\n",
       "      <td>missense_variant</td>\n",
       "      <td>p.R685T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87964</th>\n",
       "      <td>TCGA-D6-A74Q-01A-11D-A34J-08</td>\n",
       "      <td>LRCH2</td>\n",
       "      <td>missense_variant</td>\n",
       "      <td>p.L716F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87965</th>\n",
       "      <td>TCGA-D6-A74Q-01A-11D-A34J-08</td>\n",
       "      <td>BRS3</td>\n",
       "      <td>missense_variant</td>\n",
       "      <td>p.S386L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87966</th>\n",
       "      <td>TCGA-D6-A74Q-01A-11D-A34J-08</td>\n",
       "      <td>GABRQ</td>\n",
       "      <td>missense_variant</td>\n",
       "      <td>p.P605L</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>286089 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Tumor_Sample_Barcode Hugo_Symbol         Consequence  \\\n",
       "0                          HN_0-046       ASIC2  synonymous_variant   \n",
       "1                          HN_0-046    ADAMTS18    missense_variant   \n",
       "2                          HN_0-046     ADAMTS9         stop_gained   \n",
       "3                          HN_0-046        AMFR    missense_variant   \n",
       "4                          HN_0-046       ANKAR    missense_variant   \n",
       "...                             ...         ...                 ...   \n",
       "87962  TCGA-D6-A74Q-01A-11D-A34J-08      RNF128    missense_variant   \n",
       "87963  TCGA-D6-A74Q-01A-11D-A34J-08       TRPC5    missense_variant   \n",
       "87964  TCGA-D6-A74Q-01A-11D-A34J-08       LRCH2    missense_variant   \n",
       "87965  TCGA-D6-A74Q-01A-11D-A34J-08        BRS3    missense_variant   \n",
       "87966  TCGA-D6-A74Q-01A-11D-A34J-08       GABRQ    missense_variant   \n",
       "\n",
       "      HGVSp_Short  \n",
       "0         p.A402A  \n",
       "1         p.L568S  \n",
       "2         p.G379*  \n",
       "3         p.R414C  \n",
       "4         p.K882T  \n",
       "...           ...  \n",
       "87962      p.N74I  \n",
       "87963     p.R685T  \n",
       "87964     p.L716F  \n",
       "87965     p.S386L  \n",
       "87966     p.P605L  \n",
       "\n",
       "[286089 rows x 4 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_TCGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "asbkX3cItQRB",
    "outputId": "f12c36ab-4b7d-4529-a684-d976e56b7931",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1632/1632 [11:01<00:00,  2.47it/s]\n"
     ]
    }
   ],
   "source": [
    "lst = train.drop(columns=['ID','SUBCLASS']).columns\n",
    "train_col_list = list(lst)\n",
    "TCGA = df_TCGA[df_TCGA['Hugo_Symbol'].isin(train_col_list)]\n",
    "tcga_data = TCGA[['Tumor_Sample_Barcode','Hugo_Symbol','HGVSp_Short']].dropna()\n",
    "\n",
    "tcga_data['ID'] = tcga_data['Tumor_Sample_Barcode']\n",
    "tcga_data['HGVSp_Short'] = tcga_data['HGVSp_Short'].str[2:]\n",
    "tcga_data['SUBCLASS'] = 'HNSC'\n",
    "tcga_data = tcga_data.drop(columns=['Tumor_Sample_Barcode'])\n",
    "\n",
    "ID_unique = tcga_data['ID'].unique()\n",
    "\n",
    "for idx in tqdm(ID_unique):\n",
    "    data = tcga_data[tcga_data['ID']==idx].groupby('Hugo_Symbol')['HGVSp_Short'].agg(lambda x: ' '.join(x)).reset_index()\n",
    "    data['HGVSp_Short'] = data['HGVSp_Short'].apply(lambda x: ' '.join(sorted(set(x.split()), key=x.split().index)))\n",
    "\n",
    "    new_row = pd.Series(['WT'] * train.shape[1], index=train.columns)\n",
    "    new_row['ID'] = 'ID'\n",
    "    new_row['SUBCLASS'] = 'HNSC'\n",
    "\n",
    "        # Hugo_Symbol과 base_series의 인덱스 이름이 같으면 WT 값을 HGVSp_Short로 변경\n",
    "    for index, row in data.iterrows():\n",
    "        hugo_symbol = row['Hugo_Symbol']\n",
    "        mutation = row['HGVSp_Short']\n",
    "\n",
    "        # new_row의 인덱스 중 Hugo_Symbol과 일치하는 항목을 찾고 'WT'를 변이 값으로 교체\n",
    "        if hugo_symbol in new_row.index:\n",
    "            new_row[hugo_symbol] = mutation\n",
    "\n",
    "    train = pd.concat([train, pd.DataFrame(new_row).T], axis=0)\n",
    "\n",
    "train = train.reset_index(drop=True)\n",
    "\n",
    "#  507개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기까지완료"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zd-3fJdvtQRB"
   },
   "source": [
    "# additional data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jDJ8NZ50tQRC"
   },
   "source": [
    "# KIRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "\n",
    "# KIRC 폴더에 있는 모든 .tar.gz 파일을 처리\n",
    "folder_path = 'additional_data_2/KIRC'\n",
    "gz_files = [f for f in os.listdir(folder_path) if f.endswith('.tar.gz')]\n",
    "gz_files = gz_files[1:]\n",
    "# 데이터를 저장할 리스트\n",
    "df_list = []\n",
    "\n",
    "# DataFrame을 로드하는 함수 (skiprows 조정 기능 포함)\n",
    "def load_dataframe(file, sep='\\t'):\n",
    "    skiprows = 0  # 처음에는 skiprows=0으로 시도\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n",
    "            # 첫 번째 열 이름이 '#version'으로 시작하는지 확인\n",
    "            if df.columns[0].startswith('#version'):\n",
    "                print(f\"'#version' detected with skiprows={skiprows}, retrying with skiprows={skiprows+1}\")\n",
    "                skiprows += 1  # skiprows 값을 1 증가시키고 다시 시도\n",
    "                file.seek(0)  # 파일 포인터를 다시 시작점으로 이동\n",
    "            else:\n",
    "                break  # '#version'이 없으면 루프 탈출\n",
    "        except pd.errors.ParserError:\n",
    "            if skiprows == 0:\n",
    "                print(f\"ParserError encountered with skiprows=0, retrying with skiprows=2\")\n",
    "                skiprows = 2  # skiprows를 2로 설정하고 다시 시도\n",
    "                file.seek(0)  # 파일 포인터를 다시 시작점으로 이동\n",
    "            else:\n",
    "                print(f\"ParserError encountered again with skiprows={skiprows}. Skipping file.\")\n",
    "                return None  # 에러가 발생하면 None 반환\n",
    "    return df\n",
    "\n",
    "# 각 tar.gz 파일을 순차적으로 처리\n",
    "for gz_file in gz_files:\n",
    "    file_path = os.path.join(folder_path, gz_file)\n",
    "    \n",
    "    with tarfile.open(file_path, 'r:gz') as tar:\n",
    "        # 압축 파일 내부의 파일 목록을 확인\n",
    "        file_names = tar.getnames()\n",
    "        \n",
    "        # 우리가 필요한 파일 경로를 찾아서 추출\n",
    "        for file_name in file_names:\n",
    "            if 'data_mutations.txt' in file_name:  # 필요한 파일의 경로를 찾음\n",
    "                file = tar.extractfile(file_name)\n",
    "                \n",
    "                # DataFrame을 로드하는 함수 호출\n",
    "                df = load_dataframe(file, sep='\\t')\n",
    "                \n",
    "                # df가 None이 아닌 경우에만 계속 처리\n",
    "                if df is not None:\n",
    "                    # Amino_Acid_Change 열이 있는지 확인하고, 있으면 이름 변경\n",
    "                    if 'Amino_Acid_Change' in df.columns:\n",
    "                        df.rename(columns={'Amino_Acid_Change': 'HGVSp_Short'}, inplace=True)\n",
    "                    \n",
    "                    # 필요한 열만 선택\n",
    "                    df = df[['Tumor_Sample_Barcode', 'Hugo_Symbol', 'Consequence', 'HGVSp_Short']]\n",
    "                    \n",
    "                    # df_list에 추가\n",
    "                    df_list.append(df)\n",
    "                break  # 필요한 파일만 처리하므로 루프 탈출\n",
    "                \n",
    "df_TCGA = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?rwxr-xr-x runner/docker          0 2024-08-13 04:14:26 ccrcc_irc_2014/ \n",
      "?rw-r--r-- runner/docker    3910113 2024-08-13 04:59:09 ccrcc_irc_2014/data_mutations.txt \n",
      "?rw-r--r-- runner/docker        455 2024-08-13 04:14:26 ccrcc_irc_2014/LICENSE \n",
      "?rw-r--r-- runner/docker        869 2024-08-13 04:59:09 ccrcc_irc_2014/data_clinical_patient.txt \n",
      "?rw-r--r-- runner/docker        141 2024-08-13 04:14:26 ccrcc_irc_2014/meta_clinical_sample.txt \n",
      "?rw-r--r-- runner/docker        310 2024-08-13 04:14:26 ccrcc_irc_2014/meta_study.txt \n",
      "?rw-r--r-- runner/docker        143 2024-08-13 04:14:26 ccrcc_irc_2014/meta_clinical_patient.txt \n",
      "?rwxr-xr-x runner/docker          0 2024-08-13 04:14:26 ccrcc_irc_2014/case_lists/ \n",
      "?rw-r--r-- runner/docker        964 2024-08-13 04:14:26 ccrcc_irc_2014/case_lists/cases_sequenced.txt \n",
      "?rw-r--r-- runner/docker        927 2024-08-13 04:14:26 ccrcc_irc_2014/case_lists/cases_all.txt \n",
      "?rw-r--r-- runner/docker        332 2024-08-13 04:14:26 ccrcc_irc_2014/meta_mutations.txt \n",
      "?rw-r--r-- runner/docker      10287 2024-08-13 04:59:09 ccrcc_irc_2014/data_clinical_sample.txt \n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "\n",
    "# tar.gz 파일 열기\n",
    "with tarfile.open('additional_data_2/KIRC/ccrcc_irc_2014.tar.gz', 'r:gz') as tar:\n",
    "    # 압축 파일 내부의 파일 목록 보기\n",
    "    tar.list()\n",
    "\n",
    "    # 원하는 파일을 선택하여 읽기\n",
    "    file = tar.extractfile('ccrcc_irc_2014/data_mutations.txt')\n",
    "    df_TCGA_irc = pd.read_csv(file, sep='\\t',skiprows=1)  # 탭으로 구분된 텍스트 파일 읽기\n",
    "df_TCGA_irc.head(3)\n",
    "df_TCGA_irc = df_TCGA_irc[['Tumor_Sample_Barcode', 'Hugo_Symbol', 'Consequence', 'HGVSp_Short']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TCGA = pd.concat([df_TCGA,df_TCGA_irc]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\4228264068.py:7: DtypeWarning: Columns (120) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_TCGA_2 = pd.read_csv(f, sep='\\t', comment='#')  # 주석(#) 무시하고 탭으로 구분\n"
     ]
    }
   ],
   "source": [
    "###################################\n",
    "# .maf.gz 파일을 불러오기\n",
    "file_path_1 = 'additional_data/KIRC.gz'\n",
    "\n",
    "# 파일을 pandas의 DataFrame으로 읽기\n",
    "with gzip.open(file_path_1, 'rt') as f:\n",
    "    df_TCGA_2 = pd.read_csv(f, sep='\\t', comment='#')  # 주석(#) 무시하고 탭으로 구분\n",
    "    \n",
    "df_TCGA_2= df_TCGA_2[['Tumor_Sample_Barcode','Hugo_Symbol','Consequence','HGVSp_Short']]\n",
    "df_TCGA = pd.concat([df_TCGA,df_TCGA_2])\n",
    "#####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "jRS8TPc3tQRC",
    "outputId": "84f47355-f82f-45c8-b3e4-94ee7d33e807"
   },
   "outputs": [],
   "source": [
    "df_TCGA = df_TCGA.dropna(subset='HGVSp_Short')\n",
    "# 'frameshift_variant'이고, 'HGVSp_Short' 열에 'fs'가 없는 값들을 'fs'를 추가하여 수정합니다.\n",
    "df_TCGA.loc[(df_TCGA['Consequence'] == 'frameshift_variant') & ~df_TCGA['HGVSp_Short'].str.contains('fs'), 'HGVSp_Short'] = \\\n",
    "    df_TCGA.loc[(df_TCGA['Consequence'] == 'frameshift_variant') & ~df_TCGA['HGVSp_Short'].str.contains('fs'), 'HGVSp_Short'].str.replace(r'(\\*)$', r'fs\\1', regex=True)\n",
    "\n",
    "# 'Variant_Classification'이 \"Silent\"인 행의 'HGVSp_Short' 열 값을 수정합니다.\n",
    "df_TCGA.loc[df_TCGA['Consequence'] == \"synonymous_variant\", 'HGVSp_Short'] = \\\n",
    "    df_TCGA.loc[df_TCGA['Consequence'] == \"synonymous_variant\", 'HGVSp_Short'].str.replace(r'(\\w)(\\d+)=', r'\\1\\2\\1', regex=True)\n",
    "\n",
    "df_TCGA.loc[df_TCGA['Consequence'] == 'frameshift_variant', 'HGVSp_Short'] = df_TCGA.loc[df_TCGA['Consequence'] == 'frameshift_variant', 'HGVSp_Short'].str.split('*').str[0]\n",
    "df_TCGA = df_TCGA[df_TCGA['Consequence'].isin(['missense_variant', 'synonymous_variant', 'stop_gained', 'frameshift_variant','inframe_deletion','inframe_insertion'])]\n",
    "df_TCGA = df_TCGA.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tumor_Sample_Barcode</th>\n",
       "      <th>Hugo_Symbol</th>\n",
       "      <th>Consequence</th>\n",
       "      <th>HGVSp_Short</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>K29</td>\n",
       "      <td>ABCB8</td>\n",
       "      <td>inframe_deletion</td>\n",
       "      <td>p.L637_D638del</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>K44</td>\n",
       "      <td>ABTB2</td>\n",
       "      <td>missense_variant</td>\n",
       "      <td>p.S845A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>K20</td>\n",
       "      <td>ACSM5</td>\n",
       "      <td>synonymous_variant</td>\n",
       "      <td>p.N409N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>K38</td>\n",
       "      <td>ADAM11</td>\n",
       "      <td>missense_variant</td>\n",
       "      <td>p.V245A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>K20</td>\n",
       "      <td>ADCY9</td>\n",
       "      <td>missense_variant</td>\n",
       "      <td>p.H239Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25717</th>\n",
       "      <td>TCGA-CZ-4853-01A-01D-1429-08</td>\n",
       "      <td>KIAA0930</td>\n",
       "      <td>missense_variant</td>\n",
       "      <td>p.D86Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25718</th>\n",
       "      <td>TCGA-CZ-4853-01A-01D-1429-08</td>\n",
       "      <td>TLR8</td>\n",
       "      <td>missense_variant</td>\n",
       "      <td>p.L323I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25719</th>\n",
       "      <td>TCGA-CZ-4853-01A-01D-1429-08</td>\n",
       "      <td>SSX5</td>\n",
       "      <td>missense_variant</td>\n",
       "      <td>p.F70L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25720</th>\n",
       "      <td>TCGA-CZ-4853-01A-01D-1429-08</td>\n",
       "      <td>SHROOM4</td>\n",
       "      <td>synonymous_variant</td>\n",
       "      <td>p.V11V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25721</th>\n",
       "      <td>TCGA-CZ-4853-01A-01D-1429-08</td>\n",
       "      <td>NEXMIF</td>\n",
       "      <td>missense_variant</td>\n",
       "      <td>p.D693G</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75955 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Tumor_Sample_Barcode Hugo_Symbol         Consequence  \\\n",
       "1                               K29       ABCB8    inframe_deletion   \n",
       "2                               K44       ABTB2    missense_variant   \n",
       "3                               K20       ACSM5  synonymous_variant   \n",
       "4                               K38      ADAM11    missense_variant   \n",
       "5                               K20       ADCY9    missense_variant   \n",
       "...                             ...         ...                 ...   \n",
       "25717  TCGA-CZ-4853-01A-01D-1429-08    KIAA0930    missense_variant   \n",
       "25718  TCGA-CZ-4853-01A-01D-1429-08        TLR8    missense_variant   \n",
       "25719  TCGA-CZ-4853-01A-01D-1429-08        SSX5    missense_variant   \n",
       "25720  TCGA-CZ-4853-01A-01D-1429-08     SHROOM4  synonymous_variant   \n",
       "25721  TCGA-CZ-4853-01A-01D-1429-08      NEXMIF    missense_variant   \n",
       "\n",
       "          HGVSp_Short  \n",
       "1      p.L637_D638del  \n",
       "2             p.S845A  \n",
       "3             p.N409N  \n",
       "4             p.V245A  \n",
       "5             p.H239Y  \n",
       "...               ...  \n",
       "25717          p.D86Y  \n",
       "25718         p.L323I  \n",
       "25719          p.F70L  \n",
       "25720          p.V11V  \n",
       "25721         p.D693G  \n",
       "\n",
       "[75955 rows x 4 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_TCGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "F33F5Pu4tQRC",
    "outputId": "2b06bc42-15f6-4c53-fa2e-56d3eb13b58b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1051/1051 [07:27<00:00,  2.35it/s]\n"
     ]
    }
   ],
   "source": [
    "lst = train.drop(columns=['ID','SUBCLASS']).columns\n",
    "train_col_list = list(lst)\n",
    "TCGA = df_TCGA[df_TCGA['Hugo_Symbol'].isin(train_col_list)]\n",
    "tcga_data = TCGA[['Tumor_Sample_Barcode','Hugo_Symbol','HGVSp_Short']].dropna()\n",
    "\n",
    "tcga_data['ID'] = tcga_data['Tumor_Sample_Barcode']\n",
    "tcga_data['HGVSp_Short'] = tcga_data['HGVSp_Short'].str[2:]\n",
    "tcga_data['SUBCLASS'] = 'KIRC'\n",
    "tcga_data = tcga_data.drop(columns=['Tumor_Sample_Barcode'])\n",
    "\n",
    "ID_unique = tcga_data['ID'].unique()\n",
    "\n",
    "for idx in tqdm(ID_unique):\n",
    "    data = tcga_data[tcga_data['ID']==idx].groupby('Hugo_Symbol')['HGVSp_Short'].agg(lambda x: ' '.join(x)).reset_index()\n",
    "    data['HGVSp_Short'] = data['HGVSp_Short'].apply(lambda x: ' '.join(sorted(set(x.split()), key=x.split().index)))\n",
    "\n",
    "    new_row = pd.Series(['WT'] * train.shape[1], index=train.columns)\n",
    "    new_row['ID'] = 'ID'\n",
    "    new_row['SUBCLASS'] = 'KIRC'\n",
    "\n",
    "        # Hugo_Symbol과 base_series의 인덱스 이름이 같으면 WT 값을 HGVSp_Short로 변경\n",
    "    for index, row in data.iterrows():\n",
    "        hugo_symbol = row['Hugo_Symbol']\n",
    "        mutation = row['HGVSp_Short']\n",
    "\n",
    "        # new_row의 인덱스 중 Hugo_Symbol과 일치하는 항목을 찾고 'WT'를 변이 값으로 교체\n",
    "        if hugo_symbol in new_row.index:\n",
    "            new_row[hugo_symbol] = mutation\n",
    "\n",
    "    train = pd.concat([train, pd.DataFrame(new_row).T], axis=0)\n",
    "\n",
    "train = train.reset_index(drop=True)\n",
    "\n",
    "# 336개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cNYmzBOHtQRC"
   },
   "source": [
    "# additional data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AjY6yDOTtQRC"
   },
   "source": [
    "# LAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "_IFV2JHYtQRC",
    "outputId": "9fdb9847-ae72-48b9-e18f-39c16e748ece"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\2214348820.py:18: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n",
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\2214348820.py:18: DtypeWarning: Columns (1,2,5,6,15,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,42,51,57,58,63,64,65) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'#version' detected with skiprows=0, retrying with skiprows=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\2214348820.py:18: DtypeWarning: Columns (1,2,5,6,15,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,42,51,57,58,63,64,65) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'#version' detected with skiprows=1, retrying with skiprows=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\2214348820.py:18: DtypeWarning: Columns (12,60) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParserError encountered with skiprows=0, retrying with skiprows=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\2214348820.py:18: DtypeWarning: Columns (33,34,35,36,45) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "\n",
    "# KIRC 폴더에 있는 모든 .tar.gz 파일을 처리\n",
    "folder_path = 'additional_data_2/LAML'\n",
    "gz_files = [f for f in os.listdir(folder_path) if f.endswith('.tar.gz')]\n",
    "\n",
    "# 데이터를 저장할 리스트\n",
    "df_list = []\n",
    "\n",
    "# DataFrame을 로드하는 함수 (skiprows 조정 기능 포함)\n",
    "def load_dataframe(file, sep='\\t'):\n",
    "    skiprows = 0  # 처음에는 skiprows=0으로 시도\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n",
    "            # 첫 번째 열 이름이 '#version'으로 시작하는지 확인\n",
    "            if df.columns[0].startswith('#'):\n",
    "                print(f\"'#version' detected with skiprows={skiprows}, retrying with skiprows={skiprows+1}\")\n",
    "                skiprows += 1  # skiprows 값을 1 증가시키고 다시 시도\n",
    "                file.seek(0)  # 파일 포인터를 다시 시작점으로 이동\n",
    "            else:\n",
    "                break  # '#version'이 없으면 루프 탈출\n",
    "        except pd.errors.ParserError:\n",
    "            if skiprows == 0:\n",
    "                print(f\"ParserError encountered with skiprows=0, retrying with skiprows=2\")\n",
    "                skiprows = 2  # skiprows를 2로 설정하고 다시 시도\n",
    "                file.seek(0)  # 파일 포인터를 다시 시작점으로 이동\n",
    "            else:\n",
    "                print(f\"ParserError encountered again with skiprows={skiprows}. Skipping file.\")\n",
    "                return None  # 에러가 발생하면 None 반환\n",
    "    return df\n",
    "\n",
    "# 각 tar.gz 파일을 순차적으로 처리\n",
    "for gz_file in gz_files:\n",
    "    file_path = os.path.join(folder_path, gz_file)\n",
    "    \n",
    "    with tarfile.open(file_path, 'r:gz') as tar:\n",
    "        # 압축 파일 내부의 파일 목록을 확인\n",
    "        file_names = tar.getnames()\n",
    "        \n",
    "        # 우리가 필요한 파일 경로를 찾아서 추출\n",
    "        for file_name in file_names:\n",
    "            if 'data_mutations.txt' in file_name:  # 필요한 파일의 경로를 찾음\n",
    "                file = tar.extractfile(file_name)\n",
    "                \n",
    "                # DataFrame을 로드하는 함수 호출\n",
    "                df = load_dataframe(file, sep='\\t')\n",
    "                \n",
    "                # df가 None이 아닌 경우에만 계속 처리\n",
    "                if df is not None:\n",
    "                    # Amino_Acid_Change 열이 있는지 확인하고, 있으면 이름 변경\n",
    "                    if 'Amino_Acid_Change' in df.columns:\n",
    "                        df.rename(columns={'Amino_Acid_Change': 'HGVSp_Short'}, inplace=True)\n",
    "                    \n",
    "                    # 필요한 열만 선택\n",
    "                    df = df[['Tumor_Sample_Barcode', 'Hugo_Symbol', 'Consequence', 'HGVSp_Short']]\n",
    "                    \n",
    "                    # df_list에 추가\n",
    "                    df_list.append(df)\n",
    "                break  # 필요한 파일만 처리하므로 루프 탈출\n",
    "\n",
    "                \n",
    "df_TCGA = pd.concat(df_list, ignore_index=True)\n",
    "###################################\n",
    "# .maf.gz 파일을 불러오기\n",
    "file_path_1 = 'additional_data/LAML.gz'\n",
    "\n",
    "# 파일을 pandas의 DataFrame으로 읽기\n",
    "with gzip.open(file_path_1, 'rt') as f:\n",
    "    df_TCGA_2 = pd.read_csv(f, sep='\\t', comment='#')  # 주석(#) 무시하고 탭으로 구분\n",
    "    \n",
    "df_TCGA_2= df_TCGA_2[['Tumor_Sample_Barcode','Hugo_Symbol','Consequence','HGVSp_Short']]\n",
    "df_TCGA = pd.concat([df_TCGA,df_TCGA_2])\n",
    "#####################################\n",
    "df_TCGA = df_TCGA.dropna(subset='HGVSp_Short')\n",
    "# 'frameshift_variant'이고, 'HGVSp_Short' 열에 'fs'가 없는 값들을 'fs'를 추가하여 수정합니다.\n",
    "df_TCGA.loc[(df_TCGA['Consequence'] == 'frameshift_variant') & ~df_TCGA['HGVSp_Short'].str.contains('fs'), 'HGVSp_Short'] = \\\n",
    "    df_TCGA.loc[(df_TCGA['Consequence'] == 'frameshift_variant') & ~df_TCGA['HGVSp_Short'].str.contains('fs'), 'HGVSp_Short'].str.replace(r'(\\*)$', r'fs\\1', regex=True)\n",
    "\n",
    "# 'Variant_Classification'이 \"Silent\"인 행의 'HGVSp_Short' 열 값을 수정합니다.\n",
    "df_TCGA.loc[df_TCGA['Consequence'] == \"synonymous_variant\", 'HGVSp_Short'] = \\\n",
    "    df_TCGA.loc[df_TCGA['Consequence'] == \"synonymous_variant\", 'HGVSp_Short'].str.replace(r'(\\w)(\\d+)=', r'\\1\\2\\1', regex=True)\n",
    "\n",
    "df_TCGA.loc[df_TCGA['Consequence'] == 'frameshift_variant', 'HGVSp_Short'] = df_TCGA.loc[df_TCGA['Consequence'] == 'frameshift_variant', 'HGVSp_Short'].str.split('*').str[0]\n",
    "df_TCGA = df_TCGA[df_TCGA['Consequence'].isin(['missense_variant', 'synonymous_variant', 'stop_gained', 'frameshift_variant','inframe_deletion','inframe_insertion'])]\n",
    "df_TCGA = df_TCGA.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tumor_Sample_Barcode</th>\n",
       "      <th>Hugo_Symbol</th>\n",
       "      <th>Consequence</th>\n",
       "      <th>HGVSp_Short</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aml_ohsu_2018_14-00141</td>\n",
       "      <td>C1orf170</td>\n",
       "      <td>missense_variant</td>\n",
       "      <td>p.A551T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aml_ohsu_2018_15-00492</td>\n",
       "      <td>C1orf170</td>\n",
       "      <td>missense_variant</td>\n",
       "      <td>p.A396V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aml_ohsu_2018_14-00676</td>\n",
       "      <td>AGRN</td>\n",
       "      <td>missense_variant</td>\n",
       "      <td>p.G1111V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>aml_ohsu_2018_13-00515</td>\n",
       "      <td>UBE2J2</td>\n",
       "      <td>missense_variant</td>\n",
       "      <td>p.M96L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>aml_ohsu_2018_15-00805</td>\n",
       "      <td>CPSF3L</td>\n",
       "      <td>missense_variant</td>\n",
       "      <td>p.V118I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3895</th>\n",
       "      <td>TCGA-AB-2956-03A-01W-0733-08</td>\n",
       "      <td>POLR2B</td>\n",
       "      <td>missense_variant</td>\n",
       "      <td>p.I710V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3896</th>\n",
       "      <td>TCGA-AB-2956-03A-01W-0733-08</td>\n",
       "      <td>RWDD4</td>\n",
       "      <td>synonymous_variant</td>\n",
       "      <td>p.L11L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3897</th>\n",
       "      <td>TCGA-AB-2956-03A-01W-0733-08</td>\n",
       "      <td>GRM6</td>\n",
       "      <td>synonymous_variant</td>\n",
       "      <td>p.G226G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3898</th>\n",
       "      <td>TCGA-AB-2956-03A-01W-0733-08</td>\n",
       "      <td>PKP2</td>\n",
       "      <td>stop_gained</td>\n",
       "      <td>p.R147*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3899</th>\n",
       "      <td>TCGA-AB-2956-03A-01W-0733-08</td>\n",
       "      <td>LPIN2</td>\n",
       "      <td>missense_variant</td>\n",
       "      <td>p.D188N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25083 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Tumor_Sample_Barcode Hugo_Symbol         Consequence HGVSp_Short\n",
       "0           aml_ohsu_2018_14-00141    C1orf170    missense_variant     p.A551T\n",
       "2           aml_ohsu_2018_15-00492    C1orf170    missense_variant     p.A396V\n",
       "4           aml_ohsu_2018_14-00676        AGRN    missense_variant    p.G1111V\n",
       "6           aml_ohsu_2018_13-00515      UBE2J2    missense_variant      p.M96L\n",
       "7           aml_ohsu_2018_15-00805      CPSF3L    missense_variant     p.V118I\n",
       "...                            ...         ...                 ...         ...\n",
       "3895  TCGA-AB-2956-03A-01W-0733-08      POLR2B    missense_variant     p.I710V\n",
       "3896  TCGA-AB-2956-03A-01W-0733-08       RWDD4  synonymous_variant      p.L11L\n",
       "3897  TCGA-AB-2956-03A-01W-0733-08        GRM6  synonymous_variant     p.G226G\n",
       "3898  TCGA-AB-2956-03A-01W-0733-08        PKP2         stop_gained     p.R147*\n",
       "3899  TCGA-AB-2956-03A-01W-0733-08       LPIN2    missense_variant     p.D188N\n",
       "\n",
       "[25083 rows x 4 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_TCGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "LX5x0YyjtQRC",
    "outputId": "9e9e5f9a-d2c1-4292-cf19-6bdfd208169e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1590/1590 [11:50<00:00,  2.24it/s]\n"
     ]
    }
   ],
   "source": [
    "lst = train.drop(columns=['ID','SUBCLASS']).columns\n",
    "train_col_list = list(lst)\n",
    "TCGA = df_TCGA[df_TCGA['Hugo_Symbol'].isin(train_col_list)]\n",
    "tcga_data = TCGA[['Tumor_Sample_Barcode','Hugo_Symbol','HGVSp_Short']].dropna()\n",
    "\n",
    "tcga_data['ID'] = tcga_data['Tumor_Sample_Barcode']\n",
    "tcga_data['HGVSp_Short'] = tcga_data['HGVSp_Short'].str[2:]\n",
    "tcga_data['SUBCLASS'] = 'LAML'\n",
    "tcga_data = tcga_data.drop(columns=['Tumor_Sample_Barcode'])\n",
    "\n",
    "ID_unique = tcga_data['ID'].unique()\n",
    "\n",
    "for idx in tqdm(ID_unique):\n",
    "    data = tcga_data[tcga_data['ID']==idx].groupby('Hugo_Symbol')['HGVSp_Short'].agg(lambda x: ' '.join(x)).reset_index()\n",
    "    data['HGVSp_Short'] = data['HGVSp_Short'].apply(lambda x: ' '.join(sorted(set(x.split()), key=x.split().index)))\n",
    "\n",
    "    new_row = pd.Series(['WT'] * train.shape[1], index=train.columns)\n",
    "    new_row['ID'] = 'ID'\n",
    "    new_row['SUBCLASS'] = 'LAML'\n",
    "\n",
    "        # Hugo_Symbol과 base_series의 인덱스 이름이 같으면 WT 값을 HGVSp_Short로 변경\n",
    "    for index, row in data.iterrows():\n",
    "        hugo_symbol = row['Hugo_Symbol']\n",
    "        mutation = row['HGVSp_Short']\n",
    "\n",
    "        # new_row의 인덱스 중 Hugo_Symbol과 일치하는 항목을 찾고 'WT'를 변이 값으로 교체\n",
    "        if hugo_symbol in new_row.index:\n",
    "            new_row[hugo_symbol] = mutation\n",
    "\n",
    "    train = pd.concat([train, pd.DataFrame(new_row).T], axis=0)\n",
    "\n",
    "train = train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZcC8v16htQRC"
   },
   "source": [
    "# additional data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kOU3IVbDtQRC"
   },
   "source": [
    "# LGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "4g6ZTXo9tQRC",
    "outputId": "a1d10d92-5db3-470b-efb9-27541df8feff"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\898209436.py:18: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n",
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\898209436.py:73: DtypeWarning: Columns (74,78,112,114,120,130,131) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_TCGA_2 = pd.read_csv(f, sep='\\t', comment='#')  # 주석(#) 무시하고 탭으로 구분\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "\n",
    "# KIRC 폴더에 있는 모든 .tar.gz 파일을 처리\n",
    "folder_path = 'additional_data_2/LGG'\n",
    "gz_files = [f for f in os.listdir(folder_path) if f.endswith('.tar.gz')]\n",
    "\n",
    "# 데이터를 저장할 리스트\n",
    "df_list = []\n",
    "\n",
    "# DataFrame을 로드하는 함수 (skiprows 조정 기능 포함)\n",
    "def load_dataframe(file, sep='\\t'):\n",
    "    skiprows = 0  # 처음에는 skiprows=0으로 시도\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n",
    "            # 첫 번째 열 이름이 '#version'으로 시작하는지 확인\n",
    "            if df.columns[0].startswith('#'):\n",
    "                print(f\"'#version' detected with skiprows={skiprows}, retrying with skiprows={skiprows+1}\")\n",
    "                skiprows += 1  # skiprows 값을 1 증가시키고 다시 시도\n",
    "                file.seek(0)  # 파일 포인터를 다시 시작점으로 이동\n",
    "            else:\n",
    "                break  # '#version'이 없으면 루프 탈출\n",
    "        except pd.errors.ParserError:\n",
    "            if skiprows == 0:\n",
    "                print(f\"ParserError encountered with skiprows=0, retrying with skiprows=2\")\n",
    "                skiprows = 2  # skiprows를 2로 설정하고 다시 시도\n",
    "                file.seek(0)  # 파일 포인터를 다시 시작점으로 이동\n",
    "            else:\n",
    "                print(f\"ParserError encountered again with skiprows={skiprows}. Skipping file.\")\n",
    "                return None  # 에러가 발생하면 None 반환\n",
    "    return df\n",
    "\n",
    "# 각 tar.gz 파일을 순차적으로 처리\n",
    "for gz_file in gz_files:\n",
    "    file_path = os.path.join(folder_path, gz_file)\n",
    "    \n",
    "    with tarfile.open(file_path, 'r:gz') as tar:\n",
    "        # 압축 파일 내부의 파일 목록을 확인\n",
    "        file_names = tar.getnames()\n",
    "        \n",
    "        # 우리가 필요한 파일 경로를 찾아서 추출\n",
    "        for file_name in file_names:\n",
    "            if 'data_mutations.txt' in file_name:  # 필요한 파일의 경로를 찾음\n",
    "                file = tar.extractfile(file_name)\n",
    "                \n",
    "                # DataFrame을 로드하는 함수 호출\n",
    "                df = load_dataframe(file, sep='\\t')\n",
    "                \n",
    "                # df가 None이 아닌 경우에만 계속 처리\n",
    "                if df is not None:\n",
    "                    # Amino_Acid_Change 열이 있는지 확인하고, 있으면 이름 변경\n",
    "                    if 'Amino_Acid_Change' in df.columns:\n",
    "                        df.rename(columns={'Amino_Acid_Change': 'HGVSp_Short'}, inplace=True)\n",
    "                    \n",
    "                    # 필요한 열만 선택\n",
    "                    df = df[['Tumor_Sample_Barcode', 'Hugo_Symbol', 'Consequence', 'HGVSp_Short']]\n",
    "                    \n",
    "                    # df_list에 추가\n",
    "                    df_list.append(df)\n",
    "                break  # 필요한 파일만 처리하므로 루프 탈출\n",
    "\n",
    "                \n",
    "df_TCGA = pd.concat(df_list, ignore_index=True)\n",
    "###################################\n",
    "# .maf.gz 파일을 불러오기\n",
    "file_path_1 = 'additional_data/LGG.gz'\n",
    "\n",
    "# 파일을 pandas의 DataFrame으로 읽기\n",
    "with gzip.open(file_path_1, 'rt') as f:\n",
    "    df_TCGA_2 = pd.read_csv(f, sep='\\t', comment='#')  # 주석(#) 무시하고 탭으로 구분\n",
    "    \n",
    "df_TCGA_2= df_TCGA_2[['Tumor_Sample_Barcode','Hugo_Symbol','Consequence','HGVSp_Short']]\n",
    "df_TCGA = pd.concat([df_TCGA,df_TCGA_2])\n",
    "#####################################\n",
    "df_TCGA = df_TCGA.dropna(subset='HGVSp_Short')\n",
    "# 'frameshift_variant'이고, 'HGVSp_Short' 열에 'fs'가 없는 값들을 'fs'를 추가하여 수정합니다.\n",
    "df_TCGA.loc[(df_TCGA['Consequence'] == 'frameshift_variant') & ~df_TCGA['HGVSp_Short'].str.contains('fs'), 'HGVSp_Short'] = \\\n",
    "    df_TCGA.loc[(df_TCGA['Consequence'] == 'frameshift_variant') & ~df_TCGA['HGVSp_Short'].str.contains('fs'), 'HGVSp_Short'].str.replace(r'(\\*)$', r'fs\\1', regex=True)\n",
    "\n",
    "# 'Variant_Classification'이 \"Silent\"인 행의 'HGVSp_Short' 열 값을 수정합니다.\n",
    "df_TCGA.loc[df_TCGA['Consequence'] == \"synonymous_variant\", 'HGVSp_Short'] = \\\n",
    "    df_TCGA.loc[df_TCGA['Consequence'] == \"synonymous_variant\", 'HGVSp_Short'].str.replace(r'(\\w)(\\d+)=', r'\\1\\2\\1', regex=True)\n",
    "\n",
    "df_TCGA.loc[df_TCGA['Consequence'] == 'frameshift_variant', 'HGVSp_Short'] = df_TCGA.loc[df_TCGA['Consequence'] == 'frameshift_variant', 'HGVSp_Short'].str.split('*').str[0]\n",
    "df_TCGA = df_TCGA[df_TCGA['Consequence'].isin(['missense_variant', 'synonymous_variant', 'stop_gained', 'frameshift_variant','inframe_deletion','inframe_insertion'])]\n",
    "df_TCGA = df_TCGA.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tumor_Sample_Barcode</th>\n",
       "      <th>Hugo_Symbol</th>\n",
       "      <th>Consequence</th>\n",
       "      <th>HGVSp_Short</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TCGA-DU-6394-01</td>\n",
       "      <td>NOS1</td>\n",
       "      <td>synonymous_variant</td>\n",
       "      <td>p.T1128T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TCGA-DU-6394-01</td>\n",
       "      <td>JAG2</td>\n",
       "      <td>synonymous_variant</td>\n",
       "      <td>p.N547N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TCGA-DU-6394-01</td>\n",
       "      <td>ENKD1</td>\n",
       "      <td>missense_variant</td>\n",
       "      <td>p.R72H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TCGA-DU-6394-01</td>\n",
       "      <td>FKBP8</td>\n",
       "      <td>synonymous_variant</td>\n",
       "      <td>p.T168T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TCGA-DU-6394-01</td>\n",
       "      <td>IRF2BP1</td>\n",
       "      <td>missense_variant</td>\n",
       "      <td>p.C552F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32774</th>\n",
       "      <td>TCGA-P5-A77W-01A-11D-A32B-08</td>\n",
       "      <td>CCDC198</td>\n",
       "      <td>missense_variant</td>\n",
       "      <td>p.Q183P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32776</th>\n",
       "      <td>TCGA-P5-A77W-01A-11D-A32B-08</td>\n",
       "      <td>PLVAP</td>\n",
       "      <td>missense_variant</td>\n",
       "      <td>p.R19Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32777</th>\n",
       "      <td>TCGA-P5-A77W-01A-11D-A32B-08</td>\n",
       "      <td>CIC</td>\n",
       "      <td>frameshift_variant</td>\n",
       "      <td>p.V760Hfs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32778</th>\n",
       "      <td>TCGA-P5-A77W-01A-11D-A32B-08</td>\n",
       "      <td>PHKA2</td>\n",
       "      <td>missense_variant</td>\n",
       "      <td>p.R687H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32779</th>\n",
       "      <td>TCGA-P5-A77W-01A-11D-A32B-08</td>\n",
       "      <td>CXorf38</td>\n",
       "      <td>missense_variant</td>\n",
       "      <td>p.P218L</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65221 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Tumor_Sample_Barcode Hugo_Symbol         Consequence  \\\n",
       "0                   TCGA-DU-6394-01        NOS1  synonymous_variant   \n",
       "2                   TCGA-DU-6394-01        JAG2  synonymous_variant   \n",
       "3                   TCGA-DU-6394-01       ENKD1    missense_variant   \n",
       "4                   TCGA-DU-6394-01       FKBP8  synonymous_variant   \n",
       "5                   TCGA-DU-6394-01     IRF2BP1    missense_variant   \n",
       "...                             ...         ...                 ...   \n",
       "32774  TCGA-P5-A77W-01A-11D-A32B-08     CCDC198    missense_variant   \n",
       "32776  TCGA-P5-A77W-01A-11D-A32B-08       PLVAP    missense_variant   \n",
       "32777  TCGA-P5-A77W-01A-11D-A32B-08         CIC  frameshift_variant   \n",
       "32778  TCGA-P5-A77W-01A-11D-A32B-08       PHKA2    missense_variant   \n",
       "32779  TCGA-P5-A77W-01A-11D-A32B-08     CXorf38    missense_variant   \n",
       "\n",
       "      HGVSp_Short  \n",
       "0        p.T1128T  \n",
       "2         p.N547N  \n",
       "3          p.R72H  \n",
       "4         p.T168T  \n",
       "5         p.C552F  \n",
       "...           ...  \n",
       "32774     p.Q183P  \n",
       "32776      p.R19Q  \n",
       "32777   p.V760Hfs  \n",
       "32778     p.R687H  \n",
       "32779     p.P218L  \n",
       "\n",
       "[65221 rows x 4 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_TCGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "CrxQW7mXtQRC",
    "outputId": "193558ce-b65e-4794-ed60-6d2b6f0bb12d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1029/1029 [08:06<00:00,  2.12it/s]\n"
     ]
    }
   ],
   "source": [
    "lst = train.drop(columns=['ID','SUBCLASS']).columns\n",
    "train_col_list = list(lst)\n",
    "TCGA = df_TCGA[df_TCGA['Hugo_Symbol'].isin(train_col_list)]\n",
    "tcga_data = TCGA[['Tumor_Sample_Barcode','Hugo_Symbol','HGVSp_Short']].dropna()\n",
    "\n",
    "tcga_data['ID'] = tcga_data['Tumor_Sample_Barcode']\n",
    "tcga_data['HGVSp_Short'] = tcga_data['HGVSp_Short'].str[2:]\n",
    "tcga_data['SUBCLASS'] = 'LGG'\n",
    "tcga_data = tcga_data.drop(columns=['Tumor_Sample_Barcode'])\n",
    "\n",
    "ID_unique = tcga_data['ID'].unique()\n",
    "\n",
    "for idx in tqdm(ID_unique):\n",
    "    data = tcga_data[tcga_data['ID']==idx].groupby('Hugo_Symbol')['HGVSp_Short'].agg(lambda x: ' '.join(x)).reset_index()\n",
    "    data['HGVSp_Short'] = data['HGVSp_Short'].apply(lambda x: ' '.join(sorted(set(x.split()), key=x.split().index)))\n",
    "\n",
    "    new_row = pd.Series(['WT'] * train.shape[1], index=train.columns)\n",
    "    new_row['ID'] = 'ID'\n",
    "    new_row['SUBCLASS'] = 'LGG'\n",
    "\n",
    "        # Hugo_Symbol과 base_series의 인덱스 이름이 같으면 WT 값을 HGVSp_Short로 변경\n",
    "    for index, row in data.iterrows():\n",
    "        hugo_symbol = row['Hugo_Symbol']\n",
    "        mutation = row['HGVSp_Short']\n",
    "\n",
    "        # new_row의 인덱스 중 Hugo_Symbol과 일치하는 항목을 찾고 'WT'를 변이 값으로 교체\n",
    "        if hugo_symbol in new_row.index:\n",
    "            new_row[hugo_symbol] = mutation\n",
    "\n",
    "    train = pd.concat([train, pd.DataFrame(new_row).T], axis=0)\n",
    "\n",
    "train = train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>SUBCLASS</th>\n",
       "      <th>A2M</th>\n",
       "      <th>AAAS</th>\n",
       "      <th>AADAT</th>\n",
       "      <th>AARS1</th>\n",
       "      <th>ABAT</th>\n",
       "      <th>ABCA1</th>\n",
       "      <th>ABCA2</th>\n",
       "      <th>ABCA3</th>\n",
       "      <th>...</th>\n",
       "      <th>ZNF292</th>\n",
       "      <th>ZNF365</th>\n",
       "      <th>ZNF639</th>\n",
       "      <th>ZNF707</th>\n",
       "      <th>ZNFX1</th>\n",
       "      <th>ZNRF4</th>\n",
       "      <th>ZPBP</th>\n",
       "      <th>ZW10</th>\n",
       "      <th>ZWINT</th>\n",
       "      <th>ZYX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>KIPAN</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>SARC</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0002</td>\n",
       "      <td>SKCM</td>\n",
       "      <td>R895R</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0003</td>\n",
       "      <td>KIRC</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>GBMLGG</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18928</th>\n",
       "      <td>ID</td>\n",
       "      <td>LGG</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>R195G</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18929</th>\n",
       "      <td>ID</td>\n",
       "      <td>LGG</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18930</th>\n",
       "      <td>ID</td>\n",
       "      <td>LGG</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18931</th>\n",
       "      <td>ID</td>\n",
       "      <td>LGG</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18932</th>\n",
       "      <td>ID</td>\n",
       "      <td>LGG</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18933 rows × 4386 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               ID SUBCLASS    A2M AAAS AADAT AARS1 ABAT ABCA1 ABCA2 ABCA3  \\\n",
       "0      TRAIN_0000    KIPAN     WT   WT    WT    WT   WT    WT    WT    WT   \n",
       "1      TRAIN_0001     SARC     WT   WT    WT    WT   WT    WT    WT    WT   \n",
       "2      TRAIN_0002     SKCM  R895R   WT    WT    WT   WT    WT    WT    WT   \n",
       "3      TRAIN_0003     KIRC     WT   WT    WT    WT   WT    WT    WT    WT   \n",
       "4      TRAIN_0004   GBMLGG     WT   WT    WT    WT   WT    WT    WT    WT   \n",
       "...           ...      ...    ...  ...   ...   ...  ...   ...   ...   ...   \n",
       "18928          ID      LGG     WT   WT    WT    WT   WT    WT    WT    WT   \n",
       "18929          ID      LGG     WT   WT    WT    WT   WT    WT    WT    WT   \n",
       "18930          ID      LGG     WT   WT    WT    WT   WT    WT    WT    WT   \n",
       "18931          ID      LGG     WT   WT    WT    WT   WT    WT    WT    WT   \n",
       "18932          ID      LGG     WT   WT    WT    WT   WT    WT    WT    WT   \n",
       "\n",
       "       ... ZNF292 ZNF365 ZNF639 ZNF707 ZNFX1 ZNRF4 ZPBP ZW10 ZWINT ZYX  \n",
       "0      ...     WT     WT     WT     WT    WT    WT   WT   WT    WT  WT  \n",
       "1      ...     WT     WT     WT     WT    WT    WT   WT   WT    WT  WT  \n",
       "2      ...     WT     WT     WT     WT    WT    WT   WT   WT    WT  WT  \n",
       "3      ...     WT     WT     WT     WT    WT    WT   WT   WT    WT  WT  \n",
       "4      ...     WT     WT     WT     WT    WT    WT   WT   WT    WT  WT  \n",
       "...    ...    ...    ...    ...    ...   ...   ...  ...  ...   ...  ..  \n",
       "18928  ...  R195G     WT     WT     WT    WT    WT   WT   WT    WT  WT  \n",
       "18929  ...     WT     WT     WT     WT    WT    WT   WT   WT    WT  WT  \n",
       "18930  ...     WT     WT     WT     WT    WT    WT   WT   WT    WT  WT  \n",
       "18931  ...     WT     WT     WT     WT    WT    WT   WT   WT    WT  WT  \n",
       "18932  ...     WT     WT     WT     WT    WT    WT   WT   WT    WT  WT  \n",
       "\n",
       "[18933 rows x 4386 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QiMOyEIktQRC"
   },
   "source": [
    "# additional data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOdSGRFjtQRC"
   },
   "source": [
    "# LIHC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "O3rx-NoItQRC",
    "outputId": "a9d6bfc6-1a55-4181-f41b-9f7ee0dc4486"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\974844445.py:18: DtypeWarning: Columns (1,20,21,22,23,111) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'#version' detected with skiprows=0, retrying with skiprows=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\974844445.py:18: DtypeWarning: Columns (38,39) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "\n",
    "# KIRC 폴더에 있는 모든 .tar.gz 파일을 처리\n",
    "folder_path = 'additional_data_2/LIHC'\n",
    "gz_files = [f for f in os.listdir(folder_path) if f.endswith('.tar.gz')]\n",
    "\n",
    "# 데이터를 저장할 리스트\n",
    "df_list = []\n",
    "\n",
    "# DataFrame을 로드하는 함수 (skiprows 조정 기능 포함)\n",
    "def load_dataframe(file, sep='\\t'):\n",
    "    skiprows = 0  # 처음에는 skiprows=0으로 시도\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n",
    "            # 첫 번째 열 이름이 '#version'으로 시작하는지 확인\n",
    "            if df.columns[0].startswith('#'):\n",
    "                print(f\"'#version' detected with skiprows={skiprows}, retrying with skiprows={skiprows+1}\")\n",
    "                skiprows += 1  # skiprows 값을 1 증가시키고 다시 시도\n",
    "                file.seek(0)  # 파일 포인터를 다시 시작점으로 이동\n",
    "            else:\n",
    "                break  # '#version'이 없으면 루프 탈출\n",
    "        except pd.errors.ParserError:\n",
    "            if skiprows == 0:\n",
    "                print(f\"ParserError encountered with skiprows=0, retrying with skiprows=2\")\n",
    "                skiprows = 2  # skiprows를 2로 설정하고 다시 시도\n",
    "                file.seek(0)  # 파일 포인터를 다시 시작점으로 이동\n",
    "            else:\n",
    "                print(f\"ParserError encountered again with skiprows={skiprows}. Skipping file.\")\n",
    "                return None  # 에러가 발생하면 None 반환\n",
    "    return df\n",
    "\n",
    "# 각 tar.gz 파일을 순차적으로 처리\n",
    "for gz_file in gz_files:\n",
    "    file_path = os.path.join(folder_path, gz_file)\n",
    "    \n",
    "    with tarfile.open(file_path, 'r:gz') as tar:\n",
    "        # 압축 파일 내부의 파일 목록을 확인\n",
    "        file_names = tar.getnames()\n",
    "        \n",
    "        # 우리가 필요한 파일 경로를 찾아서 추출\n",
    "        for file_name in file_names:\n",
    "            if 'data_mutations.txt' in file_name:  # 필요한 파일의 경로를 찾음\n",
    "                file = tar.extractfile(file_name)\n",
    "                \n",
    "                # DataFrame을 로드하는 함수 호출\n",
    "                df = load_dataframe(file, sep='\\t')\n",
    "                \n",
    "                # df가 None이 아닌 경우에만 계속 처리\n",
    "                if df is not None:\n",
    "                    # Amino_Acid_Change 열이 있는지 확인하고, 있으면 이름 변경\n",
    "                    if 'Amino_Acid_Change' in df.columns:\n",
    "                        df.rename(columns={'Amino_Acid_Change': 'HGVSp_Short'}, inplace=True)\n",
    "                    \n",
    "                    # 필요한 열만 선택\n",
    "                    df = df[['Tumor_Sample_Barcode', 'Hugo_Symbol', 'Consequence', 'HGVSp_Short']]\n",
    "                    \n",
    "                    # df_list에 추가\n",
    "                    df_list.append(df)\n",
    "                break  # 필요한 파일만 처리하므로 루프 탈출\n",
    "\n",
    "                \n",
    "df_TCGA = pd.concat(df_list, ignore_index=True)\n",
    "###################################\n",
    "# .maf.gz 파일을 불러오기\n",
    "file_path_1 = 'additional_data/LIHC.gz'\n",
    "\n",
    "# 파일을 pandas의 DataFrame으로 읽기\n",
    "with gzip.open(file_path_1, 'rt') as f:\n",
    "    df_TCGA_2 = pd.read_csv(f, sep='\\t', comment='#')  # 주석(#) 무시하고 탭으로 구분\n",
    "    \n",
    "df_TCGA_2= df_TCGA_2[['Tumor_Sample_Barcode','Hugo_Symbol','Consequence','HGVSp_Short']]\n",
    "df_TCGA = pd.concat([df_TCGA,df_TCGA_2])\n",
    "#####################################\n",
    "df_TCGA = df_TCGA.dropna(subset='HGVSp_Short')\n",
    "# 'frameshift_variant'이고, 'HGVSp_Short' 열에 'fs'가 없는 값들을 'fs'를 추가하여 수정합니다.\n",
    "df_TCGA.loc[(df_TCGA['Consequence'] == 'frameshift_variant') & ~df_TCGA['HGVSp_Short'].str.contains('fs'), 'HGVSp_Short'] = \\\n",
    "    df_TCGA.loc[(df_TCGA['Consequence'] == 'frameshift_variant') & ~df_TCGA['HGVSp_Short'].str.contains('fs'), 'HGVSp_Short'].str.replace(r'(\\*)$', r'fs\\1', regex=True)\n",
    "\n",
    "# 'Variant_Classification'이 \"Silent\"인 행의 'HGVSp_Short' 열 값을 수정합니다.\n",
    "df_TCGA.loc[df_TCGA['Consequence'] == \"synonymous_variant\", 'HGVSp_Short'] = \\\n",
    "    df_TCGA.loc[df_TCGA['Consequence'] == \"synonymous_variant\", 'HGVSp_Short'].str.replace(r'(\\w)(\\d+)=', r'\\1\\2\\1', regex=True)\n",
    "\n",
    "df_TCGA.loc[df_TCGA['Consequence'] == 'frameshift_variant', 'HGVSp_Short'] = df_TCGA.loc[df_TCGA['Consequence'] == 'frameshift_variant', 'HGVSp_Short'].str.split('*').str[0]\n",
    "df_TCGA = df_TCGA[df_TCGA['Consequence'].isin(['missense_variant', 'synonymous_variant', 'stop_gained', 'frameshift_variant','inframe_deletion','inframe_insertion'])]\n",
    "df_TCGA = df_TCGA.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tumor_Sample_Barcode</th>\n",
       "      <th>Hugo_Symbol</th>\n",
       "      <th>Consequence</th>\n",
       "      <th>HGVSp_Short</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H050150</td>\n",
       "      <td>ATP4B</td>\n",
       "      <td>missense_variant</td>\n",
       "      <td>p.C152F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H050150</td>\n",
       "      <td>GNAT2</td>\n",
       "      <td>missense_variant</td>\n",
       "      <td>p.A111T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H050150</td>\n",
       "      <td>CLDN25</td>\n",
       "      <td>missense_variant</td>\n",
       "      <td>p.A200T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H050150</td>\n",
       "      <td>RADIL</td>\n",
       "      <td>missense_variant</td>\n",
       "      <td>p.A437T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H050150</td>\n",
       "      <td>CTNND2</td>\n",
       "      <td>missense_variant</td>\n",
       "      <td>p.S611T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45434</th>\n",
       "      <td>TCGA-2Y-A9GW-01A-11D-A382-10</td>\n",
       "      <td>CD79A</td>\n",
       "      <td>synonymous_variant</td>\n",
       "      <td>p.T146T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45435</th>\n",
       "      <td>TCGA-2Y-A9GW-01A-11D-A382-10</td>\n",
       "      <td>LILRA4</td>\n",
       "      <td>synonymous_variant</td>\n",
       "      <td>p.L464L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45436</th>\n",
       "      <td>TCGA-2Y-A9GW-01A-11D-A382-10</td>\n",
       "      <td>ZNF772</td>\n",
       "      <td>missense_variant</td>\n",
       "      <td>p.E117G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45438</th>\n",
       "      <td>TCGA-2Y-A9GW-01A-11D-A382-10</td>\n",
       "      <td>SHANK3</td>\n",
       "      <td>synonymous_variant</td>\n",
       "      <td>p.G183G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45439</th>\n",
       "      <td>TCGA-2Y-A9GW-01A-11D-A382-10</td>\n",
       "      <td>PRKX</td>\n",
       "      <td>synonymous_variant</td>\n",
       "      <td>p.L179L</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>129175 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Tumor_Sample_Barcode Hugo_Symbol         Consequence  \\\n",
       "0                           H050150       ATP4B    missense_variant   \n",
       "1                           H050150       GNAT2    missense_variant   \n",
       "2                           H050150      CLDN25    missense_variant   \n",
       "3                           H050150       RADIL    missense_variant   \n",
       "4                           H050150      CTNND2    missense_variant   \n",
       "...                             ...         ...                 ...   \n",
       "45434  TCGA-2Y-A9GW-01A-11D-A382-10       CD79A  synonymous_variant   \n",
       "45435  TCGA-2Y-A9GW-01A-11D-A382-10      LILRA4  synonymous_variant   \n",
       "45436  TCGA-2Y-A9GW-01A-11D-A382-10      ZNF772    missense_variant   \n",
       "45438  TCGA-2Y-A9GW-01A-11D-A382-10      SHANK3  synonymous_variant   \n",
       "45439  TCGA-2Y-A9GW-01A-11D-A382-10        PRKX  synonymous_variant   \n",
       "\n",
       "      HGVSp_Short  \n",
       "0         p.C152F  \n",
       "1         p.A111T  \n",
       "2         p.A200T  \n",
       "3         p.A437T  \n",
       "4         p.S611T  \n",
       "...           ...  \n",
       "45434     p.T146T  \n",
       "45435     p.L464L  \n",
       "45436     p.E117G  \n",
       "45438     p.G183G  \n",
       "45439     p.L179L  \n",
       "\n",
       "[129175 rows x 4 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_TCGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "8QbG-dw0tQRC",
    "outputId": "40ede22b-6f10-442d-d61c-c99cb11f4d79",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 986/986 [08:10<00:00,  2.01it/s]\n"
     ]
    }
   ],
   "source": [
    "lst = train.drop(columns=['ID','SUBCLASS']).columns\n",
    "train_col_list = list(lst)\n",
    "TCGA = df_TCGA[df_TCGA['Hugo_Symbol'].isin(train_col_list)]\n",
    "tcga_data = TCGA[['Tumor_Sample_Barcode','Hugo_Symbol','HGVSp_Short']].dropna()\n",
    "\n",
    "tcga_data['ID'] = tcga_data['Tumor_Sample_Barcode']\n",
    "tcga_data['HGVSp_Short'] = tcga_data['HGVSp_Short'].str[2:]\n",
    "tcga_data['SUBCLASS'] = 'LIHC'\n",
    "tcga_data = tcga_data.drop(columns=['Tumor_Sample_Barcode'])\n",
    "\n",
    "ID_unique = tcga_data['ID'].unique()\n",
    "\n",
    "for idx in tqdm(ID_unique):\n",
    "    data = tcga_data[tcga_data['ID']==idx].groupby('Hugo_Symbol')['HGVSp_Short'].agg(lambda x: ' '.join(x)).reset_index()\n",
    "    data['HGVSp_Short'] = data['HGVSp_Short'].apply(lambda x: ' '.join(sorted(set(x.split()), key=x.split().index)))\n",
    "\n",
    "    new_row = pd.Series(['WT'] * train.shape[1], index=train.columns)\n",
    "    new_row['ID'] = 'ID'\n",
    "    new_row['SUBCLASS'] = 'LIHC'\n",
    "\n",
    "        # Hugo_Symbol과 base_series의 인덱스 이름이 같으면 WT 값을 HGVSp_Short로 변경\n",
    "    for index, row in data.iterrows():\n",
    "        hugo_symbol = row['Hugo_Symbol']\n",
    "        mutation = row['HGVSp_Short']\n",
    "\n",
    "        # new_row의 인덱스 중 Hugo_Symbol과 일치하는 항목을 찾고 'WT'를 변이 값으로 교체\n",
    "        if hugo_symbol in new_row.index:\n",
    "            new_row[hugo_symbol] = mutation\n",
    "\n",
    "    train = pd.concat([train, pd.DataFrame(new_row).T], axis=0)\n",
    "\n",
    "train = train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FSXNGUYUtQRC"
   },
   "source": [
    "# additional data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dXpA5OqatQRC"
   },
   "source": [
    "# LUAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "1PBlnYT_tQRD",
    "outputId": "7c99d200-b28b-4127-bd77-c45f289a0bec"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\888325987.py:18: DtypeWarning: Columns (4,15,55) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParserError encountered with skiprows=0, retrying with skiprows=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\888325987.py:18: DtypeWarning: Columns (131) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParserError encountered with skiprows=0, retrying with skiprows=2\n",
      "ParserError encountered with skiprows=0, retrying with skiprows=2\n",
      "ParserError encountered with skiprows=0, retrying with skiprows=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\888325987.py:18: DtypeWarning: Columns (50,57,59,61,71,95,96) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParserError encountered with skiprows=0, retrying with skiprows=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\888325987.py:18: DtypeWarning: Columns (38,39) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n",
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\888325987.py:18: DtypeWarning: Columns (65,207) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "\n",
    "# KIRC 폴더에 있는 모든 .tar.gz 파일을 처리\n",
    "folder_path = 'additional_data_2/LUAD'\n",
    "gz_files = [f for f in os.listdir(folder_path) if f.endswith('.tar.gz')]\n",
    "\n",
    "# 데이터를 저장할 리스트\n",
    "df_list = []\n",
    "\n",
    "# DataFrame을 로드하는 함수 (skiprows 조정 기능 포함)\n",
    "def load_dataframe(file, sep='\\t'):\n",
    "    skiprows = 0  # 처음에는 skiprows=0으로 시도\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n",
    "            # 첫 번째 열 이름이 '#version'으로 시작하는지 확인\n",
    "            if df.columns[0].startswith('#'):\n",
    "                print(f\"'#version' detected with skiprows={skiprows}, retrying with skiprows={skiprows+1}\")\n",
    "                skiprows += 1  # skiprows 값을 1 증가시키고 다시 시도\n",
    "                file.seek(0)  # 파일 포인터를 다시 시작점으로 이동\n",
    "            else:\n",
    "                break  # '#version'이 없으면 루프 탈출\n",
    "        except pd.errors.ParserError:\n",
    "            if skiprows == 0:\n",
    "                print(f\"ParserError encountered with skiprows=0, retrying with skiprows=2\")\n",
    "                skiprows = 2  # skiprows를 2로 설정하고 다시 시도\n",
    "                file.seek(0)  # 파일 포인터를 다시 시작점으로 이동\n",
    "            else:\n",
    "                print(f\"ParserError encountered again with skiprows={skiprows}. Skipping file.\")\n",
    "                return None  # 에러가 발생하면 None 반환\n",
    "    return df\n",
    "\n",
    "# 각 tar.gz 파일을 순차적으로 처리\n",
    "for gz_file in gz_files:\n",
    "    file_path = os.path.join(folder_path, gz_file)\n",
    "    \n",
    "    with tarfile.open(file_path, 'r:gz') as tar:\n",
    "        # 압축 파일 내부의 파일 목록을 확인\n",
    "        file_names = tar.getnames()\n",
    "        \n",
    "        # 우리가 필요한 파일 경로를 찾아서 추출\n",
    "        for file_name in file_names:\n",
    "            if 'data_mutations.txt' in file_name:  # 필요한 파일의 경로를 찾음\n",
    "                file = tar.extractfile(file_name)\n",
    "                \n",
    "                # DataFrame을 로드하는 함수 호출\n",
    "                df = load_dataframe(file, sep='\\t')\n",
    "                \n",
    "                # df가 None이 아닌 경우에만 계속 처리\n",
    "                if df is not None:\n",
    "                    # Amino_Acid_Change 열이 있는지 확인하고, 있으면 이름 변경\n",
    "                    \n",
    "                    if 'Amino_Acid_Change' in df.columns and 'HGVSp_Short' in df.columns:\n",
    "                        df.drop(columns=['Amino_Acid_Change'], inplace=True)\n",
    "                        \n",
    "                    if 'Amino_Acid_Change' in df.columns:\n",
    "                        df.rename(columns={'Amino_Acid_Change': 'HGVSp_Short'}, inplace=True)\n",
    "                    \n",
    "                    # 필요한 열만 선택\n",
    "                    df = df[['Tumor_Sample_Barcode', 'Hugo_Symbol', 'Consequence', 'HGVSp_Short']]\n",
    "                    \n",
    "                    # df_list에 추가\n",
    "                    df_list.append(df)\n",
    "                break  # 필요한 파일만 처리하므로 루프 탈출\n",
    "\n",
    "                \n",
    "df_TCGA = pd.concat(df_list, ignore_index=True)\n",
    "###################################\n",
    "# .maf.gz 파일을 불러오기\n",
    "file_path_1 = 'additional_data/LUAD.gz'\n",
    "\n",
    "# 파일을 pandas의 DataFrame으로 읽기\n",
    "with gzip.open(file_path_1, 'rt') as f:\n",
    "    df_TCGA_2 = pd.read_csv(f, sep='\\t', comment='#')  # 주석(#) 무시하고 탭으로 구분\n",
    "    \n",
    "df_TCGA_2= df_TCGA_2[['Tumor_Sample_Barcode','Hugo_Symbol','Consequence','HGVSp_Short']]\n",
    "df_TCGA = pd.concat([df_TCGA,df_TCGA_2])\n",
    "#####################################\n",
    "df_TCGA = df_TCGA.dropna(subset='HGVSp_Short')\n",
    "# 'frameshift_variant'이고, 'HGVSp_Short' 열에 'fs'가 없는 값들을 'fs'를 추가하여 수정합니다.\n",
    "df_TCGA.loc[(df_TCGA['Consequence'] == 'frameshift_variant') & ~df_TCGA['HGVSp_Short'].str.contains('fs'), 'HGVSp_Short'] = \\\n",
    "    df_TCGA.loc[(df_TCGA['Consequence'] == 'frameshift_variant') & ~df_TCGA['HGVSp_Short'].str.contains('fs'), 'HGVSp_Short'].str.replace(r'(\\*)$', r'fs\\1', regex=True)\n",
    "\n",
    "# 'Variant_Classification'이 \"Silent\"인 행의 'HGVSp_Short' 열 값을 수정합니다.\n",
    "df_TCGA.loc[df_TCGA['Consequence'] == \"synonymous_variant\", 'HGVSp_Short'] = \\\n",
    "    df_TCGA.loc[df_TCGA['Consequence'] == \"synonymous_variant\", 'HGVSp_Short'].str.replace(r'(\\w)(\\d+)=', r'\\1\\2\\1', regex=True)\n",
    "\n",
    "df_TCGA.loc[df_TCGA['Consequence'] == 'frameshift_variant', 'HGVSp_Short'] = df_TCGA.loc[df_TCGA['Consequence'] == 'frameshift_variant', 'HGVSp_Short'].str.split('*').str[0]\n",
    "df_TCGA = df_TCGA[df_TCGA['Consequence'].isin(['missense_variant', 'synonymous_variant', 'stop_gained', 'frameshift_variant','inframe_deletion','inframe_insertion'])]\n",
    "df_TCGA = df_TCGA.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "bcsG8LA-tQRD",
    "outputId": "e67e16dd-829a-4758-bdfd-73ab405a88e6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3654/3654 [33:56<00:00,  1.79it/s]\n"
     ]
    }
   ],
   "source": [
    "lst = train.drop(columns=['ID','SUBCLASS']).columns\n",
    "train_col_list = list(lst)\n",
    "TCGA = df_TCGA[df_TCGA['Hugo_Symbol'].isin(train_col_list)]\n",
    "tcga_data = TCGA[['Tumor_Sample_Barcode','Hugo_Symbol','HGVSp_Short']].dropna()\n",
    "\n",
    "tcga_data['ID'] = tcga_data['Tumor_Sample_Barcode']\n",
    "tcga_data['HGVSp_Short'] = tcga_data['HGVSp_Short'].str[2:]\n",
    "tcga_data['SUBCLASS'] = 'LUAD'\n",
    "tcga_data = tcga_data.drop(columns=['Tumor_Sample_Barcode'])\n",
    "\n",
    "ID_unique = tcga_data['ID'].unique()\n",
    "\n",
    "for idx in tqdm(ID_unique):\n",
    "    data = tcga_data[tcga_data['ID']==idx].groupby('Hugo_Symbol')['HGVSp_Short'].agg(lambda x: ' '.join(x)).reset_index()\n",
    "    data['HGVSp_Short'] = data['HGVSp_Short'].apply(lambda x: ' '.join(sorted(set(x.split()), key=x.split().index)))\n",
    "\n",
    "    new_row = pd.Series(['WT'] * train.shape[1], index=train.columns)\n",
    "    new_row['ID'] = 'ID'\n",
    "    new_row['SUBCLASS'] = 'LUAD'\n",
    "\n",
    "        # Hugo_Symbol과 base_series의 인덱스 이름이 같으면 WT 값을 HGVSp_Short로 변경\n",
    "    for index, row in data.iterrows():\n",
    "        hugo_symbol = row['Hugo_Symbol']\n",
    "        mutation = row['HGVSp_Short']\n",
    "\n",
    "        # new_row의 인덱스 중 Hugo_Symbol과 일치하는 항목을 찾고 'WT'를 변이 값으로 교체\n",
    "        if hugo_symbol in new_row.index:\n",
    "            new_row[hugo_symbol] = mutation\n",
    "\n",
    "    train = pd.concat([train, pd.DataFrame(new_row).T], axis=0)\n",
    "\n",
    "train = train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>SUBCLASS</th>\n",
       "      <th>A2M</th>\n",
       "      <th>AAAS</th>\n",
       "      <th>AADAT</th>\n",
       "      <th>AARS1</th>\n",
       "      <th>ABAT</th>\n",
       "      <th>ABCA1</th>\n",
       "      <th>ABCA2</th>\n",
       "      <th>ABCA3</th>\n",
       "      <th>...</th>\n",
       "      <th>ZNF292</th>\n",
       "      <th>ZNF365</th>\n",
       "      <th>ZNF639</th>\n",
       "      <th>ZNF707</th>\n",
       "      <th>ZNFX1</th>\n",
       "      <th>ZNRF4</th>\n",
       "      <th>ZPBP</th>\n",
       "      <th>ZW10</th>\n",
       "      <th>ZWINT</th>\n",
       "      <th>ZYX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>KIPAN</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>SARC</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0002</td>\n",
       "      <td>SKCM</td>\n",
       "      <td>R895R</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0003</td>\n",
       "      <td>KIRC</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>GBMLGG</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23568</th>\n",
       "      <td>ID</td>\n",
       "      <td>LUAD</td>\n",
       "      <td>G1310W</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>V83V</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23569</th>\n",
       "      <td>ID</td>\n",
       "      <td>LUAD</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23570</th>\n",
       "      <td>ID</td>\n",
       "      <td>LUAD</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23571</th>\n",
       "      <td>ID</td>\n",
       "      <td>LUAD</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>E216K</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23572</th>\n",
       "      <td>ID</td>\n",
       "      <td>LUAD</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>R643Q</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23573 rows × 4386 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               ID SUBCLASS     A2M AAAS AADAT AARS1 ABAT ABCA1  ABCA2 ABCA3  \\\n",
       "0      TRAIN_0000    KIPAN      WT   WT    WT    WT   WT    WT     WT    WT   \n",
       "1      TRAIN_0001     SARC      WT   WT    WT    WT   WT    WT     WT    WT   \n",
       "2      TRAIN_0002     SKCM   R895R   WT    WT    WT   WT    WT     WT    WT   \n",
       "3      TRAIN_0003     KIRC      WT   WT    WT    WT   WT    WT     WT    WT   \n",
       "4      TRAIN_0004   GBMLGG      WT   WT    WT    WT   WT    WT     WT    WT   \n",
       "...           ...      ...     ...  ...   ...   ...  ...   ...    ...   ...   \n",
       "23568          ID     LUAD  G1310W   WT    WT    WT   WT    WT     WT    WT   \n",
       "23569          ID     LUAD      WT   WT    WT    WT   WT    WT     WT    WT   \n",
       "23570          ID     LUAD      WT   WT    WT    WT   WT    WT     WT    WT   \n",
       "23571          ID     LUAD      WT   WT    WT    WT   WT    WT     WT    WT   \n",
       "23572          ID     LUAD      WT   WT    WT    WT   WT    WT  R643Q    WT   \n",
       "\n",
       "       ... ZNF292 ZNF365 ZNF639 ZNF707 ZNFX1  ZNRF4  ZPBP ZW10 ZWINT ZYX  \n",
       "0      ...     WT     WT     WT     WT    WT     WT    WT   WT    WT  WT  \n",
       "1      ...     WT     WT     WT     WT    WT     WT    WT   WT    WT  WT  \n",
       "2      ...     WT     WT     WT     WT    WT     WT    WT   WT    WT  WT  \n",
       "3      ...     WT     WT     WT     WT    WT     WT    WT   WT    WT  WT  \n",
       "4      ...     WT     WT     WT     WT    WT     WT    WT   WT    WT  WT  \n",
       "...    ...    ...    ...    ...    ...   ...    ...   ...  ...   ...  ..  \n",
       "23568  ...     WT     WT     WT     WT    WT     WT  V83V   WT    WT  WT  \n",
       "23569  ...     WT     WT     WT     WT    WT     WT    WT   WT    WT  WT  \n",
       "23570  ...     WT     WT     WT     WT    WT     WT    WT   WT    WT  WT  \n",
       "23571  ...     WT     WT     WT     WT    WT  E216K    WT   WT    WT  WT  \n",
       "23572  ...     WT     WT     WT     WT    WT     WT    WT   WT    WT  WT  \n",
       "\n",
       "[23573 rows x 4386 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VxXjGwvztQRD"
   },
   "source": [
    "# additional data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NyQj9ZGAtQRD"
   },
   "source": [
    "# LUSC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "-Anv-qMYtQRD",
    "outputId": "b2884c07-464a-4664-cf81-b0b296bbb329"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\1832731999.py:18: DtypeWarning: Columns (1,5,6,15,20,21,22,23,24,25,26,30,31,45,46,47,48,49,50,58,60,61,63,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,88,91,92,95,96,97,98,100,103,106,111,117,118,119,120,121,122,123,124,125,126,127,129) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'#version' detected with skiprows=0, retrying with skiprows=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\1832731999.py:18: DtypeWarning: Columns (1,5,6,15,20,21,22,23,24,25,26,30,31,45,46,47,48,49,50,58,60,61,63,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,88,91,92,95,96,97,98,100,103,106,111,117,118,119,120,121,122,123,124,125,126,127,129) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'#version' detected with skiprows=1, retrying with skiprows=2\n",
      "ParserError encountered with skiprows=0, retrying with skiprows=2\n",
      "ParserError encountered with skiprows=0, retrying with skiprows=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\1832731999.py:18: DtypeWarning: Columns (38,39) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n",
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\1832731999.py:18: DtypeWarning: Columns (1,5,6,20,21,22,23,24,26,30,31,42,44,45,46,47,50,54,55,57,62,67,70,71,72,73,89,95,98,100,101,103,104,108,110,112,113,117,124,126,127,130,136,137,138,144,145,147,149,150,152,155,160,161,165,171,172,178,182,184,191,199,201,214,215,223,228,230,235) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'#version' detected with skiprows=0, retrying with skiprows=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\1832731999.py:18: DtypeWarning: Columns (73,101,124,138,149,165) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n",
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\1832731999.py:77: DtypeWarning: Columns (114) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_TCGA_2 = pd.read_csv(f, sep='\\t', comment='#')  # 주석(#) 무시하고 탭으로 구분\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "\n",
    "# KIRC 폴더에 있는 모든 .tar.gz 파일을 처리\n",
    "folder_path = 'additional_data_2/LUSC'\n",
    "gz_files = [f for f in os.listdir(folder_path) if f.endswith('.tar.gz')]\n",
    "\n",
    "# 데이터를 저장할 리스트\n",
    "df_list = []\n",
    "\n",
    "# DataFrame을 로드하는 함수 (skiprows 조정 기능 포함)\n",
    "def load_dataframe(file, sep='\\t'):\n",
    "    skiprows = 0  # 처음에는 skiprows=0으로 시도\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n",
    "            # 첫 번째 열 이름이 '#version'으로 시작하는지 확인\n",
    "            if df.columns[0].startswith('#'):\n",
    "                print(f\"'#version' detected with skiprows={skiprows}, retrying with skiprows={skiprows+1}\")\n",
    "                skiprows += 1  # skiprows 값을 1 증가시키고 다시 시도\n",
    "                file.seek(0)  # 파일 포인터를 다시 시작점으로 이동\n",
    "            else:\n",
    "                break  # '#version'이 없으면 루프 탈출\n",
    "        except pd.errors.ParserError:\n",
    "            if skiprows == 0:\n",
    "                print(f\"ParserError encountered with skiprows=0, retrying with skiprows=2\")\n",
    "                skiprows = 2  # skiprows를 2로 설정하고 다시 시도\n",
    "                file.seek(0)  # 파일 포인터를 다시 시작점으로 이동\n",
    "            else:\n",
    "                print(f\"ParserError encountered again with skiprows={skiprows}. Skipping file.\")\n",
    "                return None  # 에러가 발생하면 None 반환\n",
    "    return df\n",
    "\n",
    "# 각 tar.gz 파일을 순차적으로 처리\n",
    "for gz_file in gz_files:\n",
    "    file_path = os.path.join(folder_path, gz_file)\n",
    "    \n",
    "    with tarfile.open(file_path, 'r:gz') as tar:\n",
    "        # 압축 파일 내부의 파일 목록을 확인\n",
    "        file_names = tar.getnames()\n",
    "        \n",
    "        # 우리가 필요한 파일 경로를 찾아서 추출\n",
    "        for file_name in file_names:\n",
    "            if 'data_mutations.txt' in file_name:  # 필요한 파일의 경로를 찾음\n",
    "                file = tar.extractfile(file_name)\n",
    "                \n",
    "                # DataFrame을 로드하는 함수 호출\n",
    "                df = load_dataframe(file, sep='\\t')\n",
    "                \n",
    "                # df가 None이 아닌 경우에만 계속 처리\n",
    "                if df is not None:\n",
    "                    # Amino_Acid_Change 열이 있는지 확인하고, 있으면 이름 변경\n",
    "                    \n",
    "                    if 'Amino_Acid_Change' in df.columns and 'HGVSp_Short' in df.columns:\n",
    "                        df.drop(columns=['Amino_Acid_Change'], inplace=True)\n",
    "                        \n",
    "                    if 'Amino_Acid_Change' in df.columns:\n",
    "                        df.rename(columns={'Amino_Acid_Change': 'HGVSp_Short'}, inplace=True)\n",
    "                    \n",
    "                    # 필요한 열만 선택\n",
    "                    df = df[['Tumor_Sample_Barcode', 'Hugo_Symbol', 'Consequence', 'HGVSp_Short']]\n",
    "                    \n",
    "                    # df_list에 추가\n",
    "                    df_list.append(df)\n",
    "                break  # 필요한 파일만 처리하므로 루프 탈출\n",
    "\n",
    "                \n",
    "df_TCGA = pd.concat(df_list, ignore_index=True)\n",
    "###################################\n",
    "# .maf.gz 파일을 불러오기\n",
    "file_path_1 = 'additional_data/LUSC.gz'\n",
    "\n",
    "# 파일을 pandas의 DataFrame으로 읽기\n",
    "with gzip.open(file_path_1, 'rt') as f:\n",
    "    df_TCGA_2 = pd.read_csv(f, sep='\\t', comment='#')  # 주석(#) 무시하고 탭으로 구분\n",
    "    \n",
    "df_TCGA_2= df_TCGA_2[['Tumor_Sample_Barcode','Hugo_Symbol','Consequence','HGVSp_Short']]\n",
    "df_TCGA = pd.concat([df_TCGA,df_TCGA_2])\n",
    "#####################################\n",
    "df_TCGA = df_TCGA.dropna(subset='HGVSp_Short')\n",
    "# 'frameshift_variant'이고, 'HGVSp_Short' 열에 'fs'가 없는 값들을 'fs'를 추가하여 수정합니다.\n",
    "df_TCGA.loc[(df_TCGA['Consequence'] == 'frameshift_variant') & ~df_TCGA['HGVSp_Short'].str.contains('fs'), 'HGVSp_Short'] = \\\n",
    "    df_TCGA.loc[(df_TCGA['Consequence'] == 'frameshift_variant') & ~df_TCGA['HGVSp_Short'].str.contains('fs'), 'HGVSp_Short'].str.replace(r'(\\*)$', r'fs\\1', regex=True)\n",
    "\n",
    "# 'Variant_Classification'이 \"Silent\"인 행의 'HGVSp_Short' 열 값을 수정합니다.\n",
    "df_TCGA.loc[df_TCGA['Consequence'] == \"synonymous_variant\", 'HGVSp_Short'] = \\\n",
    "    df_TCGA.loc[df_TCGA['Consequence'] == \"synonymous_variant\", 'HGVSp_Short'].str.replace(r'(\\w)(\\d+)=', r'\\1\\2\\1', regex=True)\n",
    "\n",
    "df_TCGA.loc[df_TCGA['Consequence'] == 'frameshift_variant', 'HGVSp_Short'] = df_TCGA.loc[df_TCGA['Consequence'] == 'frameshift_variant', 'HGVSp_Short'].str.split('*').str[0]\n",
    "df_TCGA = df_TCGA[df_TCGA['Consequence'].isin(['missense_variant', 'synonymous_variant', 'stop_gained', 'frameshift_variant','inframe_deletion','inframe_insertion'])]\n",
    "df_TCGA = df_TCGA.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tumor_Sample_Barcode</th>\n",
       "      <th>Hugo_Symbol</th>\n",
       "      <th>Consequence</th>\n",
       "      <th>HGVSp_Short</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C3L-00081</td>\n",
       "      <td>FBN2</td>\n",
       "      <td>frameshift_variant</td>\n",
       "      <td>p.I1644Sfs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C3L-00081</td>\n",
       "      <td>HNF4G</td>\n",
       "      <td>frameshift_variant</td>\n",
       "      <td>p.M314Wfs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C3L-00081</td>\n",
       "      <td>SNRPN</td>\n",
       "      <td>frameshift_variant</td>\n",
       "      <td>p.G102Efs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C3L-00081</td>\n",
       "      <td>EIF2AK4</td>\n",
       "      <td>frameshift_variant</td>\n",
       "      <td>p.V564Yfs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C3L-00081</td>\n",
       "      <td>GSDMA</td>\n",
       "      <td>frameshift_variant</td>\n",
       "      <td>p.G67Afs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172821</th>\n",
       "      <td>TCGA-66-2785-01A-01D-1522-08</td>\n",
       "      <td>TENM1</td>\n",
       "      <td>missense_variant</td>\n",
       "      <td>p.A120G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172822</th>\n",
       "      <td>TCGA-66-2785-01A-01D-1522-08</td>\n",
       "      <td>OCRL</td>\n",
       "      <td>synonymous_variant</td>\n",
       "      <td>p.L209L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172823</th>\n",
       "      <td>TCGA-66-2785-01A-01D-1522-08</td>\n",
       "      <td>MAGEC2</td>\n",
       "      <td>missense_variant</td>\n",
       "      <td>p.G74V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172824</th>\n",
       "      <td>TCGA-66-2785-01A-01D-1522-08</td>\n",
       "      <td>F8</td>\n",
       "      <td>missense_variant</td>\n",
       "      <td>p.E871Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172825</th>\n",
       "      <td>TCGA-66-2785-01A-01D-1522-08</td>\n",
       "      <td>MTCP1</td>\n",
       "      <td>synonymous_variant</td>\n",
       "      <td>p.T37T</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>545338 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Tumor_Sample_Barcode Hugo_Symbol         Consequence  \\\n",
       "0                          C3L-00081        FBN2  frameshift_variant   \n",
       "1                          C3L-00081       HNF4G  frameshift_variant   \n",
       "2                          C3L-00081       SNRPN  frameshift_variant   \n",
       "3                          C3L-00081     EIF2AK4  frameshift_variant   \n",
       "4                          C3L-00081       GSDMA  frameshift_variant   \n",
       "...                              ...         ...                 ...   \n",
       "172821  TCGA-66-2785-01A-01D-1522-08       TENM1    missense_variant   \n",
       "172822  TCGA-66-2785-01A-01D-1522-08        OCRL  synonymous_variant   \n",
       "172823  TCGA-66-2785-01A-01D-1522-08      MAGEC2    missense_variant   \n",
       "172824  TCGA-66-2785-01A-01D-1522-08          F8    missense_variant   \n",
       "172825  TCGA-66-2785-01A-01D-1522-08       MTCP1  synonymous_variant   \n",
       "\n",
       "       HGVSp_Short  \n",
       "0       p.I1644Sfs  \n",
       "1        p.M314Wfs  \n",
       "2        p.G102Efs  \n",
       "3        p.V564Yfs  \n",
       "4         p.G67Afs  \n",
       "...            ...  \n",
       "172821     p.A120G  \n",
       "172822     p.L209L  \n",
       "172823      p.G74V  \n",
       "172824     p.E871Q  \n",
       "172825      p.T37T  \n",
       "\n",
       "[545338 rows x 4 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_TCGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "fQobvI9-tQRD",
    "outputId": "aa47d2de-4fe4-4d2c-9b98-994e9fab102a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1702/1702 [17:38<00:00,  1.61it/s]\n"
     ]
    }
   ],
   "source": [
    "lst = train.drop(columns=['ID','SUBCLASS']).columns\n",
    "train_col_list = list(lst)\n",
    "TCGA = df_TCGA[df_TCGA['Hugo_Symbol'].isin(train_col_list)]\n",
    "tcga_data = TCGA[['Tumor_Sample_Barcode','Hugo_Symbol','HGVSp_Short']].dropna()\n",
    "\n",
    "tcga_data['ID'] = tcga_data['Tumor_Sample_Barcode']\n",
    "tcga_data['HGVSp_Short'] = tcga_data['HGVSp_Short'].str[2:]\n",
    "tcga_data['SUBCLASS'] = 'LUSC'\n",
    "tcga_data = tcga_data.drop(columns=['Tumor_Sample_Barcode'])\n",
    "\n",
    "ID_unique = tcga_data['ID'].unique()\n",
    "\n",
    "for idx in tqdm(ID_unique):\n",
    "    data = tcga_data[tcga_data['ID']==idx].groupby('Hugo_Symbol')['HGVSp_Short'].agg(lambda x: ' '.join(x)).reset_index()\n",
    "    data['HGVSp_Short'] = data['HGVSp_Short'].apply(lambda x: ' '.join(sorted(set(x.split()), key=x.split().index)))\n",
    "\n",
    "    new_row = pd.Series(['WT'] * train.shape[1], index=train.columns)\n",
    "    new_row['ID'] = 'ID'\n",
    "    new_row['SUBCLASS'] = 'LUSC'\n",
    "\n",
    "        # Hugo_Symbol과 base_series의 인덱스 이름이 같으면 WT 값을 HGVSp_Short로 변경\n",
    "    for index, row in data.iterrows():\n",
    "        hugo_symbol = row['Hugo_Symbol']\n",
    "        mutation = row['HGVSp_Short']\n",
    "\n",
    "        # new_row의 인덱스 중 Hugo_Symbol과 일치하는 항목을 찾고 'WT'를 변이 값으로 교체\n",
    "        if hugo_symbol in new_row.index:\n",
    "            new_row[hugo_symbol] = mutation\n",
    "\n",
    "    train = pd.concat([train, pd.DataFrame(new_row).T], axis=0)\n",
    "\n",
    "train = train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KySxVwQctQRD"
   },
   "source": [
    "# additional data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NdHx21kHtQRD"
   },
   "source": [
    "# OV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "IonowOigtQRD",
    "outputId": "fea78d7c-947b-4e19-db80-1adfbe4d5e53"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\1074750026.py:18: DtypeWarning: Columns (1,5,6,20,21,22,23,30,31,33,34,35,36,42,44,45,46,48,55,56,57,59,60,65,66,71,72,73,74,83,91,103,105,106,108,115,119,123,124,126,128) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'#version' detected with skiprows=0, retrying with skiprows=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\1074750026.py:18: DtypeWarning: Columns (31) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "\n",
    "# KIRC 폴더에 있는 모든 .tar.gz 파일을 처리\n",
    "folder_path = 'additional_data_2/OV'\n",
    "gz_files = [f for f in os.listdir(folder_path) if f.endswith('.tar.gz')]\n",
    "\n",
    "# 데이터를 저장할 리스트\n",
    "df_list = []\n",
    "\n",
    "# DataFrame을 로드하는 함수 (skiprows 조정 기능 포함)\n",
    "def load_dataframe(file, sep='\\t'):\n",
    "    skiprows = 0  # 처음에는 skiprows=0으로 시도\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n",
    "            # 첫 번째 열 이름이 '#version'으로 시작하는지 확인\n",
    "            if df.columns[0].startswith('#'):\n",
    "                print(f\"'#version' detected with skiprows={skiprows}, retrying with skiprows={skiprows+1}\")\n",
    "                skiprows += 1  # skiprows 값을 1 증가시키고 다시 시도\n",
    "                file.seek(0)  # 파일 포인터를 다시 시작점으로 이동\n",
    "            else:\n",
    "                break  # '#version'이 없으면 루프 탈출\n",
    "        except pd.errors.ParserError:\n",
    "            if skiprows == 0:\n",
    "                print(f\"ParserError encountered with skiprows=0, retrying with skiprows=2\")\n",
    "                skiprows = 2  # skiprows를 2로 설정하고 다시 시도\n",
    "                file.seek(0)  # 파일 포인터를 다시 시작점으로 이동\n",
    "            else:\n",
    "                print(f\"ParserError encountered again with skiprows={skiprows}. Skipping file.\")\n",
    "                return None  # 에러가 발생하면 None 반환\n",
    "    return df\n",
    "\n",
    "# 각 tar.gz 파일을 순차적으로 처리\n",
    "for gz_file in gz_files:\n",
    "    file_path = os.path.join(folder_path, gz_file)\n",
    "    \n",
    "    with tarfile.open(file_path, 'r:gz') as tar:\n",
    "        # 압축 파일 내부의 파일 목록을 확인\n",
    "        file_names = tar.getnames()\n",
    "        \n",
    "        # 우리가 필요한 파일 경로를 찾아서 추출\n",
    "        for file_name in file_names:\n",
    "            if 'data_mutations.txt' in file_name:  # 필요한 파일의 경로를 찾음\n",
    "                file = tar.extractfile(file_name)\n",
    "                \n",
    "                # DataFrame을 로드하는 함수 호출\n",
    "                df = load_dataframe(file, sep='\\t')\n",
    "                \n",
    "                # df가 None이 아닌 경우에만 계속 처리\n",
    "                if df is not None:\n",
    "                    # Amino_Acid_Change 열이 있는지 확인하고, 있으면 이름 변경\n",
    "                    \n",
    "                    if 'Amino_Acid_Change' in df.columns and 'HGVSp_Short' in df.columns:\n",
    "                        df.drop(columns=['Amino_Acid_Change'], inplace=True)\n",
    "                        \n",
    "                    if 'Amino_Acid_Change' in df.columns:\n",
    "                        df.rename(columns={'Amino_Acid_Change': 'HGVSp_Short'}, inplace=True)\n",
    "                    \n",
    "                    # 필요한 열만 선택\n",
    "                    df = df[['Tumor_Sample_Barcode', 'Hugo_Symbol', 'Consequence', 'HGVSp_Short']]\n",
    "                    \n",
    "                    # df_list에 추가\n",
    "                    df_list.append(df)\n",
    "                break  # 필요한 파일만 처리하므로 루프 탈출\n",
    "\n",
    "                \n",
    "df_TCGA = pd.concat(df_list, ignore_index=True)\n",
    "###################################\n",
    "# .maf.gz 파일을 불러오기\n",
    "file_path_1 = 'additional_data/OV.gz'\n",
    "\n",
    "# 파일을 pandas의 DataFrame으로 읽기\n",
    "with gzip.open(file_path_1, 'rt') as f:\n",
    "    df_TCGA_2 = pd.read_csv(f, sep='\\t', comment='#')  # 주석(#) 무시하고 탭으로 구분\n",
    "    \n",
    "df_TCGA_2= df_TCGA_2[['Tumor_Sample_Barcode','Hugo_Symbol','Consequence','HGVSp_Short']]\n",
    "df_TCGA = pd.concat([df_TCGA,df_TCGA_2])\n",
    "#####################################\n",
    "df_TCGA = df_TCGA.dropna(subset='HGVSp_Short')\n",
    "# 'frameshift_variant'이고, 'HGVSp_Short' 열에 'fs'가 없는 값들을 'fs'를 추가하여 수정합니다.\n",
    "df_TCGA.loc[(df_TCGA['Consequence'] == 'frameshift_variant') & ~df_TCGA['HGVSp_Short'].str.contains('fs'), 'HGVSp_Short'] = \\\n",
    "    df_TCGA.loc[(df_TCGA['Consequence'] == 'frameshift_variant') & ~df_TCGA['HGVSp_Short'].str.contains('fs'), 'HGVSp_Short'].str.replace(r'(\\*)$', r'fs\\1', regex=True)\n",
    "\n",
    "# 'Variant_Classification'이 \"Silent\"인 행의 'HGVSp_Short' 열 값을 수정합니다.\n",
    "df_TCGA.loc[df_TCGA['Consequence'] == \"synonymous_variant\", 'HGVSp_Short'] = \\\n",
    "    df_TCGA.loc[df_TCGA['Consequence'] == \"synonymous_variant\", 'HGVSp_Short'].str.replace(r'(\\w)(\\d+)=', r'\\1\\2\\1', regex=True)\n",
    "\n",
    "df_TCGA.loc[df_TCGA['Consequence'] == 'frameshift_variant', 'HGVSp_Short'] = df_TCGA.loc[df_TCGA['Consequence'] == 'frameshift_variant', 'HGVSp_Short'].str.split('*').str[0]\n",
    "df_TCGA = df_TCGA[df_TCGA['Consequence'].isin(['missense_variant', 'synonymous_variant', 'stop_gained', 'frameshift_variant','inframe_deletion','inframe_insertion'])]\n",
    "df_TCGA = df_TCGA.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "fbv3wKastQRD",
    "outputId": "691e9c7e-3295-4fee-8a3c-14766c7c00f5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 964/964 [10:24<00:00,  1.54it/s]\n"
     ]
    }
   ],
   "source": [
    "lst = train.drop(columns=['ID','SUBCLASS']).columns\n",
    "train_col_list = list(lst)\n",
    "TCGA = df_TCGA[df_TCGA['Hugo_Symbol'].isin(train_col_list)]\n",
    "tcga_data = TCGA[['Tumor_Sample_Barcode','Hugo_Symbol','HGVSp_Short']].dropna()\n",
    "\n",
    "tcga_data['ID'] = tcga_data['Tumor_Sample_Barcode']\n",
    "tcga_data['HGVSp_Short'] = tcga_data['HGVSp_Short'].str[2:]\n",
    "tcga_data['SUBCLASS'] = 'OV'\n",
    "tcga_data = tcga_data.drop(columns=['Tumor_Sample_Barcode'])\n",
    "\n",
    "ID_unique = tcga_data['ID'].unique()\n",
    "\n",
    "for idx in tqdm(ID_unique):\n",
    "    data = tcga_data[tcga_data['ID']==idx].groupby('Hugo_Symbol')['HGVSp_Short'].agg(lambda x: ' '.join(x)).reset_index()\n",
    "    data['HGVSp_Short'] = data['HGVSp_Short'].apply(lambda x: ' '.join(sorted(set(x.split()), key=x.split().index)))\n",
    "\n",
    "    new_row = pd.Series(['WT'] * train.shape[1], index=train.columns)\n",
    "    new_row['ID'] = 'ID'\n",
    "    new_row['SUBCLASS'] = 'OV'\n",
    "\n",
    "        # Hugo_Symbol과 base_series의 인덱스 이름이 같으면 WT 값을 HGVSp_Short로 변경\n",
    "    for index, row in data.iterrows():\n",
    "        hugo_symbol = row['Hugo_Symbol']\n",
    "        mutation = row['HGVSp_Short']\n",
    "\n",
    "        # new_row의 인덱스 중 Hugo_Symbol과 일치하는 항목을 찾고 'WT'를 변이 값으로 교체\n",
    "        if hugo_symbol in new_row.index:\n",
    "            new_row[hugo_symbol] = mutation\n",
    "\n",
    "    train = pd.concat([train, pd.DataFrame(new_row).T], axis=0)\n",
    "\n",
    "train = train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2KPG_BLvtQRD"
   },
   "source": [
    "# additional data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-PNx1TDtQRD"
   },
   "source": [
    "# PAAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "nYKL_chetQRD",
    "outputId": "fad8bbe6-0895-4677-b643-b70117791103"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\1577363837.py:18: DtypeWarning: Columns (4,51,94,135,172,243) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParserError encountered with skiprows=0, retrying with skiprows=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\1577363837.py:18: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n",
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\1577363837.py:77: DtypeWarning: Columns (120) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_TCGA_2 = pd.read_csv(f, sep='\\t', comment='#')  # 주석(#) 무시하고 탭으로 구분\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "\n",
    "# KIRC 폴더에 있는 모든 .tar.gz 파일을 처리\n",
    "folder_path = 'additional_data_2/PAAD'\n",
    "gz_files = [f for f in os.listdir(folder_path) if f.endswith('.tar.gz')]\n",
    "\n",
    "# 데이터를 저장할 리스트\n",
    "df_list = []\n",
    "\n",
    "# DataFrame을 로드하는 함수 (skiprows 조정 기능 포함)\n",
    "def load_dataframe(file, sep='\\t'):\n",
    "    skiprows = 0  # 처음에는 skiprows=0으로 시도\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n",
    "            # 첫 번째 열 이름이 '#version'으로 시작하는지 확인\n",
    "            if df.columns[0].startswith('#'):\n",
    "                print(f\"'#version' detected with skiprows={skiprows}, retrying with skiprows={skiprows+1}\")\n",
    "                skiprows += 1  # skiprows 값을 1 증가시키고 다시 시도\n",
    "                file.seek(0)  # 파일 포인터를 다시 시작점으로 이동\n",
    "            else:\n",
    "                break  # '#version'이 없으면 루프 탈출\n",
    "        except pd.errors.ParserError:\n",
    "            if skiprows == 0:\n",
    "                print(f\"ParserError encountered with skiprows=0, retrying with skiprows=2\")\n",
    "                skiprows = 2  # skiprows를 2로 설정하고 다시 시도\n",
    "                file.seek(0)  # 파일 포인터를 다시 시작점으로 이동\n",
    "            else:\n",
    "                print(f\"ParserError encountered again with skiprows={skiprows}. Skipping file.\")\n",
    "                return None  # 에러가 발생하면 None 반환\n",
    "    return df\n",
    "\n",
    "# 각 tar.gz 파일을 순차적으로 처리\n",
    "for gz_file in gz_files:\n",
    "    file_path = os.path.join(folder_path, gz_file)\n",
    "    \n",
    "    with tarfile.open(file_path, 'r:gz') as tar:\n",
    "        # 압축 파일 내부의 파일 목록을 확인\n",
    "        file_names = tar.getnames()\n",
    "        \n",
    "        # 우리가 필요한 파일 경로를 찾아서 추출\n",
    "        for file_name in file_names:\n",
    "            if 'data_mutations.txt' in file_name:  # 필요한 파일의 경로를 찾음\n",
    "                file = tar.extractfile(file_name)\n",
    "                \n",
    "                # DataFrame을 로드하는 함수 호출\n",
    "                df = load_dataframe(file, sep='\\t')\n",
    "                \n",
    "                # df가 None이 아닌 경우에만 계속 처리\n",
    "                if df is not None:\n",
    "                    # Amino_Acid_Change 열이 있는지 확인하고, 있으면 이름 변경\n",
    "                    \n",
    "                    if 'Amino_Acid_Change' in df.columns and 'HGVSp_Short' in df.columns:\n",
    "                        df.drop(columns=['Amino_Acid_Change'], inplace=True)\n",
    "                        \n",
    "                    if 'Amino_Acid_Change' in df.columns:\n",
    "                        df.rename(columns={'Amino_Acid_Change': 'HGVSp_Short'}, inplace=True)\n",
    "                    \n",
    "                    # 필요한 열만 선택\n",
    "                    df = df[['Tumor_Sample_Barcode', 'Hugo_Symbol', 'Consequence', 'HGVSp_Short']]\n",
    "                    \n",
    "                    # df_list에 추가\n",
    "                    df_list.append(df)\n",
    "                break  # 필요한 파일만 처리하므로 루프 탈출\n",
    "\n",
    "                \n",
    "df_TCGA = pd.concat(df_list, ignore_index=True)\n",
    "###################################\n",
    "# .maf.gz 파일을 불러오기\n",
    "file_path_1 = 'additional_data/PAAD.gz'\n",
    "\n",
    "# 파일을 pandas의 DataFrame으로 읽기\n",
    "with gzip.open(file_path_1, 'rt') as f:\n",
    "    df_TCGA_2 = pd.read_csv(f, sep='\\t', comment='#')  # 주석(#) 무시하고 탭으로 구분\n",
    "    \n",
    "df_TCGA_2= df_TCGA_2[['Tumor_Sample_Barcode','Hugo_Symbol','Consequence','HGVSp_Short']]\n",
    "df_TCGA = pd.concat([df_TCGA,df_TCGA_2])\n",
    "#####################################\n",
    "df_TCGA = df_TCGA.dropna(subset='HGVSp_Short')\n",
    "# 'frameshift_variant'이고, 'HGVSp_Short' 열에 'fs'가 없는 값들을 'fs'를 추가하여 수정합니다.\n",
    "df_TCGA.loc[(df_TCGA['Consequence'] == 'frameshift_variant') & ~df_TCGA['HGVSp_Short'].str.contains('fs'), 'HGVSp_Short'] = \\\n",
    "    df_TCGA.loc[(df_TCGA['Consequence'] == 'frameshift_variant') & ~df_TCGA['HGVSp_Short'].str.contains('fs'), 'HGVSp_Short'].str.replace(r'(\\*)$', r'fs\\1', regex=True)\n",
    "\n",
    "# 'Variant_Classification'이 \"Silent\"인 행의 'HGVSp_Short' 열 값을 수정합니다.\n",
    "df_TCGA.loc[df_TCGA['Consequence'] == \"synonymous_variant\", 'HGVSp_Short'] = \\\n",
    "    df_TCGA.loc[df_TCGA['Consequence'] == \"synonymous_variant\", 'HGVSp_Short'].str.replace(r'(\\w)(\\d+)=', r'\\1\\2\\1', regex=True)\n",
    "\n",
    "df_TCGA.loc[df_TCGA['Consequence'] == 'frameshift_variant', 'HGVSp_Short'] = df_TCGA.loc[df_TCGA['Consequence'] == 'frameshift_variant', 'HGVSp_Short'].str.split('*').str[0]\n",
    "df_TCGA = df_TCGA[df_TCGA['Consequence'].isin(['missense_variant', 'synonymous_variant', 'stop_gained', 'frameshift_variant','inframe_deletion','inframe_insertion'])]\n",
    "df_TCGA = df_TCGA.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "z78o198btQRD",
    "outputId": "d92994e9-7c8c-4401-ab37-56f76ddb5420",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 960/960 [10:43<00:00,  1.49it/s]\n"
     ]
    }
   ],
   "source": [
    "lst = train.drop(columns=['ID','SUBCLASS']).columns\n",
    "train_col_list = list(lst)\n",
    "TCGA = df_TCGA[df_TCGA['Hugo_Symbol'].isin(train_col_list)]\n",
    "tcga_data = TCGA[['Tumor_Sample_Barcode','Hugo_Symbol','HGVSp_Short']].dropna()\n",
    "\n",
    "tcga_data['ID'] = tcga_data['Tumor_Sample_Barcode']\n",
    "tcga_data['HGVSp_Short'] = tcga_data['HGVSp_Short'].str[2:]\n",
    "tcga_data['SUBCLASS'] = 'PAAD'\n",
    "tcga_data = tcga_data.drop(columns=['Tumor_Sample_Barcode'])\n",
    "\n",
    "ID_unique = tcga_data['ID'].unique()\n",
    "\n",
    "for idx in tqdm(ID_unique):\n",
    "    data = tcga_data[tcga_data['ID']==idx].groupby('Hugo_Symbol')['HGVSp_Short'].agg(lambda x: ' '.join(x)).reset_index()\n",
    "    data['HGVSp_Short'] = data['HGVSp_Short'].apply(lambda x: ' '.join(sorted(set(x.split()), key=x.split().index)))\n",
    "\n",
    "    new_row = pd.Series(['WT'] * train.shape[1], index=train.columns)\n",
    "    new_row['ID'] = 'ID'\n",
    "    new_row['SUBCLASS'] = 'PAAD'\n",
    "\n",
    "        # Hugo_Symbol과 base_series의 인덱스 이름이 같으면 WT 값을 HGVSp_Short로 변경\n",
    "    for index, row in data.iterrows():\n",
    "        hugo_symbol = row['Hugo_Symbol']\n",
    "        mutation = row['HGVSp_Short']\n",
    "\n",
    "        # new_row의 인덱스 중 Hugo_Symbol과 일치하는 항목을 찾고 'WT'를 변이 값으로 교체\n",
    "        if hugo_symbol in new_row.index:\n",
    "            new_row[hugo_symbol] = mutation\n",
    "\n",
    "    train = pd.concat([train, pd.DataFrame(new_row).T], axis=0)\n",
    "\n",
    "train = train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPd0_MQMtQRD"
   },
   "source": [
    "# additional data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qrnM-Am4tQRD"
   },
   "source": [
    "# PCPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "xqiXmis7tQRD"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\2915514043.py:18: DtypeWarning: Columns (63,224) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "\n",
    "# KIRC 폴더에 있는 모든 .tar.gz 파일을 처리\n",
    "folder_path = 'additional_data_2/PCPG'\n",
    "gz_files = [f for f in os.listdir(folder_path) if f.endswith('.tar.gz')]\n",
    "\n",
    "# 데이터를 저장할 리스트\n",
    "df_list = []\n",
    "\n",
    "# DataFrame을 로드하는 함수 (skiprows 조정 기능 포함)\n",
    "def load_dataframe(file, sep='\\t'):\n",
    "    skiprows = 0  # 처음에는 skiprows=0으로 시도\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n",
    "            # 첫 번째 열 이름이 '#version'으로 시작하는지 확인\n",
    "            if df.columns[0].startswith('#'):\n",
    "                print(f\"'#version' detected with skiprows={skiprows}, retrying with skiprows={skiprows+1}\")\n",
    "                skiprows += 1  # skiprows 값을 1 증가시키고 다시 시도\n",
    "                file.seek(0)  # 파일 포인터를 다시 시작점으로 이동\n",
    "            else:\n",
    "                break  # '#version'이 없으면 루프 탈출\n",
    "        except pd.errors.ParserError:\n",
    "            if skiprows == 0:\n",
    "                print(f\"ParserError encountered with skiprows=0, retrying with skiprows=2\")\n",
    "                skiprows = 2  # skiprows를 2로 설정하고 다시 시도\n",
    "                file.seek(0)  # 파일 포인터를 다시 시작점으로 이동\n",
    "            else:\n",
    "                print(f\"ParserError encountered again with skiprows={skiprows}. Skipping file.\")\n",
    "                return None  # 에러가 발생하면 None 반환\n",
    "    return df\n",
    "\n",
    "# 각 tar.gz 파일을 순차적으로 처리\n",
    "for gz_file in gz_files:\n",
    "    file_path = os.path.join(folder_path, gz_file)\n",
    "    \n",
    "    with tarfile.open(file_path, 'r:gz') as tar:\n",
    "        # 압축 파일 내부의 파일 목록을 확인\n",
    "        file_names = tar.getnames()\n",
    "        \n",
    "        # 우리가 필요한 파일 경로를 찾아서 추출\n",
    "        for file_name in file_names:\n",
    "            if 'data_mutations.txt' in file_name:  # 필요한 파일의 경로를 찾음\n",
    "                file = tar.extractfile(file_name)\n",
    "                \n",
    "                # DataFrame을 로드하는 함수 호출\n",
    "                df = load_dataframe(file, sep='\\t')\n",
    "                \n",
    "                # df가 None이 아닌 경우에만 계속 처리\n",
    "                if df is not None:\n",
    "                    # Amino_Acid_Change 열이 있는지 확인하고, 있으면 이름 변경\n",
    "                    \n",
    "                    if 'Amino_Acid_Change' in df.columns and 'HGVSp_Short' in df.columns:\n",
    "                        df.drop(columns=['Amino_Acid_Change'], inplace=True)\n",
    "                        \n",
    "                    if 'Amino_Acid_Change' in df.columns:\n",
    "                        df.rename(columns={'Amino_Acid_Change': 'HGVSp_Short'}, inplace=True)\n",
    "                    \n",
    "                    # 필요한 열만 선택\n",
    "                    df = df[['Tumor_Sample_Barcode', 'Hugo_Symbol', 'Consequence', 'HGVSp_Short']]\n",
    "                    \n",
    "                    # df_list에 추가\n",
    "                    df_list.append(df)\n",
    "                break  # 필요한 파일만 처리하므로 루프 탈출\n",
    "\n",
    "                \n",
    "df_TCGA = pd.concat(df_list, ignore_index=True)\n",
    "###################################\n",
    "# .maf.gz 파일을 불러오기\n",
    "file_path_1 = 'additional_data/PCPG.gz'\n",
    "\n",
    "# 파일을 pandas의 DataFrame으로 읽기\n",
    "with gzip.open(file_path_1, 'rt') as f:\n",
    "    df_TCGA_2 = pd.read_csv(f, sep='\\t', comment='#')  # 주석(#) 무시하고 탭으로 구분\n",
    "    \n",
    "df_TCGA_2= df_TCGA_2[['Tumor_Sample_Barcode','Hugo_Symbol','Consequence','HGVSp_Short']]\n",
    "df_TCGA = pd.concat([df_TCGA,df_TCGA_2])\n",
    "#####################################\n",
    "df_TCGA = df_TCGA.dropna(subset='HGVSp_Short')\n",
    "# 'frameshift_variant'이고, 'HGVSp_Short' 열에 'fs'가 없는 값들을 'fs'를 추가하여 수정합니다.\n",
    "df_TCGA.loc[(df_TCGA['Consequence'] == 'frameshift_variant') & ~df_TCGA['HGVSp_Short'].str.contains('fs'), 'HGVSp_Short'] = \\\n",
    "    df_TCGA.loc[(df_TCGA['Consequence'] == 'frameshift_variant') & ~df_TCGA['HGVSp_Short'].str.contains('fs'), 'HGVSp_Short'].str.replace(r'(\\*)$', r'fs\\1', regex=True)\n",
    "\n",
    "# 'Variant_Classification'이 \"Silent\"인 행의 'HGVSp_Short' 열 값을 수정합니다.\n",
    "df_TCGA.loc[df_TCGA['Consequence'] == \"synonymous_variant\", 'HGVSp_Short'] = \\\n",
    "    df_TCGA.loc[df_TCGA['Consequence'] == \"synonymous_variant\", 'HGVSp_Short'].str.replace(r'(\\w)(\\d+)=', r'\\1\\2\\1', regex=True)\n",
    "\n",
    "df_TCGA.loc[df_TCGA['Consequence'] == 'frameshift_variant', 'HGVSp_Short'] = df_TCGA.loc[df_TCGA['Consequence'] == 'frameshift_variant', 'HGVSp_Short'].str.split('*').str[0]\n",
    "df_TCGA = df_TCGA[df_TCGA['Consequence'].isin(['missense_variant', 'synonymous_variant', 'stop_gained', 'frameshift_variant','inframe_deletion','inframe_insertion'])]\n",
    "df_TCGA = df_TCGA.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "BOOmfW7otQRE",
    "outputId": "7975f4a7-802a-43bb-f353-dd4c8f883ff6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 343/343 [03:51<00:00,  1.48it/s]\n"
     ]
    }
   ],
   "source": [
    "lst = train.drop(columns=['ID','SUBCLASS']).columns\n",
    "train_col_list = list(lst)\n",
    "TCGA = df_TCGA[df_TCGA['Hugo_Symbol'].isin(train_col_list)]\n",
    "tcga_data = TCGA[['Tumor_Sample_Barcode','Hugo_Symbol','HGVSp_Short']].dropna()\n",
    "\n",
    "tcga_data['ID'] = tcga_data['Tumor_Sample_Barcode']\n",
    "tcga_data['HGVSp_Short'] = tcga_data['HGVSp_Short'].str[2:]\n",
    "tcga_data['SUBCLASS'] = 'PCPG'\n",
    "tcga_data = tcga_data.drop(columns=['Tumor_Sample_Barcode'])\n",
    "\n",
    "ID_unique = tcga_data['ID'].unique()\n",
    "\n",
    "for idx in tqdm(ID_unique):\n",
    "    data = tcga_data[tcga_data['ID']==idx].groupby('Hugo_Symbol')['HGVSp_Short'].agg(lambda x: ' '.join(x)).reset_index()\n",
    "    data['HGVSp_Short'] = data['HGVSp_Short'].apply(lambda x: ' '.join(sorted(set(x.split()), key=x.split().index)))\n",
    "\n",
    "    new_row = pd.Series(['WT'] * train.shape[1], index=train.columns)\n",
    "    new_row['ID'] = 'ID'\n",
    "    new_row['SUBCLASS'] = 'PCPG'\n",
    "\n",
    "        # Hugo_Symbol과 base_series의 인덱스 이름이 같으면 WT 값을 HGVSp_Short로 변경\n",
    "    for index, row in data.iterrows():\n",
    "        hugo_symbol = row['Hugo_Symbol']\n",
    "        mutation = row['HGVSp_Short']\n",
    "\n",
    "        # new_row의 인덱스 중 Hugo_Symbol과 일치하는 항목을 찾고 'WT'를 변이 값으로 교체\n",
    "        if hugo_symbol in new_row.index:\n",
    "            new_row[hugo_symbol] = mutation\n",
    "\n",
    "    train = pd.concat([train, pd.DataFrame(new_row).T], axis=0)\n",
    "\n",
    "train = train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dkxmrwRVtQRE"
   },
   "source": [
    "# additional data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FKDqqqdRtQRE"
   },
   "source": [
    "# PRAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "oo3GYCfctQRE",
    "outputId": "fa5316da-71f1-49b9-bb88-7043066b5421"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'#version' detected with skiprows=0, retrying with skiprows=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\1622103289.py:18: DtypeWarning: Columns (103,235) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'#version' detected with skiprows=0, retrying with skiprows=1\n",
      "'#version' detected with skiprows=1, retrying with skiprows=2\n",
      "'#version' detected with skiprows=0, retrying with skiprows=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\1622103289.py:18: DtypeWarning: Columns (100) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n",
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\1622103289.py:18: DtypeWarning: Columns (67) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n",
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\1622103289.py:18: DtypeWarning: Columns (38,39) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n",
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\1622103289.py:18: DtypeWarning: Columns (1,5,6,20,21,22,23,24,25,30,31,33,34,35,36,42,44,45,46,48,53,54,55,56,57,61,64,65,66,67,73,79,87,89,90,92,97,100,104,106,108) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'#version' detected with skiprows=0, retrying with skiprows=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\1622103289.py:77: DtypeWarning: Columns (120) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_TCGA_2 = pd.read_csv(f, sep='\\t', comment='#')  # 주석(#) 무시하고 탭으로 구분\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "\n",
    "# KIRC 폴더에 있는 모든 .tar.gz 파일을 처리\n",
    "folder_path = 'additional_data_2/PRAD'\n",
    "gz_files = [f for f in os.listdir(folder_path) if f.endswith('.tar.gz')]\n",
    "\n",
    "# 데이터를 저장할 리스트\n",
    "df_list = []\n",
    "\n",
    "# DataFrame을 로드하는 함수 (skiprows 조정 기능 포함)\n",
    "def load_dataframe(file, sep='\\t'):\n",
    "    skiprows = 0  # 처음에는 skiprows=0으로 시도\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n",
    "            # 첫 번째 열 이름이 '#version'으로 시작하는지 확인\n",
    "            if df.columns[0].startswith('#'):\n",
    "                print(f\"'#version' detected with skiprows={skiprows}, retrying with skiprows={skiprows+1}\")\n",
    "                skiprows += 1  # skiprows 값을 1 증가시키고 다시 시도\n",
    "                file.seek(0)  # 파일 포인터를 다시 시작점으로 이동\n",
    "            else:\n",
    "                break  # '#version'이 없으면 루프 탈출\n",
    "        except pd.errors.ParserError:\n",
    "            if skiprows == 0:\n",
    "                print(f\"ParserError encountered with skiprows=0, retrying with skiprows=2\")\n",
    "                skiprows = 2  # skiprows를 2로 설정하고 다시 시도\n",
    "                file.seek(0)  # 파일 포인터를 다시 시작점으로 이동\n",
    "            else:\n",
    "                print(f\"ParserError encountered again with skiprows={skiprows}. Skipping file.\")\n",
    "                return None  # 에러가 발생하면 None 반환\n",
    "    return df\n",
    "\n",
    "# 각 tar.gz 파일을 순차적으로 처리\n",
    "for gz_file in gz_files:\n",
    "    file_path = os.path.join(folder_path, gz_file)\n",
    "    \n",
    "    with tarfile.open(file_path, 'r:gz') as tar:\n",
    "        # 압축 파일 내부의 파일 목록을 확인\n",
    "        file_names = tar.getnames()\n",
    "        \n",
    "        # 우리가 필요한 파일 경로를 찾아서 추출\n",
    "        for file_name in file_names:\n",
    "            if 'data_mutations.txt' in file_name:  # 필요한 파일의 경로를 찾음\n",
    "                file = tar.extractfile(file_name)\n",
    "                \n",
    "                # DataFrame을 로드하는 함수 호출\n",
    "                df = load_dataframe(file, sep='\\t')\n",
    "                \n",
    "                # df가 None이 아닌 경우에만 계속 처리\n",
    "                if df is not None:\n",
    "                    # Amino_Acid_Change 열이 있는지 확인하고, 있으면 이름 변경\n",
    "                    \n",
    "                    if 'Amino_Acid_Change' in df.columns and 'HGVSp_Short' in df.columns:\n",
    "                        df.drop(columns=['Amino_Acid_Change'], inplace=True)\n",
    "                        \n",
    "                    if 'Amino_Acid_Change' in df.columns:\n",
    "                        df.rename(columns={'Amino_Acid_Change': 'HGVSp_Short'}, inplace=True)\n",
    "                    \n",
    "                    # 필요한 열만 선택\n",
    "                    df = df[['Tumor_Sample_Barcode', 'Hugo_Symbol', 'Consequence', 'HGVSp_Short']]\n",
    "                    \n",
    "                    # df_list에 추가\n",
    "                    df_list.append(df)\n",
    "                break  # 필요한 파일만 처리하므로 루프 탈출\n",
    "\n",
    "                \n",
    "df_TCGA = pd.concat(df_list, ignore_index=True)\n",
    "###################################\n",
    "# .maf.gz 파일을 불러오기\n",
    "file_path_1 = 'additional_data/PRAD.gz'\n",
    "\n",
    "# 파일을 pandas의 DataFrame으로 읽기\n",
    "with gzip.open(file_path_1, 'rt') as f:\n",
    "    df_TCGA_2 = pd.read_csv(f, sep='\\t', comment='#')  # 주석(#) 무시하고 탭으로 구분\n",
    "    \n",
    "df_TCGA_2= df_TCGA_2[['Tumor_Sample_Barcode','Hugo_Symbol','Consequence','HGVSp_Short']]\n",
    "df_TCGA = pd.concat([df_TCGA,df_TCGA_2])\n",
    "#####################################\n",
    "df_TCGA = df_TCGA.dropna(subset='HGVSp_Short')\n",
    "# 'frameshift_variant'이고, 'HGVSp_Short' 열에 'fs'가 없는 값들을 'fs'를 추가하여 수정합니다.\n",
    "df_TCGA.loc[(df_TCGA['Consequence'] == 'frameshift_variant') & ~df_TCGA['HGVSp_Short'].str.contains('fs'), 'HGVSp_Short'] = \\\n",
    "    df_TCGA.loc[(df_TCGA['Consequence'] == 'frameshift_variant') & ~df_TCGA['HGVSp_Short'].str.contains('fs'), 'HGVSp_Short'].str.replace(r'(\\*)$', r'fs\\1', regex=True)\n",
    "\n",
    "# 'Variant_Classification'이 \"Silent\"인 행의 'HGVSp_Short' 열 값을 수정합니다.\n",
    "df_TCGA.loc[df_TCGA['Consequence'] == \"synonymous_variant\", 'HGVSp_Short'] = \\\n",
    "    df_TCGA.loc[df_TCGA['Consequence'] == \"synonymous_variant\", 'HGVSp_Short'].str.replace(r'(\\w)(\\d+)=', r'\\1\\2\\1', regex=True)\n",
    "\n",
    "df_TCGA.loc[df_TCGA['Consequence'] == 'frameshift_variant', 'HGVSp_Short'] = df_TCGA.loc[df_TCGA['Consequence'] == 'frameshift_variant', 'HGVSp_Short'].str.split('*').str[0]\n",
    "df_TCGA = df_TCGA[df_TCGA['Consequence'].isin(['missense_variant', 'synonymous_variant', 'stop_gained', 'frameshift_variant','inframe_deletion','inframe_insertion'])]\n",
    "df_TCGA = df_TCGA.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "oxFCJK8MtQRE",
    "outputId": "180694b4-a999-46d0-c08e-f6c8f2a712e7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3980/3980 [47:50<00:00,  1.39it/s]\n"
     ]
    }
   ],
   "source": [
    "lst = train.drop(columns=['ID','SUBCLASS']).columns\n",
    "train_col_list = list(lst)\n",
    "TCGA = df_TCGA[df_TCGA['Hugo_Symbol'].isin(train_col_list)]\n",
    "tcga_data = TCGA[['Tumor_Sample_Barcode','Hugo_Symbol','HGVSp_Short']].dropna()\n",
    "\n",
    "tcga_data['ID'] = tcga_data['Tumor_Sample_Barcode']\n",
    "tcga_data['HGVSp_Short'] = tcga_data['HGVSp_Short'].str[2:]\n",
    "tcga_data['SUBCLASS'] = 'PRAD'\n",
    "tcga_data = tcga_data.drop(columns=['Tumor_Sample_Barcode'])\n",
    "\n",
    "ID_unique = tcga_data['ID'].unique()\n",
    "\n",
    "for idx in tqdm(ID_unique):\n",
    "    data = tcga_data[tcga_data['ID']==idx].groupby('Hugo_Symbol')['HGVSp_Short'].agg(lambda x: ' '.join(x)).reset_index()\n",
    "    data['HGVSp_Short'] = data['HGVSp_Short'].apply(lambda x: ' '.join(sorted(set(x.split()), key=x.split().index)))\n",
    "\n",
    "    new_row = pd.Series(['WT'] * train.shape[1], index=train.columns)\n",
    "    new_row['ID'] = 'ID'\n",
    "    new_row['SUBCLASS'] = 'PRAD'\n",
    "\n",
    "        # Hugo_Symbol과 base_series의 인덱스 이름이 같으면 WT 값을 HGVSp_Short로 변경\n",
    "    for index, row in data.iterrows():\n",
    "        hugo_symbol = row['Hugo_Symbol']\n",
    "        mutation = row['HGVSp_Short']\n",
    "\n",
    "        # new_row의 인덱스 중 Hugo_Symbol과 일치하는 항목을 찾고 'WT'를 변이 값으로 교체\n",
    "        if hugo_symbol in new_row.index:\n",
    "            new_row[hugo_symbol] = mutation\n",
    "\n",
    "    train = pd.concat([train, pd.DataFrame(new_row).T], axis=0)\n",
    "\n",
    "train = train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>SUBCLASS</th>\n",
       "      <th>A2M</th>\n",
       "      <th>AAAS</th>\n",
       "      <th>AADAT</th>\n",
       "      <th>AARS1</th>\n",
       "      <th>ABAT</th>\n",
       "      <th>ABCA1</th>\n",
       "      <th>ABCA2</th>\n",
       "      <th>ABCA3</th>\n",
       "      <th>...</th>\n",
       "      <th>ZNF292</th>\n",
       "      <th>ZNF365</th>\n",
       "      <th>ZNF639</th>\n",
       "      <th>ZNF707</th>\n",
       "      <th>ZNFX1</th>\n",
       "      <th>ZNRF4</th>\n",
       "      <th>ZPBP</th>\n",
       "      <th>ZW10</th>\n",
       "      <th>ZWINT</th>\n",
       "      <th>ZYX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>KIPAN</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>SARC</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0002</td>\n",
       "      <td>SKCM</td>\n",
       "      <td>R895R</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0003</td>\n",
       "      <td>KIRC</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>GBMLGG</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31517</th>\n",
       "      <td>ID</td>\n",
       "      <td>PRAD</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31518</th>\n",
       "      <td>ID</td>\n",
       "      <td>PRAD</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31519</th>\n",
       "      <td>ID</td>\n",
       "      <td>PRAD</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31520</th>\n",
       "      <td>ID</td>\n",
       "      <td>PRAD</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31521</th>\n",
       "      <td>ID</td>\n",
       "      <td>PRAD</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>L1640P</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31522 rows × 4386 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               ID SUBCLASS    A2M AAAS AADAT AARS1 ABAT ABCA1 ABCA2 ABCA3  \\\n",
       "0      TRAIN_0000    KIPAN     WT   WT    WT    WT   WT    WT    WT    WT   \n",
       "1      TRAIN_0001     SARC     WT   WT    WT    WT   WT    WT    WT    WT   \n",
       "2      TRAIN_0002     SKCM  R895R   WT    WT    WT   WT    WT    WT    WT   \n",
       "3      TRAIN_0003     KIRC     WT   WT    WT    WT   WT    WT    WT    WT   \n",
       "4      TRAIN_0004   GBMLGG     WT   WT    WT    WT   WT    WT    WT    WT   \n",
       "...           ...      ...    ...  ...   ...   ...  ...   ...   ...   ...   \n",
       "31517          ID     PRAD     WT   WT    WT    WT   WT    WT    WT    WT   \n",
       "31518          ID     PRAD     WT   WT    WT    WT   WT    WT    WT    WT   \n",
       "31519          ID     PRAD     WT   WT    WT    WT   WT    WT    WT    WT   \n",
       "31520          ID     PRAD     WT   WT    WT    WT   WT    WT    WT    WT   \n",
       "31521          ID     PRAD     WT   WT    WT    WT   WT    WT    WT    WT   \n",
       "\n",
       "       ... ZNF292 ZNF365 ZNF639 ZNF707   ZNFX1 ZNRF4 ZPBP ZW10 ZWINT ZYX  \n",
       "0      ...     WT     WT     WT     WT      WT    WT   WT   WT    WT  WT  \n",
       "1      ...     WT     WT     WT     WT      WT    WT   WT   WT    WT  WT  \n",
       "2      ...     WT     WT     WT     WT      WT    WT   WT   WT    WT  WT  \n",
       "3      ...     WT     WT     WT     WT      WT    WT   WT   WT    WT  WT  \n",
       "4      ...     WT     WT     WT     WT      WT    WT   WT   WT    WT  WT  \n",
       "...    ...    ...    ...    ...    ...     ...   ...  ...  ...   ...  ..  \n",
       "31517  ...     WT     WT     WT     WT      WT    WT   WT   WT    WT  WT  \n",
       "31518  ...     WT     WT     WT     WT      WT    WT   WT   WT    WT  WT  \n",
       "31519  ...     WT     WT     WT     WT      WT    WT   WT   WT    WT  WT  \n",
       "31520  ...     WT     WT     WT     WT      WT    WT   WT   WT    WT  WT  \n",
       "31521  ...     WT     WT     WT     WT  L1640P    WT   WT   WT    WT  WT  \n",
       "\n",
       "[31522 rows x 4386 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z4s1g2kAtQRE"
   },
   "source": [
    "# additional data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ZQ8cRoctQRE"
   },
   "source": [
    "# SARC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "L7jKsro9tQRE",
    "outputId": "7d66bfbb-f59a-4084-dde4-30d194a0bf37"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "\n",
    "# KIRC 폴더에 있는 모든 .tar.gz 파일을 처리\n",
    "folder_path = 'additional_data_2/SARC'\n",
    "gz_files = [f for f in os.listdir(folder_path) if f.endswith('.tar.gz')]\n",
    "\n",
    "# 데이터를 저장할 리스트\n",
    "df_list = []\n",
    "\n",
    "# DataFrame을 로드하는 함수 (skiprows 조정 기능 포함)\n",
    "def load_dataframe(file, sep='\\t'):\n",
    "    skiprows = 0  # 처음에는 skiprows=0으로 시도\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n",
    "            # 첫 번째 열 이름이 '#version'으로 시작하는지 확인\n",
    "            if df.columns[0].startswith('#'):\n",
    "                print(f\"'#version' detected with skiprows={skiprows}, retrying with skiprows={skiprows+1}\")\n",
    "                skiprows += 1  # skiprows 값을 1 증가시키고 다시 시도\n",
    "                file.seek(0)  # 파일 포인터를 다시 시작점으로 이동\n",
    "            else:\n",
    "                break  # '#version'이 없으면 루프 탈출\n",
    "        except pd.errors.ParserError:\n",
    "            if skiprows == 0:\n",
    "                print(f\"ParserError encountered with skiprows=0, retrying with skiprows=2\")\n",
    "                skiprows = 2  # skiprows를 2로 설정하고 다시 시도\n",
    "                file.seek(0)  # 파일 포인터를 다시 시작점으로 이동\n",
    "            else:\n",
    "                print(f\"ParserError encountered again with skiprows={skiprows}. Skipping file.\")\n",
    "                return None  # 에러가 발생하면 None 반환\n",
    "    return df\n",
    "\n",
    "# 각 tar.gz 파일을 순차적으로 처리\n",
    "for gz_file in gz_files:\n",
    "    file_path = os.path.join(folder_path, gz_file)\n",
    "    \n",
    "    with tarfile.open(file_path, 'r:gz') as tar:\n",
    "        # 압축 파일 내부의 파일 목록을 확인\n",
    "        file_names = tar.getnames()\n",
    "        \n",
    "        # 우리가 필요한 파일 경로를 찾아서 추출\n",
    "        for file_name in file_names:\n",
    "            if 'data_mutations.txt' in file_name:  # 필요한 파일의 경로를 찾음\n",
    "                file = tar.extractfile(file_name)\n",
    "                \n",
    "                # DataFrame을 로드하는 함수 호출\n",
    "                df = load_dataframe(file, sep='\\t')\n",
    "                \n",
    "                # df가 None이 아닌 경우에만 계속 처리\n",
    "                if df is not None:\n",
    "                    # Amino_Acid_Change 열이 있는지 확인하고, 있으면 이름 변경\n",
    "                    \n",
    "                    if 'Amino_Acid_Change' in df.columns and 'HGVSp_Short' in df.columns:\n",
    "                        df.drop(columns=['Amino_Acid_Change'], inplace=True)\n",
    "                        \n",
    "                    if 'Amino_Acid_Change' in df.columns:\n",
    "                        df.rename(columns={'Amino_Acid_Change': 'HGVSp_Short'}, inplace=True)\n",
    "                    \n",
    "                    # 필요한 열만 선택\n",
    "                    df = df[['Tumor_Sample_Barcode', 'Hugo_Symbol', 'Consequence', 'HGVSp_Short']]\n",
    "                    \n",
    "                    # df_list에 추가\n",
    "                    df_list.append(df)\n",
    "                break  # 필요한 파일만 처리하므로 루프 탈출\n",
    "\n",
    "                \n",
    "df_TCGA = pd.concat(df_list, ignore_index=True)\n",
    "###################################\n",
    "# .maf.gz 파일을 불러오기\n",
    "file_path_1 = 'additional_data/SARC.gz'\n",
    "\n",
    "# 파일을 pandas의 DataFrame으로 읽기\n",
    "with gzip.open(file_path_1, 'rt') as f:\n",
    "    df_TCGA_2 = pd.read_csv(f, sep='\\t', comment='#')  # 주석(#) 무시하고 탭으로 구분\n",
    "    \n",
    "df_TCGA_2= df_TCGA_2[['Tumor_Sample_Barcode','Hugo_Symbol','Consequence','HGVSp_Short']]\n",
    "df_TCGA = pd.concat([df_TCGA,df_TCGA_2])\n",
    "#####################################\n",
    "df_TCGA = df_TCGA.dropna(subset='HGVSp_Short')\n",
    "# 'frameshift_variant'이고, 'HGVSp_Short' 열에 'fs'가 없는 값들을 'fs'를 추가하여 수정합니다.\n",
    "df_TCGA.loc[(df_TCGA['Consequence'] == 'frameshift_variant') & ~df_TCGA['HGVSp_Short'].str.contains('fs'), 'HGVSp_Short'] = \\\n",
    "    df_TCGA.loc[(df_TCGA['Consequence'] == 'frameshift_variant') & ~df_TCGA['HGVSp_Short'].str.contains('fs'), 'HGVSp_Short'].str.replace(r'(\\*)$', r'fs\\1', regex=True)\n",
    "\n",
    "# 'Variant_Classification'이 \"Silent\"인 행의 'HGVSp_Short' 열 값을 수정합니다.\n",
    "df_TCGA.loc[df_TCGA['Consequence'] == \"synonymous_variant\", 'HGVSp_Short'] = \\\n",
    "    df_TCGA.loc[df_TCGA['Consequence'] == \"synonymous_variant\", 'HGVSp_Short'].str.replace(r'(\\w)(\\d+)=', r'\\1\\2\\1', regex=True)\n",
    "\n",
    "df_TCGA.loc[df_TCGA['Consequence'] == 'frameshift_variant', 'HGVSp_Short'] = df_TCGA.loc[df_TCGA['Consequence'] == 'frameshift_variant', 'HGVSp_Short'].str.split('*').str[0]\n",
    "df_TCGA = df_TCGA[df_TCGA['Consequence'].isin(['missense_variant', 'synonymous_variant', 'stop_gained', 'frameshift_variant','inframe_deletion','inframe_insertion'])]\n",
    "df_TCGA = df_TCGA.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "mYYD4PwatQRE",
    "outputId": "349b8eb8-ebd6-4f6b-fbd2-83416bb8d077",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1803/1803 [23:05<00:00,  1.30it/s]\n"
     ]
    }
   ],
   "source": [
    "lst = train.drop(columns=['ID','SUBCLASS']).columns\n",
    "train_col_list = list(lst)\n",
    "TCGA = df_TCGA[df_TCGA['Hugo_Symbol'].isin(train_col_list)]\n",
    "tcga_data = TCGA[['Tumor_Sample_Barcode','Hugo_Symbol','HGVSp_Short']].dropna()\n",
    "\n",
    "tcga_data['ID'] = tcga_data['Tumor_Sample_Barcode']\n",
    "tcga_data['HGVSp_Short'] = tcga_data['HGVSp_Short'].str[2:]\n",
    "tcga_data['SUBCLASS'] = 'SARC'\n",
    "tcga_data = tcga_data.drop(columns=['Tumor_Sample_Barcode'])\n",
    "\n",
    "ID_unique = tcga_data['ID'].unique()\n",
    "\n",
    "for idx in tqdm(ID_unique):\n",
    "    data = tcga_data[tcga_data['ID']==idx].groupby('Hugo_Symbol')['HGVSp_Short'].agg(lambda x: ' '.join(x)).reset_index()\n",
    "    data['HGVSp_Short'] = data['HGVSp_Short'].apply(lambda x: ' '.join(sorted(set(x.split()), key=x.split().index)))\n",
    "\n",
    "    new_row = pd.Series(['WT'] * train.shape[1], index=train.columns)\n",
    "    new_row['ID'] = 'ID'\n",
    "    new_row['SUBCLASS'] = 'SARC'\n",
    "\n",
    "        # Hugo_Symbol과 base_series의 인덱스 이름이 같으면 WT 값을 HGVSp_Short로 변경\n",
    "    for index, row in data.iterrows():\n",
    "        hugo_symbol = row['Hugo_Symbol']\n",
    "        mutation = row['HGVSp_Short']\n",
    "\n",
    "        # new_row의 인덱스 중 Hugo_Symbol과 일치하는 항목을 찾고 'WT'를 변이 값으로 교체\n",
    "        if hugo_symbol in new_row.index:\n",
    "            new_row[hugo_symbol] = mutation\n",
    "\n",
    "    train = pd.concat([train, pd.DataFrame(new_row).T], axis=0)\n",
    "\n",
    "train = train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rMHN-cHvtQRE"
   },
   "source": [
    "# additional data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LTD9eyQMtQRE"
   },
   "source": [
    "# SKCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "9GLRDEDCtQRE",
    "outputId": "738d6e25-714f-4b38-fb73-6a6042b5b3a0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\4222089456.py:18: DtypeWarning: Columns (1,4,5,6,20,21,22,23,24,25,26,27,28,29,30,31,42,44,45,46,48,53,54,55,56,57,61,64,65,66,67,73,74,80,88,90,91,93,96,99,102,107,110,112,113) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'#version' detected with skiprows=0, retrying with skiprows=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\4222089456.py:18: DtypeWarning: Columns (4,53,64,90,113) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n",
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\4222089456.py:18: DtypeWarning: Columns (46,67,89) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n",
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\4222089456.py:18: DtypeWarning: Columns (4,38,39,43,84) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n",
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\4222089456.py:18: DtypeWarning: Columns (4,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "\n",
    "# KIRC 폴더에 있는 모든 .tar.gz 파일을 처리\n",
    "folder_path = 'additional_data_2/SKCM'\n",
    "gz_files = [f for f in os.listdir(folder_path) if f.endswith('.tar.gz')]\n",
    "\n",
    "# 데이터를 저장할 리스트\n",
    "df_list = []\n",
    "\n",
    "# DataFrame을 로드하는 함수 (skiprows 조정 기능 포함)\n",
    "def load_dataframe(file, sep='\\t'):\n",
    "    skiprows = 0  # 처음에는 skiprows=0으로 시도\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n",
    "            # 첫 번째 열 이름이 '#version'으로 시작하는지 확인\n",
    "            if df.columns[0].startswith('#'):\n",
    "                print(f\"'#version' detected with skiprows={skiprows}, retrying with skiprows={skiprows+1}\")\n",
    "                skiprows += 1  # skiprows 값을 1 증가시키고 다시 시도\n",
    "                file.seek(0)  # 파일 포인터를 다시 시작점으로 이동\n",
    "            else:\n",
    "                break  # '#version'이 없으면 루프 탈출\n",
    "        except pd.errors.ParserError:\n",
    "            if skiprows == 0:\n",
    "                print(f\"ParserError encountered with skiprows=0, retrying with skiprows=2\")\n",
    "                skiprows = 2  # skiprows를 2로 설정하고 다시 시도\n",
    "                file.seek(0)  # 파일 포인터를 다시 시작점으로 이동\n",
    "            else:\n",
    "                print(f\"ParserError encountered again with skiprows={skiprows}. Skipping file.\")\n",
    "                return None  # 에러가 발생하면 None 반환\n",
    "    return df\n",
    "\n",
    "# 각 tar.gz 파일을 순차적으로 처리\n",
    "for gz_file in gz_files:\n",
    "    file_path = os.path.join(folder_path, gz_file)\n",
    "    \n",
    "    with tarfile.open(file_path, 'r:gz') as tar:\n",
    "        # 압축 파일 내부의 파일 목록을 확인\n",
    "        file_names = tar.getnames()\n",
    "        \n",
    "        # 우리가 필요한 파일 경로를 찾아서 추출\n",
    "        for file_name in file_names:\n",
    "            if 'data_mutations.txt' in file_name:  # 필요한 파일의 경로를 찾음\n",
    "                file = tar.extractfile(file_name)\n",
    "                \n",
    "                # DataFrame을 로드하는 함수 호출\n",
    "                df = load_dataframe(file, sep='\\t')\n",
    "                \n",
    "                # df가 None이 아닌 경우에만 계속 처리\n",
    "                if df is not None:\n",
    "                    # Amino_Acid_Change 열이 있는지 확인하고, 있으면 이름 변경\n",
    "                    \n",
    "                    if 'Amino_Acid_Change' in df.columns and 'HGVSp_Short' in df.columns:\n",
    "                        df.drop(columns=['Amino_Acid_Change'], inplace=True)\n",
    "                        \n",
    "                    if 'Amino_Acid_Change' in df.columns:\n",
    "                        df.rename(columns={'Amino_Acid_Change': 'HGVSp_Short'}, inplace=True)\n",
    "                    \n",
    "                    # 필요한 열만 선택\n",
    "                    df = df[['Tumor_Sample_Barcode', 'Hugo_Symbol', 'Consequence', 'HGVSp_Short']]\n",
    "                    \n",
    "                    # df_list에 추가\n",
    "                    df_list.append(df)\n",
    "                break  # 필요한 파일만 처리하므로 루프 탈출\n",
    "\n",
    "                \n",
    "df_TCGA = pd.concat(df_list, ignore_index=True)\n",
    "###################################\n",
    "# .maf.gz 파일을 불러오기\n",
    "file_path_1 = 'additional_data/SKCM.gz'\n",
    "\n",
    "# 파일을 pandas의 DataFrame으로 읽기\n",
    "with gzip.open(file_path_1, 'rt') as f:\n",
    "    df_TCGA_2 = pd.read_csv(f, sep='\\t', comment='#')  # 주석(#) 무시하고 탭으로 구분\n",
    "    \n",
    "df_TCGA_2= df_TCGA_2[['Tumor_Sample_Barcode','Hugo_Symbol','Consequence','HGVSp_Short']]\n",
    "df_TCGA = pd.concat([df_TCGA,df_TCGA_2])\n",
    "#####################################\n",
    "df_TCGA = df_TCGA.dropna(subset='HGVSp_Short')\n",
    "# 'frameshift_variant'이고, 'HGVSp_Short' 열에 'fs'가 없는 값들을 'fs'를 추가하여 수정합니다.\n",
    "df_TCGA.loc[(df_TCGA['Consequence'] == 'frameshift_variant') & ~df_TCGA['HGVSp_Short'].str.contains('fs'), 'HGVSp_Short'] = \\\n",
    "    df_TCGA.loc[(df_TCGA['Consequence'] == 'frameshift_variant') & ~df_TCGA['HGVSp_Short'].str.contains('fs'), 'HGVSp_Short'].str.replace(r'(\\*)$', r'fs\\1', regex=True)\n",
    "\n",
    "# 'Variant_Classification'이 \"Silent\"인 행의 'HGVSp_Short' 열 값을 수정합니다.\n",
    "df_TCGA.loc[df_TCGA['Consequence'] == \"synonymous_variant\", 'HGVSp_Short'] = \\\n",
    "    df_TCGA.loc[df_TCGA['Consequence'] == \"synonymous_variant\", 'HGVSp_Short'].str.replace(r'(\\w)(\\d+)=', r'\\1\\2\\1', regex=True)\n",
    "\n",
    "df_TCGA.loc[df_TCGA['Consequence'] == 'frameshift_variant', 'HGVSp_Short'] = df_TCGA.loc[df_TCGA['Consequence'] == 'frameshift_variant', 'HGVSp_Short'].str.split('*').str[0]\n",
    "df_TCGA = df_TCGA[df_TCGA['Consequence'].isin(['missense_variant', 'synonymous_variant', 'stop_gained', 'frameshift_variant','inframe_deletion','inframe_insertion'])]\n",
    "df_TCGA = df_TCGA.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "eaos5ZAztQRE",
    "outputId": "64b2d4c1-a9fa-4d7d-ff47-fcb929840dca",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1198/1198 [16:29<00:00,  1.21it/s]\n"
     ]
    }
   ],
   "source": [
    "lst = train.drop(columns=['ID','SUBCLASS']).columns\n",
    "train_col_list = list(lst)\n",
    "TCGA = df_TCGA[df_TCGA['Hugo_Symbol'].isin(train_col_list)]\n",
    "tcga_data = TCGA[['Tumor_Sample_Barcode','Hugo_Symbol','HGVSp_Short']].dropna()\n",
    "\n",
    "tcga_data['ID'] = tcga_data['Tumor_Sample_Barcode']\n",
    "tcga_data['HGVSp_Short'] = tcga_data['HGVSp_Short'].str[2:]\n",
    "tcga_data['SUBCLASS'] = 'SKCM'\n",
    "tcga_data = tcga_data.drop(columns=['Tumor_Sample_Barcode'])\n",
    "\n",
    "ID_unique = tcga_data['ID'].unique()\n",
    "\n",
    "for idx in tqdm(ID_unique):\n",
    "    data = tcga_data[tcga_data['ID']==idx].groupby('Hugo_Symbol')['HGVSp_Short'].agg(lambda x: ' '.join(x)).reset_index()\n",
    "    data['HGVSp_Short'] = data['HGVSp_Short'].apply(lambda x: ' '.join(sorted(set(x.split()), key=x.split().index)))\n",
    "\n",
    "    new_row = pd.Series(['WT'] * train.shape[1], index=train.columns)\n",
    "    new_row['ID'] = 'ID'\n",
    "    new_row['SUBCLASS'] = 'SKCM'\n",
    "\n",
    "        # Hugo_Symbol과 base_series의 인덱스 이름이 같으면 WT 값을 HGVSp_Short로 변경\n",
    "    for index, row in data.iterrows():\n",
    "        hugo_symbol = row['Hugo_Symbol']\n",
    "        mutation = row['HGVSp_Short']\n",
    "\n",
    "        # new_row의 인덱스 중 Hugo_Symbol과 일치하는 항목을 찾고 'WT'를 변이 값으로 교체\n",
    "        if hugo_symbol in new_row.index:\n",
    "            new_row[hugo_symbol] = mutation\n",
    "\n",
    "    train = pd.concat([train, pd.DataFrame(new_row).T], axis=0)\n",
    "\n",
    "train = train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>SUBCLASS</th>\n",
       "      <th>A2M</th>\n",
       "      <th>AAAS</th>\n",
       "      <th>AADAT</th>\n",
       "      <th>AARS1</th>\n",
       "      <th>ABAT</th>\n",
       "      <th>ABCA1</th>\n",
       "      <th>ABCA2</th>\n",
       "      <th>ABCA3</th>\n",
       "      <th>...</th>\n",
       "      <th>ZNF292</th>\n",
       "      <th>ZNF365</th>\n",
       "      <th>ZNF639</th>\n",
       "      <th>ZNF707</th>\n",
       "      <th>ZNFX1</th>\n",
       "      <th>ZNRF4</th>\n",
       "      <th>ZPBP</th>\n",
       "      <th>ZW10</th>\n",
       "      <th>ZWINT</th>\n",
       "      <th>ZYX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>KIPAN</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>SARC</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0002</td>\n",
       "      <td>SKCM</td>\n",
       "      <td>R895R</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0003</td>\n",
       "      <td>KIRC</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>GBMLGG</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26754</th>\n",
       "      <td>ID</td>\n",
       "      <td>SKCM</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26755</th>\n",
       "      <td>ID</td>\n",
       "      <td>SKCM</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26756</th>\n",
       "      <td>ID</td>\n",
       "      <td>SKCM</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26757</th>\n",
       "      <td>ID</td>\n",
       "      <td>SKCM</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26758</th>\n",
       "      <td>ID</td>\n",
       "      <td>SKCM</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26759 rows × 4386 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               ID SUBCLASS    A2M AAAS AADAT AARS1 ABAT ABCA1 ABCA2 ABCA3  \\\n",
       "0      TRAIN_0000    KIPAN     WT   WT    WT    WT   WT    WT    WT    WT   \n",
       "1      TRAIN_0001     SARC     WT   WT    WT    WT   WT    WT    WT    WT   \n",
       "2      TRAIN_0002     SKCM  R895R   WT    WT    WT   WT    WT    WT    WT   \n",
       "3      TRAIN_0003     KIRC     WT   WT    WT    WT   WT    WT    WT    WT   \n",
       "4      TRAIN_0004   GBMLGG     WT   WT    WT    WT   WT    WT    WT    WT   \n",
       "...           ...      ...    ...  ...   ...   ...  ...   ...   ...   ...   \n",
       "26754          ID     SKCM     WT   WT    WT    WT   WT    WT    WT    WT   \n",
       "26755          ID     SKCM     WT   WT    WT    WT   WT    WT    WT    WT   \n",
       "26756          ID     SKCM     WT   WT    WT    WT   WT    WT    WT    WT   \n",
       "26757          ID     SKCM     WT   WT    WT    WT   WT    WT    WT    WT   \n",
       "26758          ID     SKCM     WT   WT    WT    WT   WT    WT    WT    WT   \n",
       "\n",
       "       ... ZNF292 ZNF365 ZNF639 ZNF707 ZNFX1 ZNRF4 ZPBP ZW10 ZWINT ZYX  \n",
       "0      ...     WT     WT     WT     WT    WT    WT   WT   WT    WT  WT  \n",
       "1      ...     WT     WT     WT     WT    WT    WT   WT   WT    WT  WT  \n",
       "2      ...     WT     WT     WT     WT    WT    WT   WT   WT    WT  WT  \n",
       "3      ...     WT     WT     WT     WT    WT    WT   WT   WT    WT  WT  \n",
       "4      ...     WT     WT     WT     WT    WT    WT   WT   WT    WT  WT  \n",
       "...    ...    ...    ...    ...    ...   ...   ...  ...  ...   ...  ..  \n",
       "26754  ...     WT     WT     WT     WT    WT    WT   WT   WT    WT  WT  \n",
       "26755  ...     WT     WT     WT     WT    WT    WT   WT   WT    WT  WT  \n",
       "26756  ...     WT     WT     WT     WT    WT    WT   WT   WT    WT  WT  \n",
       "26757  ...     WT     WT     WT     WT    WT    WT   WT   WT    WT  WT  \n",
       "26758  ...     WT     WT     WT     WT    WT    WT   WT   WT    WT  WT  \n",
       "\n",
       "[26759 rows x 4386 columns]"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jE8XWhZltQRE"
   },
   "source": [
    "# additional data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jH58BrPttQRE"
   },
   "source": [
    "# TGCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "UShkHVBetQRE"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "\n",
    "# KIRC 폴더에 있는 모든 .tar.gz 파일을 처리\n",
    "folder_path = 'additional_data_2/TGCT'\n",
    "gz_files = [f for f in os.listdir(folder_path) if f.endswith('.tar.gz')]\n",
    "\n",
    "# 데이터를 저장할 리스트\n",
    "df_list = []\n",
    "\n",
    "# DataFrame을 로드하는 함수 (skiprows 조정 기능 포함)\n",
    "def load_dataframe(file, sep='\\t'):\n",
    "    skiprows = 0  # 처음에는 skiprows=0으로 시도\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n",
    "            # 첫 번째 열 이름이 '#version'으로 시작하는지 확인\n",
    "            if df.columns[0].startswith('#'):\n",
    "                print(f\"'#version' detected with skiprows={skiprows}, retrying with skiprows={skiprows+1}\")\n",
    "                skiprows += 1  # skiprows 값을 1 증가시키고 다시 시도\n",
    "                file.seek(0)  # 파일 포인터를 다시 시작점으로 이동\n",
    "            else:\n",
    "                break  # '#version'이 없으면 루프 탈출\n",
    "        except pd.errors.ParserError:\n",
    "            if skiprows == 0:\n",
    "                print(f\"ParserError encountered with skiprows=0, retrying with skiprows=2\")\n",
    "                skiprows = 2  # skiprows를 2로 설정하고 다시 시도\n",
    "                file.seek(0)  # 파일 포인터를 다시 시작점으로 이동\n",
    "            else:\n",
    "                print(f\"ParserError encountered again with skiprows={skiprows}. Skipping file.\")\n",
    "                return None  # 에러가 발생하면 None 반환\n",
    "    return df\n",
    "\n",
    "# 각 tar.gz 파일을 순차적으로 처리\n",
    "for gz_file in gz_files:\n",
    "    file_path = os.path.join(folder_path, gz_file)\n",
    "    \n",
    "    with tarfile.open(file_path, 'r:gz') as tar:\n",
    "        # 압축 파일 내부의 파일 목록을 확인\n",
    "        file_names = tar.getnames()\n",
    "        \n",
    "        # 우리가 필요한 파일 경로를 찾아서 추출\n",
    "        for file_name in file_names:\n",
    "            if 'data_mutations.txt' in file_name:  # 필요한 파일의 경로를 찾음\n",
    "                file = tar.extractfile(file_name)\n",
    "                \n",
    "                # DataFrame을 로드하는 함수 호출\n",
    "                df = load_dataframe(file, sep='\\t')\n",
    "                \n",
    "                # df가 None이 아닌 경우에만 계속 처리\n",
    "                if df is not None:\n",
    "                    # Amino_Acid_Change 열이 있는지 확인하고, 있으면 이름 변경\n",
    "                    \n",
    "                    if 'Amino_Acid_Change' in df.columns and 'HGVSp_Short' in df.columns:\n",
    "                        df.drop(columns=['Amino_Acid_Change'], inplace=True)\n",
    "                        \n",
    "                    if 'Amino_Acid_Change' in df.columns:\n",
    "                        df.rename(columns={'Amino_Acid_Change': 'HGVSp_Short'}, inplace=True)\n",
    "                    \n",
    "                    # 필요한 열만 선택\n",
    "                    df = df[['Tumor_Sample_Barcode', 'Hugo_Symbol', 'Consequence', 'HGVSp_Short']]\n",
    "                    \n",
    "                    # df_list에 추가\n",
    "                    df_list.append(df)\n",
    "                break  # 필요한 파일만 처리하므로 루프 탈출\n",
    "\n",
    "                \n",
    "df_TCGA = pd.concat(df_list, ignore_index=True)\n",
    "###################################\n",
    "# .maf.gz 파일을 불러오기\n",
    "file_path_1 = 'additional_data/TGCT.gz'\n",
    "\n",
    "# 파일을 pandas의 DataFrame으로 읽기\n",
    "with gzip.open(file_path_1, 'rt') as f:\n",
    "    df_TCGA_2 = pd.read_csv(f, sep='\\t', comment='#')  # 주석(#) 무시하고 탭으로 구분\n",
    "    \n",
    "df_TCGA_2= df_TCGA_2[['Tumor_Sample_Barcode','Hugo_Symbol','Consequence','HGVSp_Short']]\n",
    "df_TCGA = pd.concat([df_TCGA,df_TCGA_2])\n",
    "#####################################\n",
    "df_TCGA = df_TCGA.dropna(subset='HGVSp_Short')\n",
    "# 'frameshift_variant'이고, 'HGVSp_Short' 열에 'fs'가 없는 값들을 'fs'를 추가하여 수정합니다.\n",
    "df_TCGA.loc[(df_TCGA['Consequence'] == 'frameshift_variant') & ~df_TCGA['HGVSp_Short'].str.contains('fs'), 'HGVSp_Short'] = \\\n",
    "    df_TCGA.loc[(df_TCGA['Consequence'] == 'frameshift_variant') & ~df_TCGA['HGVSp_Short'].str.contains('fs'), 'HGVSp_Short'].str.replace(r'(\\*)$', r'fs\\1', regex=True)\n",
    "\n",
    "# 'Variant_Classification'이 \"Silent\"인 행의 'HGVSp_Short' 열 값을 수정합니다.\n",
    "df_TCGA.loc[df_TCGA['Consequence'] == \"synonymous_variant\", 'HGVSp_Short'] = \\\n",
    "    df_TCGA.loc[df_TCGA['Consequence'] == \"synonymous_variant\", 'HGVSp_Short'].str.replace(r'(\\w)(\\d+)=', r'\\1\\2\\1', regex=True)\n",
    "\n",
    "df_TCGA.loc[df_TCGA['Consequence'] == 'frameshift_variant', 'HGVSp_Short'] = df_TCGA.loc[df_TCGA['Consequence'] == 'frameshift_variant', 'HGVSp_Short'].str.split('*').str[0]\n",
    "df_TCGA = df_TCGA[df_TCGA['Consequence'].isin(['missense_variant', 'synonymous_variant', 'stop_gained', 'frameshift_variant','inframe_deletion','inframe_insertion'])]\n",
    "df_TCGA = df_TCGA.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "7BIulgZBtQRE",
    "outputId": "c22902e9-55bb-4527-bd50-4a68ce6dfe87",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 295/295 [04:08<00:00,  1.18it/s]\n"
     ]
    }
   ],
   "source": [
    "lst = train.drop(columns=['ID','SUBCLASS']).columns\n",
    "train_col_list = list(lst)\n",
    "TCGA = df_TCGA[df_TCGA['Hugo_Symbol'].isin(train_col_list)]\n",
    "tcga_data = TCGA[['Tumor_Sample_Barcode','Hugo_Symbol','HGVSp_Short']].dropna()\n",
    "\n",
    "tcga_data['ID'] = tcga_data['Tumor_Sample_Barcode']\n",
    "tcga_data['HGVSp_Short'] = tcga_data['HGVSp_Short'].str[2:]\n",
    "tcga_data['SUBCLASS'] = 'TGCT'\n",
    "tcga_data = tcga_data.drop(columns=['Tumor_Sample_Barcode'])\n",
    "\n",
    "ID_unique = tcga_data['ID'].unique()\n",
    "\n",
    "for idx in tqdm(ID_unique):\n",
    "    data = tcga_data[tcga_data['ID']==idx].groupby('Hugo_Symbol')['HGVSp_Short'].agg(lambda x: ' '.join(x)).reset_index()\n",
    "    data['HGVSp_Short'] = data['HGVSp_Short'].apply(lambda x: ' '.join(sorted(set(x.split()), key=x.split().index)))\n",
    "\n",
    "    new_row = pd.Series(['WT'] * train.shape[1], index=train.columns)\n",
    "    new_row['ID'] = 'ID'\n",
    "    new_row['SUBCLASS'] = 'TGCT'\n",
    "\n",
    "        # Hugo_Symbol과 base_series의 인덱스 이름이 같으면 WT 값을 HGVSp_Short로 변경\n",
    "    for index, row in data.iterrows():\n",
    "        hugo_symbol = row['Hugo_Symbol']\n",
    "        mutation = row['HGVSp_Short']\n",
    "\n",
    "        # new_row의 인덱스 중 Hugo_Symbol과 일치하는 항목을 찾고 'WT'를 변이 값으로 교체\n",
    "        if hugo_symbol in new_row.index:\n",
    "            new_row[hugo_symbol] = mutation\n",
    "\n",
    "    train = pd.concat([train, pd.DataFrame(new_row).T], axis=0)\n",
    "\n",
    "train = train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EkHvR7IitQRE"
   },
   "source": [
    "# additional data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A2tOwN4HtQRE"
   },
   "source": [
    "# THCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "MXuU5dlLtQRF",
    "outputId": "d2c19a0b-ccc2-4b47-8aa7-a512cdc279d2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "\n",
    "# KIRC 폴더에 있는 모든 .tar.gz 파일을 처리\n",
    "folder_path = 'additional_data_2/THCA'\n",
    "gz_files = [f for f in os.listdir(folder_path) if f.endswith('.tar.gz')]\n",
    "\n",
    "# 데이터를 저장할 리스트\n",
    "df_list = []\n",
    "\n",
    "# DataFrame을 로드하는 함수 (skiprows 조정 기능 포함)\n",
    "def load_dataframe(file, sep='\\t'):\n",
    "    skiprows = 0  # 처음에는 skiprows=0으로 시도\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n",
    "            # 첫 번째 열 이름이 '#version'으로 시작하는지 확인\n",
    "            if df.columns[0].startswith('#'):\n",
    "                print(f\"'#version' detected with skiprows={skiprows}, retrying with skiprows={skiprows+1}\")\n",
    "                skiprows += 1  # skiprows 값을 1 증가시키고 다시 시도\n",
    "                file.seek(0)  # 파일 포인터를 다시 시작점으로 이동\n",
    "            else:\n",
    "                break  # '#version'이 없으면 루프 탈출\n",
    "        except pd.errors.ParserError:\n",
    "            if skiprows == 0:\n",
    "                print(f\"ParserError encountered with skiprows=0, retrying with skiprows=2\")\n",
    "                skiprows = 2  # skiprows를 2로 설정하고 다시 시도\n",
    "                file.seek(0)  # 파일 포인터를 다시 시작점으로 이동\n",
    "            else:\n",
    "                print(f\"ParserError encountered again with skiprows={skiprows}. Skipping file.\")\n",
    "                return None  # 에러가 발생하면 None 반환\n",
    "    return df\n",
    "\n",
    "# 각 tar.gz 파일을 순차적으로 처리\n",
    "for gz_file in gz_files:\n",
    "    file_path = os.path.join(folder_path, gz_file)\n",
    "    \n",
    "    with tarfile.open(file_path, 'r:gz') as tar:\n",
    "        # 압축 파일 내부의 파일 목록을 확인\n",
    "        file_names = tar.getnames()\n",
    "        \n",
    "        # 우리가 필요한 파일 경로를 찾아서 추출\n",
    "        for file_name in file_names:\n",
    "            if 'data_mutations.txt' in file_name:  # 필요한 파일의 경로를 찾음\n",
    "                file = tar.extractfile(file_name)\n",
    "                \n",
    "                # DataFrame을 로드하는 함수 호출\n",
    "                df = load_dataframe(file, sep='\\t')\n",
    "                \n",
    "                # df가 None이 아닌 경우에만 계속 처리\n",
    "                if df is not None:\n",
    "                    # Amino_Acid_Change 열이 있는지 확인하고, 있으면 이름 변경\n",
    "                    \n",
    "                    if 'Amino_Acid_Change' in df.columns and 'HGVSp_Short' in df.columns:\n",
    "                        df.drop(columns=['Amino_Acid_Change'], inplace=True)\n",
    "                        \n",
    "                    if 'Amino_Acid_Change' in df.columns:\n",
    "                        df.rename(columns={'Amino_Acid_Change': 'HGVSp_Short'}, inplace=True)\n",
    "                    \n",
    "                    # 필요한 열만 선택\n",
    "                    df = df[['Tumor_Sample_Barcode', 'Hugo_Symbol', 'Consequence', 'HGVSp_Short']]\n",
    "                    \n",
    "                    # df_list에 추가\n",
    "                    df_list.append(df)\n",
    "                break  # 필요한 파일만 처리하므로 루프 탈출\n",
    "\n",
    "                \n",
    "df_TCGA = pd.concat(df_list, ignore_index=True)\n",
    "###################################\n",
    "# .maf.gz 파일을 불러오기\n",
    "file_path_1 = 'additional_data/THCA.gz'\n",
    "\n",
    "# 파일을 pandas의 DataFrame으로 읽기\n",
    "with gzip.open(file_path_1, 'rt') as f:\n",
    "    df_TCGA_2 = pd.read_csv(f, sep='\\t', comment='#')  # 주석(#) 무시하고 탭으로 구분\n",
    "    \n",
    "df_TCGA_2= df_TCGA_2[['Tumor_Sample_Barcode','Hugo_Symbol','Consequence','HGVSp_Short']]\n",
    "df_TCGA = pd.concat([df_TCGA,df_TCGA_2])\n",
    "#####################################\n",
    "df_TCGA = df_TCGA.dropna(subset='HGVSp_Short')\n",
    "# 'frameshift_variant'이고, 'HGVSp_Short' 열에 'fs'가 없는 값들을 'fs'를 추가하여 수정합니다.\n",
    "df_TCGA.loc[(df_TCGA['Consequence'] == 'frameshift_variant') & ~df_TCGA['HGVSp_Short'].str.contains('fs'), 'HGVSp_Short'] = \\\n",
    "    df_TCGA.loc[(df_TCGA['Consequence'] == 'frameshift_variant') & ~df_TCGA['HGVSp_Short'].str.contains('fs'), 'HGVSp_Short'].str.replace(r'(\\*)$', r'fs\\1', regex=True)\n",
    "\n",
    "# 'Variant_Classification'이 \"Silent\"인 행의 'HGVSp_Short' 열 값을 수정합니다.\n",
    "df_TCGA.loc[df_TCGA['Consequence'] == \"synonymous_variant\", 'HGVSp_Short'] = \\\n",
    "    df_TCGA.loc[df_TCGA['Consequence'] == \"synonymous_variant\", 'HGVSp_Short'].str.replace(r'(\\w)(\\d+)=', r'\\1\\2\\1', regex=True)\n",
    "\n",
    "df_TCGA.loc[df_TCGA['Consequence'] == 'frameshift_variant', 'HGVSp_Short'] = df_TCGA.loc[df_TCGA['Consequence'] == 'frameshift_variant', 'HGVSp_Short'].str.split('*').str[0]\n",
    "df_TCGA = df_TCGA[df_TCGA['Consequence'].isin(['missense_variant', 'synonymous_variant', 'stop_gained', 'frameshift_variant','inframe_deletion','inframe_insertion'])]\n",
    "df_TCGA = df_TCGA.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "vr7G8T8ttQRF",
    "outputId": "5d0377b5-7db6-415d-ca94-995a99b9a1f3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 938/938 [13:13<00:00,  1.18it/s]\n"
     ]
    }
   ],
   "source": [
    "lst = train.drop(columns=['ID','SUBCLASS']).columns\n",
    "train_col_list = list(lst)\n",
    "TCGA = df_TCGA[df_TCGA['Hugo_Symbol'].isin(train_col_list)]\n",
    "tcga_data = TCGA[['Tumor_Sample_Barcode','Hugo_Symbol','HGVSp_Short']].dropna()\n",
    "\n",
    "tcga_data['ID'] = tcga_data['Tumor_Sample_Barcode']\n",
    "tcga_data['HGVSp_Short'] = tcga_data['HGVSp_Short'].str[2:]\n",
    "tcga_data['SUBCLASS'] = 'THCA'\n",
    "tcga_data = tcga_data.drop(columns=['Tumor_Sample_Barcode'])\n",
    "\n",
    "ID_unique = tcga_data['ID'].unique()\n",
    "\n",
    "for idx in tqdm(ID_unique):\n",
    "    data = tcga_data[tcga_data['ID']==idx].groupby('Hugo_Symbol')['HGVSp_Short'].agg(lambda x: ' '.join(x)).reset_index()\n",
    "    data['HGVSp_Short'] = data['HGVSp_Short'].apply(lambda x: ' '.join(sorted(set(x.split()), key=x.split().index)))\n",
    "\n",
    "    new_row = pd.Series(['WT'] * train.shape[1], index=train.columns)\n",
    "    new_row['ID'] = 'ID'\n",
    "    new_row['SUBCLASS'] = 'THCA'\n",
    "\n",
    "        # Hugo_Symbol과 base_series의 인덱스 이름이 같으면 WT 값을 HGVSp_Short로 변경\n",
    "    for index, row in data.iterrows():\n",
    "        hugo_symbol = row['Hugo_Symbol']\n",
    "        mutation = row['HGVSp_Short']\n",
    "\n",
    "        # new_row의 인덱스 중 Hugo_Symbol과 일치하는 항목을 찾고 'WT'를 변이 값으로 교체\n",
    "        if hugo_symbol in new_row.index:\n",
    "            new_row[hugo_symbol] = mutation\n",
    "\n",
    "    train = pd.concat([train, pd.DataFrame(new_row).T], axis=0)\n",
    "\n",
    "train = train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eqUG_h9EtQRF"
   },
   "source": [
    "# additional data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKP2Mh5HtQRF"
   },
   "source": [
    "# THYM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "MIVNDzW2tQRF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParserError encountered with skiprows=0, retrying with skiprows=2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "\n",
    "# KIRC 폴더에 있는 모든 .tar.gz 파일을 처리\n",
    "folder_path = 'additional_data_2/THYM'\n",
    "gz_files = [f for f in os.listdir(folder_path) if f.endswith('.tar.gz')]\n",
    "\n",
    "# 데이터를 저장할 리스트\n",
    "df_list = []\n",
    "\n",
    "# DataFrame을 로드하는 함수 (skiprows 조정 기능 포함)\n",
    "def load_dataframe(file, sep='\\t'):\n",
    "    skiprows = 0  # 처음에는 skiprows=0으로 시도\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n",
    "            # 첫 번째 열 이름이 '#version'으로 시작하는지 확인\n",
    "            if df.columns[0].startswith('#'):\n",
    "                print(f\"'#version' detected with skiprows={skiprows}, retrying with skiprows={skiprows+1}\")\n",
    "                skiprows += 1  # skiprows 값을 1 증가시키고 다시 시도\n",
    "                file.seek(0)  # 파일 포인터를 다시 시작점으로 이동\n",
    "            else:\n",
    "                break  # '#version'이 없으면 루프 탈출\n",
    "        except pd.errors.ParserError:\n",
    "            if skiprows == 0:\n",
    "                print(f\"ParserError encountered with skiprows=0, retrying with skiprows=2\")\n",
    "                skiprows = 2  # skiprows를 2로 설정하고 다시 시도\n",
    "                file.seek(0)  # 파일 포인터를 다시 시작점으로 이동\n",
    "            else:\n",
    "                print(f\"ParserError encountered again with skiprows={skiprows}. Skipping file.\")\n",
    "                return None  # 에러가 발생하면 None 반환\n",
    "    return df\n",
    "\n",
    "# 각 tar.gz 파일을 순차적으로 처리\n",
    "for gz_file in gz_files:\n",
    "    file_path = os.path.join(folder_path, gz_file)\n",
    "    \n",
    "    with tarfile.open(file_path, 'r:gz') as tar:\n",
    "        # 압축 파일 내부의 파일 목록을 확인\n",
    "        file_names = tar.getnames()\n",
    "        \n",
    "        # 우리가 필요한 파일 경로를 찾아서 추출\n",
    "        for file_name in file_names:\n",
    "            if 'data_mutations.txt' in file_name:  # 필요한 파일의 경로를 찾음\n",
    "                file = tar.extractfile(file_name)\n",
    "                \n",
    "                # DataFrame을 로드하는 함수 호출\n",
    "                df = load_dataframe(file, sep='\\t')\n",
    "                \n",
    "                # df가 None이 아닌 경우에만 계속 처리\n",
    "                if df is not None:\n",
    "                    # Amino_Acid_Change 열이 있는지 확인하고, 있으면 이름 변경\n",
    "                    \n",
    "                    if 'Amino_Acid_Change' in df.columns and 'HGVSp_Short' in df.columns:\n",
    "                        df.drop(columns=['Amino_Acid_Change'], inplace=True)\n",
    "                        \n",
    "                    if 'Amino_Acid_Change' in df.columns:\n",
    "                        df.rename(columns={'Amino_Acid_Change': 'HGVSp_Short'}, inplace=True)\n",
    "                    \n",
    "                    # 필요한 열만 선택\n",
    "                    df = df[['Tumor_Sample_Barcode', 'Hugo_Symbol', 'Consequence', 'HGVSp_Short']]\n",
    "                    \n",
    "                    # df_list에 추가\n",
    "                    df_list.append(df)\n",
    "                break  # 필요한 파일만 처리하므로 루프 탈출\n",
    "\n",
    "                \n",
    "df_TCGA = pd.concat(df_list, ignore_index=True)\n",
    "###################################\n",
    "# .maf.gz 파일을 불러오기\n",
    "file_path_1 = 'additional_data/THYM.gz'\n",
    "\n",
    "# 파일을 pandas의 DataFrame으로 읽기\n",
    "with gzip.open(file_path_1, 'rt') as f:\n",
    "    df_TCGA_2 = pd.read_csv(f, sep='\\t', comment='#')  # 주석(#) 무시하고 탭으로 구분\n",
    "    \n",
    "df_TCGA_2= df_TCGA_2[['Tumor_Sample_Barcode','Hugo_Symbol','Consequence','HGVSp_Short']]\n",
    "df_TCGA = pd.concat([df_TCGA,df_TCGA_2])\n",
    "#####################################\n",
    "df_TCGA = df_TCGA.dropna(subset='HGVSp_Short')\n",
    "# 'frameshift_variant'이고, 'HGVSp_Short' 열에 'fs'가 없는 값들을 'fs'를 추가하여 수정합니다.\n",
    "df_TCGA.loc[(df_TCGA['Consequence'] == 'frameshift_variant') & ~df_TCGA['HGVSp_Short'].str.contains('fs'), 'HGVSp_Short'] = \\\n",
    "    df_TCGA.loc[(df_TCGA['Consequence'] == 'frameshift_variant') & ~df_TCGA['HGVSp_Short'].str.contains('fs'), 'HGVSp_Short'].str.replace(r'(\\*)$', r'fs\\1', regex=True)\n",
    "\n",
    "# 'Variant_Classification'이 \"Silent\"인 행의 'HGVSp_Short' 열 값을 수정합니다.\n",
    "df_TCGA.loc[df_TCGA['Consequence'] == \"synonymous_variant\", 'HGVSp_Short'] = \\\n",
    "    df_TCGA.loc[df_TCGA['Consequence'] == \"synonymous_variant\", 'HGVSp_Short'].str.replace(r'(\\w)(\\d+)=', r'\\1\\2\\1', regex=True)\n",
    "\n",
    "df_TCGA.loc[df_TCGA['Consequence'] == 'frameshift_variant', 'HGVSp_Short'] = df_TCGA.loc[df_TCGA['Consequence'] == 'frameshift_variant', 'HGVSp_Short'].str.split('*').str[0]\n",
    "df_TCGA = df_TCGA[df_TCGA['Consequence'].isin(['missense_variant', 'synonymous_variant', 'stop_gained', 'frameshift_variant','inframe_deletion','inframe_insertion'])]\n",
    "df_TCGA = df_TCGA.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "3QAqDUXbtQRF",
    "outputId": "01a6acb4-3214-482a-bd2c-7dcdfdebdce4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 323/323 [04:35<00:00,  1.17it/s]\n"
     ]
    }
   ],
   "source": [
    "lst = train.drop(columns=['ID','SUBCLASS']).columns\n",
    "train_col_list = list(lst)\n",
    "TCGA = df_TCGA[df_TCGA['Hugo_Symbol'].isin(train_col_list)]\n",
    "tcga_data = TCGA[['Tumor_Sample_Barcode','Hugo_Symbol','HGVSp_Short']].dropna()\n",
    "\n",
    "tcga_data['ID'] = tcga_data['Tumor_Sample_Barcode']\n",
    "tcga_data['HGVSp_Short'] = tcga_data['HGVSp_Short'].str[2:]\n",
    "tcga_data['SUBCLASS'] = 'THYM'\n",
    "tcga_data = tcga_data.drop(columns=['Tumor_Sample_Barcode'])\n",
    "\n",
    "ID_unique = tcga_data['ID'].unique()\n",
    "\n",
    "for idx in tqdm(ID_unique):\n",
    "    data = tcga_data[tcga_data['ID']==idx].groupby('Hugo_Symbol')['HGVSp_Short'].agg(lambda x: ' '.join(x)).reset_index()\n",
    "    data['HGVSp_Short'] = data['HGVSp_Short'].apply(lambda x: ' '.join(sorted(set(x.split()), key=x.split().index)))\n",
    "\n",
    "    new_row = pd.Series(['WT'] * train.shape[1], index=train.columns)\n",
    "    new_row['ID'] = 'ID'\n",
    "    new_row['SUBCLASS'] = 'THYM'\n",
    "\n",
    "        # Hugo_Symbol과 base_series의 인덱스 이름이 같으면 WT 값을 HGVSp_Short로 변경\n",
    "    for index, row in data.iterrows():\n",
    "        hugo_symbol = row['Hugo_Symbol']\n",
    "        mutation = row['HGVSp_Short']\n",
    "\n",
    "        # new_row의 인덱스 중 Hugo_Symbol과 일치하는 항목을 찾고 'WT'를 변이 값으로 교체\n",
    "        if hugo_symbol in new_row.index:\n",
    "            new_row[hugo_symbol] = mutation\n",
    "\n",
    "    train = pd.concat([train, pd.DataFrame(new_row).T], axis=0)\n",
    "\n",
    "train = train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-cIR0PhtQRF"
   },
   "source": [
    "# additional data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4jy9XvTqtQRF"
   },
   "source": [
    "# UCEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "BR6e9-SjtQRF",
    "outputId": "cb9bc2bd-0a37-4723-ed39-c39d9a844bb2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\2142324066.py:18: DtypeWarning: Columns (4,38,39,43) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n",
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\2142324066.py:18: DtypeWarning: Columns (1,4,5,6,20,21,22,23,29,30,33,34,35,36,42,44,45,46,48,53,54,55,57,58,63,67,68,69,70,72,76,80,87,94,95,99,101,102,104,109,112,116,117,121,123,124,125) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'#version' detected with skiprows=0, retrying with skiprows=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\2142324066.py:18: DtypeWarning: Columns (125) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n",
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\2142324066.py:77: DtypeWarning: Columns (74,114,120) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_TCGA_2 = pd.read_csv(f, sep='\\t', comment='#')  # 주석(#) 무시하고 탭으로 구분\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "\n",
    "# KIRC 폴더에 있는 모든 .tar.gz 파일을 처리\n",
    "folder_path = 'additional_data_2/UCEC'\n",
    "gz_files = [f for f in os.listdir(folder_path) if f.endswith('.tar.gz')]\n",
    "\n",
    "# 데이터를 저장할 리스트\n",
    "df_list = []\n",
    "\n",
    "# DataFrame을 로드하는 함수 (skiprows 조정 기능 포함)\n",
    "def load_dataframe(file, sep='\\t'):\n",
    "    skiprows = 0  # 처음에는 skiprows=0으로 시도\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n",
    "            # 첫 번째 열 이름이 '#version'으로 시작하는지 확인\n",
    "            if df.columns[0].startswith('#'):\n",
    "                print(f\"'#version' detected with skiprows={skiprows}, retrying with skiprows={skiprows+1}\")\n",
    "                skiprows += 1  # skiprows 값을 1 증가시키고 다시 시도\n",
    "                file.seek(0)  # 파일 포인터를 다시 시작점으로 이동\n",
    "            else:\n",
    "                break  # '#version'이 없으면 루프 탈출\n",
    "        except pd.errors.ParserError:\n",
    "            if skiprows == 0:\n",
    "                print(f\"ParserError encountered with skiprows=0, retrying with skiprows=2\")\n",
    "                skiprows = 2  # skiprows를 2로 설정하고 다시 시도\n",
    "                file.seek(0)  # 파일 포인터를 다시 시작점으로 이동\n",
    "            else:\n",
    "                print(f\"ParserError encountered again with skiprows={skiprows}. Skipping file.\")\n",
    "                return None  # 에러가 발생하면 None 반환\n",
    "    return df\n",
    "\n",
    "# 각 tar.gz 파일을 순차적으로 처리\n",
    "for gz_file in gz_files:\n",
    "    file_path = os.path.join(folder_path, gz_file)\n",
    "    \n",
    "    with tarfile.open(file_path, 'r:gz') as tar:\n",
    "        # 압축 파일 내부의 파일 목록을 확인\n",
    "        file_names = tar.getnames()\n",
    "        \n",
    "        # 우리가 필요한 파일 경로를 찾아서 추출\n",
    "        for file_name in file_names:\n",
    "            if 'data_mutations.txt' in file_name:  # 필요한 파일의 경로를 찾음\n",
    "                file = tar.extractfile(file_name)\n",
    "                \n",
    "                # DataFrame을 로드하는 함수 호출\n",
    "                df = load_dataframe(file, sep='\\t')\n",
    "                \n",
    "                # df가 None이 아닌 경우에만 계속 처리\n",
    "                if df is not None:\n",
    "                    # Amino_Acid_Change 열이 있는지 확인하고, 있으면 이름 변경\n",
    "                    \n",
    "                    if 'Amino_Acid_Change' in df.columns and 'HGVSp_Short' in df.columns:\n",
    "                        df.drop(columns=['Amino_Acid_Change'], inplace=True)\n",
    "                        \n",
    "                    if 'Amino_Acid_Change' in df.columns:\n",
    "                        df.rename(columns={'Amino_Acid_Change': 'HGVSp_Short'}, inplace=True)\n",
    "                    \n",
    "                    # 필요한 열만 선택\n",
    "                    df = df[['Tumor_Sample_Barcode', 'Hugo_Symbol', 'Consequence', 'HGVSp_Short']]\n",
    "                    \n",
    "                    # df_list에 추가\n",
    "                    df_list.append(df)\n",
    "                break  # 필요한 파일만 처리하므로 루프 탈출\n",
    "\n",
    "                \n",
    "df_TCGA = pd.concat(df_list, ignore_index=True)\n",
    "###################################\n",
    "# .maf.gz 파일을 불러오기\n",
    "file_path_1 = 'additional_data/UCEC.gz'\n",
    "\n",
    "# 파일을 pandas의 DataFrame으로 읽기\n",
    "with gzip.open(file_path_1, 'rt') as f:\n",
    "    df_TCGA_2 = pd.read_csv(f, sep='\\t', comment='#')  # 주석(#) 무시하고 탭으로 구분\n",
    "    \n",
    "df_TCGA_2= df_TCGA_2[['Tumor_Sample_Barcode','Hugo_Symbol','Consequence','HGVSp_Short']]\n",
    "df_TCGA = pd.concat([df_TCGA,df_TCGA_2])\n",
    "#####################################\n",
    "df_TCGA = df_TCGA.dropna(subset='HGVSp_Short')\n",
    "# 'frameshift_variant'이고, 'HGVSp_Short' 열에 'fs'가 없는 값들을 'fs'를 추가하여 수정합니다.\n",
    "df_TCGA.loc[(df_TCGA['Consequence'] == 'frameshift_variant') & ~df_TCGA['HGVSp_Short'].str.contains('fs'), 'HGVSp_Short'] = \\\n",
    "    df_TCGA.loc[(df_TCGA['Consequence'] == 'frameshift_variant') & ~df_TCGA['HGVSp_Short'].str.contains('fs'), 'HGVSp_Short'].str.replace(r'(\\*)$', r'fs\\1', regex=True)\n",
    "\n",
    "# 'Variant_Classification'이 \"Silent\"인 행의 'HGVSp_Short' 열 값을 수정합니다.\n",
    "df_TCGA.loc[df_TCGA['Consequence'] == \"synonymous_variant\", 'HGVSp_Short'] = \\\n",
    "    df_TCGA.loc[df_TCGA['Consequence'] == \"synonymous_variant\", 'HGVSp_Short'].str.replace(r'(\\w)(\\d+)=', r'\\1\\2\\1', regex=True)\n",
    "\n",
    "df_TCGA.loc[df_TCGA['Consequence'] == 'frameshift_variant', 'HGVSp_Short'] = df_TCGA.loc[df_TCGA['Consequence'] == 'frameshift_variant', 'HGVSp_Short'].str.split('*').str[0]\n",
    "df_TCGA = df_TCGA[df_TCGA['Consequence'].isin(['missense_variant', 'synonymous_variant', 'stop_gained', 'frameshift_variant','inframe_deletion','inframe_insertion'])]\n",
    "df_TCGA = df_TCGA.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "f9lXgU0HtQRF",
    "outputId": "4f3b8ef6-cd4d-4b4a-8394-ba59a8e1ec3c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1036/1036 [15:46<00:00,  1.09it/s]\n"
     ]
    }
   ],
   "source": [
    "lst = train.drop(columns=['ID','SUBCLASS']).columns\n",
    "train_col_list = list(lst)\n",
    "TCGA = df_TCGA[df_TCGA['Hugo_Symbol'].isin(train_col_list)]\n",
    "tcga_data = TCGA[['Tumor_Sample_Barcode','Hugo_Symbol','HGVSp_Short']].dropna()\n",
    "\n",
    "tcga_data['ID'] = tcga_data['Tumor_Sample_Barcode']\n",
    "tcga_data['HGVSp_Short'] = tcga_data['HGVSp_Short'].str[2:]\n",
    "tcga_data['SUBCLASS'] = 'UCEC'\n",
    "tcga_data = tcga_data.drop(columns=['Tumor_Sample_Barcode'])\n",
    "\n",
    "ID_unique = tcga_data['ID'].unique()\n",
    "\n",
    "for idx in tqdm(ID_unique):\n",
    "    data = tcga_data[tcga_data['ID']==idx].groupby('Hugo_Symbol')['HGVSp_Short'].agg(lambda x: ' '.join(x)).reset_index()\n",
    "    data['HGVSp_Short'] = data['HGVSp_Short'].apply(lambda x: ' '.join(sorted(set(x.split()), key=x.split().index)))\n",
    "\n",
    "    new_row = pd.Series(['WT'] * train.shape[1], index=train.columns)\n",
    "    new_row['ID'] = 'ID'\n",
    "    new_row['SUBCLASS'] = 'UCEC'\n",
    "\n",
    "        # Hugo_Symbol과 base_series의 인덱스 이름이 같으면 WT 값을 HGVSp_Short로 변경\n",
    "    for index, row in data.iterrows():\n",
    "        hugo_symbol = row['Hugo_Symbol']\n",
    "        mutation = row['HGVSp_Short']\n",
    "\n",
    "        # new_row의 인덱스 중 Hugo_Symbol과 일치하는 항목을 찾고 'WT'를 변이 값으로 교체\n",
    "        if hugo_symbol in new_row.index:\n",
    "            new_row[hugo_symbol] = mutation\n",
    "\n",
    "    train = pd.concat([train, pd.DataFrame(new_row).T], axis=0)\n",
    "\n",
    "train = train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>SUBCLASS</th>\n",
       "      <th>A2M</th>\n",
       "      <th>AAAS</th>\n",
       "      <th>AADAT</th>\n",
       "      <th>AARS1</th>\n",
       "      <th>ABAT</th>\n",
       "      <th>ABCA1</th>\n",
       "      <th>ABCA2</th>\n",
       "      <th>ABCA3</th>\n",
       "      <th>...</th>\n",
       "      <th>ZNF292</th>\n",
       "      <th>ZNF365</th>\n",
       "      <th>ZNF639</th>\n",
       "      <th>ZNF707</th>\n",
       "      <th>ZNFX1</th>\n",
       "      <th>ZNRF4</th>\n",
       "      <th>ZPBP</th>\n",
       "      <th>ZW10</th>\n",
       "      <th>ZWINT</th>\n",
       "      <th>ZYX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>KIPAN</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>SARC</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0002</td>\n",
       "      <td>SKCM</td>\n",
       "      <td>R895R</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0003</td>\n",
       "      <td>KIRC</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>GBMLGG</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28126</th>\n",
       "      <td>ID</td>\n",
       "      <td>UCEC</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28127</th>\n",
       "      <td>ID</td>\n",
       "      <td>UCEC</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>H233H</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>Q220*</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28128</th>\n",
       "      <td>ID</td>\n",
       "      <td>UCEC</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28129</th>\n",
       "      <td>ID</td>\n",
       "      <td>UCEC</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28130</th>\n",
       "      <td>ID</td>\n",
       "      <td>UCEC</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28131 rows × 4386 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               ID SUBCLASS    A2M AAAS AADAT AARS1 ABAT ABCA1 ABCA2 ABCA3  \\\n",
       "0      TRAIN_0000    KIPAN     WT   WT    WT    WT   WT    WT    WT    WT   \n",
       "1      TRAIN_0001     SARC     WT   WT    WT    WT   WT    WT    WT    WT   \n",
       "2      TRAIN_0002     SKCM  R895R   WT    WT    WT   WT    WT    WT    WT   \n",
       "3      TRAIN_0003     KIRC     WT   WT    WT    WT   WT    WT    WT    WT   \n",
       "4      TRAIN_0004   GBMLGG     WT   WT    WT    WT   WT    WT    WT    WT   \n",
       "...           ...      ...    ...  ...   ...   ...  ...   ...   ...   ...   \n",
       "28126          ID     UCEC     WT   WT    WT    WT   WT    WT    WT    WT   \n",
       "28127          ID     UCEC     WT   WT    WT    WT   WT    WT    WT    WT   \n",
       "28128          ID     UCEC     WT   WT    WT    WT   WT    WT    WT    WT   \n",
       "28129          ID     UCEC     WT   WT    WT    WT   WT    WT    WT    WT   \n",
       "28130          ID     UCEC     WT   WT    WT    WT   WT    WT    WT    WT   \n",
       "\n",
       "       ... ZNF292 ZNF365 ZNF639 ZNF707 ZNFX1  ZNRF4 ZPBP ZW10  ZWINT ZYX  \n",
       "0      ...     WT     WT     WT     WT    WT     WT   WT   WT     WT  WT  \n",
       "1      ...     WT     WT     WT     WT    WT     WT   WT   WT     WT  WT  \n",
       "2      ...     WT     WT     WT     WT    WT     WT   WT   WT     WT  WT  \n",
       "3      ...     WT     WT     WT     WT    WT     WT   WT   WT     WT  WT  \n",
       "4      ...     WT     WT     WT     WT    WT     WT   WT   WT     WT  WT  \n",
       "...    ...    ...    ...    ...    ...   ...    ...  ...  ...    ...  ..  \n",
       "28126  ...     WT     WT     WT     WT    WT     WT   WT   WT     WT  WT  \n",
       "28127  ...     WT     WT     WT     WT    WT  H233H   WT   WT  Q220*  WT  \n",
       "28128  ...     WT     WT     WT     WT    WT     WT   WT   WT     WT  WT  \n",
       "28129  ...     WT     WT     WT     WT    WT     WT   WT   WT     WT  WT  \n",
       "28130  ...     WT     WT     WT     WT    WT     WT   WT   WT     WT  WT  \n",
       "\n",
       "[28131 rows x 4386 columns]"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E1Q5y20RtQRF"
   },
   "source": [
    "# KIPAN, STES 찾아봐야됌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8dBMQX8MtQRF"
   },
   "source": [
    "# additional data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lBKMvjWptQRF"
   },
   "source": [
    "# KICH(KIPAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "1Z4vL741tQRF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'#version' detected with skiprows=0, retrying with skiprows=1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "\n",
    "# KIRC 폴더에 있는 모든 .tar.gz 파일을 처리\n",
    "folder_path = 'additional_data_2/KICH'\n",
    "gz_files = [f for f in os.listdir(folder_path) if f.endswith('.tar.gz')]\n",
    "\n",
    "# 데이터를 저장할 리스트\n",
    "df_list = []\n",
    "\n",
    "# DataFrame을 로드하는 함수 (skiprows 조정 기능 포함)\n",
    "def load_dataframe(file, sep='\\t'):\n",
    "    skiprows = 0  # 처음에는 skiprows=0으로 시도\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n",
    "            # 첫 번째 열 이름이 '#version'으로 시작하는지 확인\n",
    "            if df.columns[0].startswith('#'):\n",
    "                print(f\"'#version' detected with skiprows={skiprows}, retrying with skiprows={skiprows+1}\")\n",
    "                skiprows += 1  # skiprows 값을 1 증가시키고 다시 시도\n",
    "                file.seek(0)  # 파일 포인터를 다시 시작점으로 이동\n",
    "            else:\n",
    "                break  # '#version'이 없으면 루프 탈출\n",
    "        except pd.errors.ParserError:\n",
    "            if skiprows == 0:\n",
    "                print(f\"ParserError encountered with skiprows=0, retrying with skiprows=2\")\n",
    "                skiprows = 2  # skiprows를 2로 설정하고 다시 시도\n",
    "                file.seek(0)  # 파일 포인터를 다시 시작점으로 이동\n",
    "            else:\n",
    "                print(f\"ParserError encountered again with skiprows={skiprows}. Skipping file.\")\n",
    "                return None  # 에러가 발생하면 None 반환\n",
    "    return df\n",
    "\n",
    "# 각 tar.gz 파일을 순차적으로 처리\n",
    "for gz_file in gz_files:\n",
    "    file_path = os.path.join(folder_path, gz_file)\n",
    "    \n",
    "    with tarfile.open(file_path, 'r:gz') as tar:\n",
    "        # 압축 파일 내부의 파일 목록을 확인\n",
    "        file_names = tar.getnames()\n",
    "        \n",
    "        # 우리가 필요한 파일 경로를 찾아서 추출\n",
    "        for file_name in file_names:\n",
    "            if 'data_mutations.txt' in file_name:  # 필요한 파일의 경로를 찾음\n",
    "                file = tar.extractfile(file_name)\n",
    "                \n",
    "                # DataFrame을 로드하는 함수 호출\n",
    "                df = load_dataframe(file, sep='\\t')\n",
    "                \n",
    "                # df가 None이 아닌 경우에만 계속 처리\n",
    "                if df is not None:\n",
    "                    # Amino_Acid_Change 열이 있는지 확인하고, 있으면 이름 변경\n",
    "                    \n",
    "                    if 'Amino_Acid_Change' in df.columns and 'HGVSp_Short' in df.columns:\n",
    "                        df.drop(columns=['Amino_Acid_Change'], inplace=True)\n",
    "                        \n",
    "                    if 'Amino_Acid_Change' in df.columns:\n",
    "                        df.rename(columns={'Amino_Acid_Change': 'HGVSp_Short'}, inplace=True)\n",
    "                    \n",
    "                    # 필요한 열만 선택\n",
    "                    df = df[['Tumor_Sample_Barcode', 'Hugo_Symbol', 'Consequence', 'HGVSp_Short']]\n",
    "                    \n",
    "                    # df_list에 추가\n",
    "                    df_list.append(df)\n",
    "                break  # 필요한 파일만 처리하므로 루프 탈출\n",
    "\n",
    "                \n",
    "df_TCGA = pd.concat(df_list, ignore_index=True)\n",
    "###################################\n",
    "# .maf.gz 파일을 불러오기\n",
    "file_path_1 = 'additional_data/KICH.gz'\n",
    "\n",
    "# 파일을 pandas의 DataFrame으로 읽기\n",
    "with gzip.open(file_path_1, 'rt') as f:\n",
    "    df_TCGA_2 = pd.read_csv(f, sep='\\t', comment='#')  # 주석(#) 무시하고 탭으로 구분\n",
    "    \n",
    "df_TCGA_2= df_TCGA_2[['Tumor_Sample_Barcode','Hugo_Symbol','Consequence','HGVSp_Short']]\n",
    "df_TCGA = pd.concat([df_TCGA,df_TCGA_2])\n",
    "#####################################\n",
    "df_TCGA = df_TCGA.dropna(subset='HGVSp_Short')\n",
    "# 'frameshift_variant'이고, 'HGVSp_Short' 열에 'fs'가 없는 값들을 'fs'를 추가하여 수정합니다.\n",
    "df_TCGA.loc[(df_TCGA['Consequence'] == 'frameshift_variant') & ~df_TCGA['HGVSp_Short'].str.contains('fs'), 'HGVSp_Short'] = \\\n",
    "    df_TCGA.loc[(df_TCGA['Consequence'] == 'frameshift_variant') & ~df_TCGA['HGVSp_Short'].str.contains('fs'), 'HGVSp_Short'].str.replace(r'(\\*)$', r'fs\\1', regex=True)\n",
    "\n",
    "# 'Variant_Classification'이 \"Silent\"인 행의 'HGVSp_Short' 열 값을 수정합니다.\n",
    "df_TCGA.loc[df_TCGA['Consequence'] == \"synonymous_variant\", 'HGVSp_Short'] = \\\n",
    "    df_TCGA.loc[df_TCGA['Consequence'] == \"synonymous_variant\", 'HGVSp_Short'].str.replace(r'(\\w)(\\d+)=', r'\\1\\2\\1', regex=True)\n",
    "\n",
    "df_TCGA.loc[df_TCGA['Consequence'] == 'frameshift_variant', 'HGVSp_Short'] = df_TCGA.loc[df_TCGA['Consequence'] == 'frameshift_variant', 'HGVSp_Short'].str.split('*').str[0]\n",
    "df_TCGA = df_TCGA[df_TCGA['Consequence'].isin(['missense_variant', 'synonymous_variant', 'stop_gained', 'frameshift_variant','inframe_deletion','inframe_insertion'])]\n",
    "df_TCGA = df_TCGA.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "W3g2h0hgtQRF",
    "outputId": "38ae6d94-cd3a-4590-d06c-65bbb575f835",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 131/131 [01:57<00:00,  1.12it/s]\n"
     ]
    }
   ],
   "source": [
    "lst = train.drop(columns=['ID','SUBCLASS']).columns\n",
    "train_col_list = list(lst)\n",
    "TCGA = df_TCGA[df_TCGA['Hugo_Symbol'].isin(train_col_list)]\n",
    "tcga_data = TCGA[['Tumor_Sample_Barcode','Hugo_Symbol','HGVSp_Short']].dropna()\n",
    "\n",
    "tcga_data['ID'] = tcga_data['Tumor_Sample_Barcode']\n",
    "tcga_data['HGVSp_Short'] = tcga_data['HGVSp_Short'].str[2:]\n",
    "tcga_data['SUBCLASS'] = 'KICH'\n",
    "tcga_data = tcga_data.drop(columns=['Tumor_Sample_Barcode'])\n",
    "\n",
    "ID_unique = tcga_data['ID'].unique()\n",
    "\n",
    "for idx in tqdm(ID_unique):\n",
    "    data = tcga_data[tcga_data['ID']==idx].groupby('Hugo_Symbol')['HGVSp_Short'].agg(lambda x: ' '.join(x)).reset_index()\n",
    "    data['HGVSp_Short'] = data['HGVSp_Short'].apply(lambda x: ' '.join(sorted(set(x.split()), key=x.split().index)))\n",
    "\n",
    "    new_row = pd.Series(['WT'] * train.shape[1], index=train.columns)\n",
    "    new_row['ID'] = 'ID'\n",
    "    new_row['SUBCLASS'] = 'KICH'\n",
    "\n",
    "        # Hugo_Symbol과 base_series의 인덱스 이름이 같으면 WT 값을 HGVSp_Short로 변경\n",
    "    for index, row in data.iterrows():\n",
    "        hugo_symbol = row['Hugo_Symbol']\n",
    "        mutation = row['HGVSp_Short']\n",
    "\n",
    "        # new_row의 인덱스 중 Hugo_Symbol과 일치하는 항목을 찾고 'WT'를 변이 값으로 교체\n",
    "        if hugo_symbol in new_row.index:\n",
    "            new_row[hugo_symbol] = mutation\n",
    "\n",
    "    train = pd.concat([train, pd.DataFrame(new_row).T], axis=0)\n",
    "\n",
    "train = train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqo2A18ztQRF"
   },
   "source": [
    "# additional data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dW7wYvkutQRF"
   },
   "source": [
    "# KIRP(KIPAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "6_NAF6KdtQRF",
    "outputId": "a9fdaad0-936b-43f3-bb4e-aaf5f6c9886d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\4184964100.py:18: DtypeWarning: Columns (38,39) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n",
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\4184964100.py:77: DtypeWarning: Columns (120) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_TCGA_2 = pd.read_csv(f, sep='\\t', comment='#')  # 주석(#) 무시하고 탭으로 구분\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "\n",
    "# KIRC 폴더에 있는 모든 .tar.gz 파일을 처리\n",
    "folder_path = 'additional_data_2/KIRP'\n",
    "gz_files = [f for f in os.listdir(folder_path) if f.endswith('.tar.gz')]\n",
    "\n",
    "# 데이터를 저장할 리스트\n",
    "df_list = []\n",
    "\n",
    "# DataFrame을 로드하는 함수 (skiprows 조정 기능 포함)\n",
    "def load_dataframe(file, sep='\\t'):\n",
    "    skiprows = 0  # 처음에는 skiprows=0으로 시도\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n",
    "            # 첫 번째 열 이름이 '#version'으로 시작하는지 확인\n",
    "            if df.columns[0].startswith('#'):\n",
    "                print(f\"'#version' detected with skiprows={skiprows}, retrying with skiprows={skiprows+1}\")\n",
    "                skiprows += 1  # skiprows 값을 1 증가시키고 다시 시도\n",
    "                file.seek(0)  # 파일 포인터를 다시 시작점으로 이동\n",
    "            else:\n",
    "                break  # '#version'이 없으면 루프 탈출\n",
    "        except pd.errors.ParserError:\n",
    "            if skiprows == 0:\n",
    "                print(f\"ParserError encountered with skiprows=0, retrying with skiprows=2\")\n",
    "                skiprows = 2  # skiprows를 2로 설정하고 다시 시도\n",
    "                file.seek(0)  # 파일 포인터를 다시 시작점으로 이동\n",
    "            else:\n",
    "                print(f\"ParserError encountered again with skiprows={skiprows}. Skipping file.\")\n",
    "                return None  # 에러가 발생하면 None 반환\n",
    "    return df\n",
    "\n",
    "# 각 tar.gz 파일을 순차적으로 처리\n",
    "for gz_file in gz_files:\n",
    "    file_path = os.path.join(folder_path, gz_file)\n",
    "    \n",
    "    with tarfile.open(file_path, 'r:gz') as tar:\n",
    "        # 압축 파일 내부의 파일 목록을 확인\n",
    "        file_names = tar.getnames()\n",
    "        \n",
    "        # 우리가 필요한 파일 경로를 찾아서 추출\n",
    "        for file_name in file_names:\n",
    "            if 'data_mutations.txt' in file_name:  # 필요한 파일의 경로를 찾음\n",
    "                file = tar.extractfile(file_name)\n",
    "                \n",
    "                # DataFrame을 로드하는 함수 호출\n",
    "                df = load_dataframe(file, sep='\\t')\n",
    "                \n",
    "                # df가 None이 아닌 경우에만 계속 처리\n",
    "                if df is not None:\n",
    "                    # Amino_Acid_Change 열이 있는지 확인하고, 있으면 이름 변경\n",
    "                    \n",
    "                    if 'Amino_Acid_Change' in df.columns and 'HGVSp_Short' in df.columns:\n",
    "                        df.drop(columns=['Amino_Acid_Change'], inplace=True)\n",
    "                        \n",
    "                    if 'Amino_Acid_Change' in df.columns:\n",
    "                        df.rename(columns={'Amino_Acid_Change': 'HGVSp_Short'}, inplace=True)\n",
    "                    \n",
    "                    # 필요한 열만 선택\n",
    "                    df = df[['Tumor_Sample_Barcode', 'Hugo_Symbol', 'Consequence', 'HGVSp_Short']]\n",
    "                    \n",
    "                    # df_list에 추가\n",
    "                    df_list.append(df)\n",
    "                break  # 필요한 파일만 처리하므로 루프 탈출\n",
    "\n",
    "                \n",
    "df_TCGA = pd.concat(df_list, ignore_index=True)\n",
    "###################################\n",
    "# .maf.gz 파일을 불러오기\n",
    "file_path_1 = 'additional_data/KIRP.gz'\n",
    "\n",
    "# 파일을 pandas의 DataFrame으로 읽기\n",
    "with gzip.open(file_path_1, 'rt') as f:\n",
    "    df_TCGA_2 = pd.read_csv(f, sep='\\t', comment='#')  # 주석(#) 무시하고 탭으로 구분\n",
    "    \n",
    "df_TCGA_2= df_TCGA_2[['Tumor_Sample_Barcode','Hugo_Symbol','Consequence','HGVSp_Short']]\n",
    "df_TCGA = pd.concat([df_TCGA,df_TCGA_2])\n",
    "#####################################\n",
    "df_TCGA = df_TCGA.dropna(subset='HGVSp_Short')\n",
    "# 'frameshift_variant'이고, 'HGVSp_Short' 열에 'fs'가 없는 값들을 'fs'를 추가하여 수정합니다.\n",
    "df_TCGA.loc[(df_TCGA['Consequence'] == 'frameshift_variant') & ~df_TCGA['HGVSp_Short'].str.contains('fs'), 'HGVSp_Short'] = \\\n",
    "    df_TCGA.loc[(df_TCGA['Consequence'] == 'frameshift_variant') & ~df_TCGA['HGVSp_Short'].str.contains('fs'), 'HGVSp_Short'].str.replace(r'(\\*)$', r'fs\\1', regex=True)\n",
    "\n",
    "# 'Variant_Classification'이 \"Silent\"인 행의 'HGVSp_Short' 열 값을 수정합니다.\n",
    "df_TCGA.loc[df_TCGA['Consequence'] == \"synonymous_variant\", 'HGVSp_Short'] = \\\n",
    "    df_TCGA.loc[df_TCGA['Consequence'] == \"synonymous_variant\", 'HGVSp_Short'].str.replace(r'(\\w)(\\d+)=', r'\\1\\2\\1', regex=True)\n",
    "\n",
    "df_TCGA.loc[df_TCGA['Consequence'] == 'frameshift_variant', 'HGVSp_Short'] = df_TCGA.loc[df_TCGA['Consequence'] == 'frameshift_variant', 'HGVSp_Short'].str.split('*').str[0]\n",
    "df_TCGA = df_TCGA[df_TCGA['Consequence'].isin(['missense_variant', 'synonymous_variant', 'stop_gained', 'frameshift_variant','inframe_deletion','inframe_insertion'])]\n",
    "df_TCGA = df_TCGA.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "id": "0Er4rWfutQRF",
    "outputId": "548e1e18-8298-4b4a-c0ea-abc6ad5a368f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 564/564 [08:29<00:00,  1.11it/s]\n"
     ]
    }
   ],
   "source": [
    "lst = train.drop(columns=['ID','SUBCLASS']).columns\n",
    "train_col_list = list(lst)\n",
    "TCGA = df_TCGA[df_TCGA['Hugo_Symbol'].isin(train_col_list)]\n",
    "tcga_data = TCGA[['Tumor_Sample_Barcode','Hugo_Symbol','HGVSp_Short']].dropna()\n",
    "\n",
    "tcga_data['ID'] = tcga_data['Tumor_Sample_Barcode']\n",
    "tcga_data['HGVSp_Short'] = tcga_data['HGVSp_Short'].str[2:]\n",
    "tcga_data['SUBCLASS'] = 'KIRP'\n",
    "tcga_data = tcga_data.drop(columns=['Tumor_Sample_Barcode'])\n",
    "\n",
    "ID_unique = tcga_data['ID'].unique()\n",
    "\n",
    "for idx in tqdm(ID_unique):\n",
    "    data = tcga_data[tcga_data['ID']==idx].groupby('Hugo_Symbol')['HGVSp_Short'].agg(lambda x: ' '.join(x)).reset_index()\n",
    "    data['HGVSp_Short'] = data['HGVSp_Short'].apply(lambda x: ' '.join(sorted(set(x.split()), key=x.split().index)))\n",
    "\n",
    "    new_row = pd.Series(['WT'] * train.shape[1], index=train.columns)\n",
    "    new_row['ID'] = 'ID'\n",
    "    new_row['SUBCLASS'] = 'KIRP'\n",
    "\n",
    "        # Hugo_Symbol과 base_series의 인덱스 이름이 같으면 WT 값을 HGVSp_Short로 변경\n",
    "    for index, row in data.iterrows():\n",
    "        hugo_symbol = row['Hugo_Symbol']\n",
    "        mutation = row['HGVSp_Short']\n",
    "\n",
    "        # new_row의 인덱스 중 Hugo_Symbol과 일치하는 항목을 찾고 'WT'를 변이 값으로 교체\n",
    "        if hugo_symbol in new_row.index:\n",
    "            new_row[hugo_symbol] = mutation\n",
    "\n",
    "    train = pd.concat([train, pd.DataFrame(new_row).T], axis=0)\n",
    "\n",
    "train = train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KrDli4igtQRG"
   },
   "source": [
    "# additional data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R4P-1GE3tQRG"
   },
   "source": [
    "# ESCA(STES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "e-Iffj71tQRG",
    "outputId": "bc7cbeb6-de08-4170-fd63-28983c4b0456"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "\n",
    "# KIRC 폴더에 있는 모든 .tar.gz 파일을 처리\n",
    "folder_path = 'additional_data_2/ESCA'\n",
    "gz_files = [f for f in os.listdir(folder_path) if f.endswith('.tar.gz')]\n",
    "\n",
    "# 데이터를 저장할 리스트\n",
    "df_list = []\n",
    "\n",
    "# DataFrame을 로드하는 함수 (skiprows 조정 기능 포함)\n",
    "def load_dataframe(file, sep='\\t'):\n",
    "    skiprows = 0  # 처음에는 skiprows=0으로 시도\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n",
    "            # 첫 번째 열 이름이 '#version'으로 시작하는지 확인\n",
    "            if df.columns[0].startswith('#'):\n",
    "                print(f\"'#version' detected with skiprows={skiprows}, retrying with skiprows={skiprows+1}\")\n",
    "                skiprows += 1  # skiprows 값을 1 증가시키고 다시 시도\n",
    "                file.seek(0)  # 파일 포인터를 다시 시작점으로 이동\n",
    "            else:\n",
    "                break  # '#version'이 없으면 루프 탈출\n",
    "        except pd.errors.ParserError:\n",
    "            if skiprows == 0:\n",
    "                print(f\"ParserError encountered with skiprows=0, retrying with skiprows=2\")\n",
    "                skiprows = 2  # skiprows를 2로 설정하고 다시 시도\n",
    "                file.seek(0)  # 파일 포인터를 다시 시작점으로 이동\n",
    "            else:\n",
    "                print(f\"ParserError encountered again with skiprows={skiprows}. Skipping file.\")\n",
    "                return None  # 에러가 발생하면 None 반환\n",
    "    return df\n",
    "\n",
    "# 각 tar.gz 파일을 순차적으로 처리\n",
    "for gz_file in gz_files:\n",
    "    file_path = os.path.join(folder_path, gz_file)\n",
    "    \n",
    "    with tarfile.open(file_path, 'r:gz') as tar:\n",
    "        # 압축 파일 내부의 파일 목록을 확인\n",
    "        file_names = tar.getnames()\n",
    "        \n",
    "        # 우리가 필요한 파일 경로를 찾아서 추출\n",
    "        for file_name in file_names:\n",
    "            if 'data_mutations.txt' in file_name:  # 필요한 파일의 경로를 찾음\n",
    "                file = tar.extractfile(file_name)\n",
    "                \n",
    "                # DataFrame을 로드하는 함수 호출\n",
    "                df = load_dataframe(file, sep='\\t')\n",
    "                \n",
    "                # df가 None이 아닌 경우에만 계속 처리\n",
    "                if df is not None:\n",
    "                    # Amino_Acid_Change 열이 있는지 확인하고, 있으면 이름 변경\n",
    "                    \n",
    "                    if 'Amino_Acid_Change' in df.columns and 'HGVSp_Short' in df.columns:\n",
    "                        df.drop(columns=['Amino_Acid_Change'], inplace=True)\n",
    "                        \n",
    "                    if 'Amino_Acid_Change' in df.columns:\n",
    "                        df.rename(columns={'Amino_Acid_Change': 'HGVSp_Short'}, inplace=True)\n",
    "                    \n",
    "                    # 필요한 열만 선택\n",
    "                    df = df[['Tumor_Sample_Barcode', 'Hugo_Symbol', 'Consequence', 'HGVSp_Short']]\n",
    "                    \n",
    "                    # df_list에 추가\n",
    "                    df_list.append(df)\n",
    "                break  # 필요한 파일만 처리하므로 루프 탈출\n",
    "\n",
    "                \n",
    "df_TCGA = pd.concat(df_list, ignore_index=True)\n",
    "###################################\n",
    "# .maf.gz 파일을 불러오기\n",
    "file_path_1 = 'additional_data/ESCA.gz'\n",
    "\n",
    "# 파일을 pandas의 DataFrame으로 읽기\n",
    "with gzip.open(file_path_1, 'rt') as f:\n",
    "    df_TCGA_2 = pd.read_csv(f, sep='\\t', comment='#')  # 주석(#) 무시하고 탭으로 구분\n",
    "    \n",
    "df_TCGA_2= df_TCGA_2[['Tumor_Sample_Barcode','Hugo_Symbol','Consequence','HGVSp_Short']]\n",
    "df_TCGA = pd.concat([df_TCGA,df_TCGA_2])\n",
    "#####################################\n",
    "df_TCGA = df_TCGA.dropna(subset='HGVSp_Short')\n",
    "# 'frameshift_variant'이고, 'HGVSp_Short' 열에 'fs'가 없는 값들을 'fs'를 추가하여 수정합니다.\n",
    "df_TCGA.loc[(df_TCGA['Consequence'] == 'frameshift_variant') & ~df_TCGA['HGVSp_Short'].str.contains('fs'), 'HGVSp_Short'] = \\\n",
    "    df_TCGA.loc[(df_TCGA['Consequence'] == 'frameshift_variant') & ~df_TCGA['HGVSp_Short'].str.contains('fs'), 'HGVSp_Short'].str.replace(r'(\\*)$', r'fs\\1', regex=True)\n",
    "\n",
    "# 'Variant_Classification'이 \"Silent\"인 행의 'HGVSp_Short' 열 값을 수정합니다.\n",
    "df_TCGA.loc[df_TCGA['Consequence'] == \"synonymous_variant\", 'HGVSp_Short'] = \\\n",
    "    df_TCGA.loc[df_TCGA['Consequence'] == \"synonymous_variant\", 'HGVSp_Short'].str.replace(r'(\\w)(\\d+)=', r'\\1\\2\\1', regex=True)\n",
    "\n",
    "df_TCGA.loc[df_TCGA['Consequence'] == 'frameshift_variant', 'HGVSp_Short'] = df_TCGA.loc[df_TCGA['Consequence'] == 'frameshift_variant', 'HGVSp_Short'].str.split('*').str[0]\n",
    "df_TCGA = df_TCGA[df_TCGA['Consequence'].isin(['missense_variant', 'synonymous_variant', 'stop_gained', 'frameshift_variant','inframe_deletion','inframe_insertion'])]\n",
    "df_TCGA = df_TCGA.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "TNl7kvVvtQRG",
    "outputId": "7e1a763d-48dd-4c2a-e20c-12bd1af3aa6d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 714/714 [11:03<00:00,  1.08it/s]\n"
     ]
    }
   ],
   "source": [
    "lst = train.drop(columns=['ID','SUBCLASS']).columns\n",
    "train_col_list = list(lst)\n",
    "TCGA = df_TCGA[df_TCGA['Hugo_Symbol'].isin(train_col_list)]\n",
    "tcga_data = TCGA[['Tumor_Sample_Barcode','Hugo_Symbol','HGVSp_Short']].dropna()\n",
    "\n",
    "tcga_data['ID'] = tcga_data['Tumor_Sample_Barcode']\n",
    "tcga_data['HGVSp_Short'] = tcga_data['HGVSp_Short'].str[2:]\n",
    "tcga_data['SUBCLASS'] = 'ESCA'\n",
    "tcga_data = tcga_data.drop(columns=['Tumor_Sample_Barcode'])\n",
    "\n",
    "ID_unique = tcga_data['ID'].unique()\n",
    "\n",
    "for idx in tqdm(ID_unique):\n",
    "    data = tcga_data[tcga_data['ID']==idx].groupby('Hugo_Symbol')['HGVSp_Short'].agg(lambda x: ' '.join(x)).reset_index()\n",
    "    data['HGVSp_Short'] = data['HGVSp_Short'].apply(lambda x: ' '.join(sorted(set(x.split()), key=x.split().index)))\n",
    "\n",
    "    new_row = pd.Series(['WT'] * train.shape[1], index=train.columns)\n",
    "    new_row['ID'] = 'ID'\n",
    "    new_row['SUBCLASS'] = 'ESCA'\n",
    "\n",
    "        # Hugo_Symbol과 base_series의 인덱스 이름이 같으면 WT 값을 HGVSp_Short로 변경\n",
    "    for index, row in data.iterrows():\n",
    "        hugo_symbol = row['Hugo_Symbol']\n",
    "        mutation = row['HGVSp_Short']\n",
    "\n",
    "        # new_row의 인덱스 중 Hugo_Symbol과 일치하는 항목을 찾고 'WT'를 변이 값으로 교체\n",
    "        if hugo_symbol in new_row.index:\n",
    "            new_row[hugo_symbol] = mutation\n",
    "\n",
    "    train = pd.concat([train, pd.DataFrame(new_row).T], axis=0)\n",
    "\n",
    "train = train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-nyiizuEtQRG"
   },
   "source": [
    "# additional data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lMM-u2LGtQRG"
   },
   "source": [
    "# STAD(STES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "cvgo46iqtQRG",
    "outputId": "2623d701-1235-4327-ead0-dbe8173e0a84"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\1723642408.py:18: DtypeWarning: Columns (56,61,65) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParserError encountered with skiprows=0, retrying with skiprows=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\1723642408.py:18: DtypeWarning: Columns (38,39) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n",
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_31332\\1723642408.py:18: DtypeWarning: Columns (51,81,89) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "\n",
    "# KIRC 폴더에 있는 모든 .tar.gz 파일을 처리\n",
    "folder_path = 'additional_data_2/STAD'\n",
    "gz_files = [f for f in os.listdir(folder_path) if f.endswith('.tar.gz')]\n",
    "\n",
    "# 데이터를 저장할 리스트\n",
    "df_list = []\n",
    "\n",
    "# DataFrame을 로드하는 함수 (skiprows 조정 기능 포함)\n",
    "def load_dataframe(file, sep='\\t'):\n",
    "    skiprows = 0  # 처음에는 skiprows=0으로 시도\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            df = pd.read_csv(file, sep=sep, skiprows=skiprows)\n",
    "            # 첫 번째 열 이름이 '#version'으로 시작하는지 확인\n",
    "            if df.columns[0].startswith('#'):\n",
    "                print(f\"'#version' detected with skiprows={skiprows}, retrying with skiprows={skiprows+1}\")\n",
    "                skiprows += 1  # skiprows 값을 1 증가시키고 다시 시도\n",
    "                file.seek(0)  # 파일 포인터를 다시 시작점으로 이동\n",
    "            else:\n",
    "                break  # '#version'이 없으면 루프 탈출\n",
    "        except pd.errors.ParserError:\n",
    "            if skiprows == 0:\n",
    "                print(f\"ParserError encountered with skiprows=0, retrying with skiprows=2\")\n",
    "                skiprows = 2  # skiprows를 2로 설정하고 다시 시도\n",
    "                file.seek(0)  # 파일 포인터를 다시 시작점으로 이동\n",
    "            else:\n",
    "                print(f\"ParserError encountered again with skiprows={skiprows}. Skipping file.\")\n",
    "                return None  # 에러가 발생하면 None 반환\n",
    "    return df\n",
    "\n",
    "# 각 tar.gz 파일을 순차적으로 처리\n",
    "for gz_file in gz_files:\n",
    "    file_path = os.path.join(folder_path, gz_file)\n",
    "    \n",
    "    with tarfile.open(file_path, 'r:gz') as tar:\n",
    "        # 압축 파일 내부의 파일 목록을 확인\n",
    "        file_names = tar.getnames()\n",
    "        \n",
    "        # 우리가 필요한 파일 경로를 찾아서 추출\n",
    "        for file_name in file_names:\n",
    "            if 'data_mutations.txt' in file_name:  # 필요한 파일의 경로를 찾음\n",
    "                file = tar.extractfile(file_name)\n",
    "                \n",
    "                # DataFrame을 로드하는 함수 호출\n",
    "                df = load_dataframe(file, sep='\\t')\n",
    "                \n",
    "                # df가 None이 아닌 경우에만 계속 처리\n",
    "                if df is not None:\n",
    "                    # Amino_Acid_Change 열이 있는지 확인하고, 있으면 이름 변경\n",
    "                    \n",
    "                    if 'Amino_Acid_Change' in df.columns and 'HGVSp_Short' in df.columns:\n",
    "                        df.drop(columns=['Amino_Acid_Change'], inplace=True)\n",
    "                        \n",
    "                    if 'Amino_Acid_Change' in df.columns:\n",
    "                        df.rename(columns={'Amino_Acid_Change': 'HGVSp_Short'}, inplace=True)\n",
    "                    \n",
    "                    # 필요한 열만 선택\n",
    "                    df = df[['Tumor_Sample_Barcode', 'Hugo_Symbol', 'Consequence', 'HGVSp_Short']]\n",
    "                    \n",
    "                    # df_list에 추가\n",
    "                    df_list.append(df)\n",
    "                break  # 필요한 파일만 처리하므로 루프 탈출\n",
    "\n",
    "                \n",
    "df_TCGA = pd.concat(df_list, ignore_index=True)\n",
    "###################################\n",
    "# .maf.gz 파일을 불러오기\n",
    "file_path_1 = 'additional_data/STAD.gz'\n",
    "\n",
    "# 파일을 pandas의 DataFrame으로 읽기\n",
    "with gzip.open(file_path_1, 'rt') as f:\n",
    "    df_TCGA_2 = pd.read_csv(f, sep='\\t', comment='#')  # 주석(#) 무시하고 탭으로 구분\n",
    "    \n",
    "df_TCGA_2= df_TCGA_2[['Tumor_Sample_Barcode','Hugo_Symbol','Consequence','HGVSp_Short']]\n",
    "df_TCGA = pd.concat([df_TCGA,df_TCGA_2])\n",
    "#####################################\n",
    "df_TCGA = df_TCGA.dropna(subset='HGVSp_Short')\n",
    "# 'frameshift_variant'이고, 'HGVSp_Short' 열에 'fs'가 없는 값들을 'fs'를 추가하여 수정합니다.\n",
    "df_TCGA.loc[(df_TCGA['Consequence'] == 'frameshift_variant') & ~df_TCGA['HGVSp_Short'].str.contains('fs'), 'HGVSp_Short'] = \\\n",
    "    df_TCGA.loc[(df_TCGA['Consequence'] == 'frameshift_variant') & ~df_TCGA['HGVSp_Short'].str.contains('fs'), 'HGVSp_Short'].str.replace(r'(\\*)$', r'fs\\1', regex=True)\n",
    "\n",
    "# 'Variant_Classification'이 \"Silent\"인 행의 'HGVSp_Short' 열 값을 수정합니다.\n",
    "df_TCGA.loc[df_TCGA['Consequence'] == \"synonymous_variant\", 'HGVSp_Short'] = \\\n",
    "    df_TCGA.loc[df_TCGA['Consequence'] == \"synonymous_variant\", 'HGVSp_Short'].str.replace(r'(\\w)(\\d+)=', r'\\1\\2\\1', regex=True)\n",
    "\n",
    "df_TCGA.loc[df_TCGA['Consequence'] == 'frameshift_variant', 'HGVSp_Short'] = df_TCGA.loc[df_TCGA['Consequence'] == 'frameshift_variant', 'HGVSp_Short'].str.split('*').str[0]\n",
    "df_TCGA = df_TCGA[df_TCGA['Consequence'].isin(['missense_variant', 'synonymous_variant', 'stop_gained', 'frameshift_variant','inframe_deletion','inframe_insertion'])]\n",
    "df_TCGA = df_TCGA.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "id": "Ug9o2VFftQRG",
    "outputId": "30a79b84-d792-4a26-83a4-a627014cd143",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1444/1444 [23:23<00:00,  1.03it/s]\n"
     ]
    }
   ],
   "source": [
    "lst = train.drop(columns=['ID','SUBCLASS']).columns\n",
    "train_col_list = list(lst)\n",
    "TCGA = df_TCGA[df_TCGA['Hugo_Symbol'].isin(train_col_list)]\n",
    "tcga_data = TCGA[['Tumor_Sample_Barcode','Hugo_Symbol','HGVSp_Short']].dropna()\n",
    "\n",
    "\n",
    "\n",
    "tcga_data['ID'] = tcga_data['Tumor_Sample_Barcode']\n",
    "tcga_data['HGVSp_Short'] = tcga_data['HGVSp_Short'].str[2:]\n",
    "tcga_data['SUBCLASS'] = 'STAD'\n",
    "tcga_data = tcga_data.drop(columns=['Tumor_Sample_Barcode'])\n",
    "\n",
    "ID_unique = tcga_data['ID'].unique()\n",
    "\n",
    "for idx in tqdm(ID_unique):\n",
    "    data = tcga_data[tcga_data['ID']==idx].groupby('Hugo_Symbol')['HGVSp_Short'].agg(lambda x: ' '.join(x)).reset_index()\n",
    "    data['HGVSp_Short'] = data['HGVSp_Short'].apply(lambda x: ' '.join(sorted(set(x.split()), key=x.split().index)))\n",
    "\n",
    "    new_row = pd.Series(['WT'] * train.shape[1], index=train.columns)\n",
    "    new_row['ID'] = 'ID'\n",
    "    new_row['SUBCLASS'] = 'STAD'\n",
    "\n",
    "        # Hugo_Symbol과 base_series의 인덱스 이름이 같으면 WT 값을 HGVSp_Short로 변경\n",
    "    for index, row in data.iterrows():\n",
    "        hugo_symbol = row['Hugo_Symbol']\n",
    "        mutation = row['HGVSp_Short']\n",
    "\n",
    "        # new_row의 인덱스 중 Hugo_Symbol과 일치하는 항목을 찾고 'WT'를 변이 값으로 교체\n",
    "        if hugo_symbol in new_row.index:\n",
    "            new_row[hugo_symbol] = mutation\n",
    "\n",
    "    train = pd.concat([train, pd.DataFrame(new_row).T], axis=0)\n",
    "\n",
    "train = train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>SUBCLASS</th>\n",
       "      <th>A2M</th>\n",
       "      <th>AAAS</th>\n",
       "      <th>AADAT</th>\n",
       "      <th>AARS1</th>\n",
       "      <th>ABAT</th>\n",
       "      <th>ABCA1</th>\n",
       "      <th>ABCA2</th>\n",
       "      <th>ABCA3</th>\n",
       "      <th>...</th>\n",
       "      <th>ZNF292</th>\n",
       "      <th>ZNF365</th>\n",
       "      <th>ZNF639</th>\n",
       "      <th>ZNF707</th>\n",
       "      <th>ZNFX1</th>\n",
       "      <th>ZNRF4</th>\n",
       "      <th>ZPBP</th>\n",
       "      <th>ZW10</th>\n",
       "      <th>ZWINT</th>\n",
       "      <th>ZYX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>KIPAN</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>SARC</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0002</td>\n",
       "      <td>SKCM</td>\n",
       "      <td>R895R</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0003</td>\n",
       "      <td>KIRC</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>GBMLGG</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39963</th>\n",
       "      <td>ID</td>\n",
       "      <td>STAD</td>\n",
       "      <td>M713I</td>\n",
       "      <td>Q456Sfs</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>R220R A304T</td>\n",
       "      <td>L2012P</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39964</th>\n",
       "      <td>ID</td>\n",
       "      <td>STAD</td>\n",
       "      <td>C48R</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>R1305W</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39965</th>\n",
       "      <td>ID</td>\n",
       "      <td>STAD</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>K2231K</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>S354L</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39966</th>\n",
       "      <td>ID</td>\n",
       "      <td>STAD</td>\n",
       "      <td>I822I F229L</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>E2204D S1024N</td>\n",
       "      <td>WT</td>\n",
       "      <td>N553N L276F</td>\n",
       "      <td>...</td>\n",
       "      <td>R1397* R2115*</td>\n",
       "      <td>WT</td>\n",
       "      <td>K51T K174T</td>\n",
       "      <td>WT</td>\n",
       "      <td>L465I</td>\n",
       "      <td>WT</td>\n",
       "      <td>H335Y E275D</td>\n",
       "      <td>V450G</td>\n",
       "      <td>WT</td>\n",
       "      <td>V524A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39967</th>\n",
       "      <td>ID</td>\n",
       "      <td>STAD</td>\n",
       "      <td>L1086L R1031*</td>\n",
       "      <td>WT</td>\n",
       "      <td>S32L</td>\n",
       "      <td>V135A</td>\n",
       "      <td>WT</td>\n",
       "      <td>D1910V Q1543*</td>\n",
       "      <td>A1502V A1377V</td>\n",
       "      <td>D1676D A1669V</td>\n",
       "      <td>...</td>\n",
       "      <td>G225C S1501I G1611D</td>\n",
       "      <td>K399Sfs</td>\n",
       "      <td>WT</td>\n",
       "      <td>V57I</td>\n",
       "      <td>I1657fs</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39968 rows × 4386 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               ID SUBCLASS            A2M     AAAS AADAT  AARS1         ABAT  \\\n",
       "0      TRAIN_0000    KIPAN             WT       WT    WT     WT           WT   \n",
       "1      TRAIN_0001     SARC             WT       WT    WT     WT           WT   \n",
       "2      TRAIN_0002     SKCM          R895R       WT    WT     WT           WT   \n",
       "3      TRAIN_0003     KIRC             WT       WT    WT     WT           WT   \n",
       "4      TRAIN_0004   GBMLGG             WT       WT    WT     WT           WT   \n",
       "...           ...      ...            ...      ...   ...    ...          ...   \n",
       "39963          ID     STAD          M713I  Q456Sfs    WT     WT  R220R A304T   \n",
       "39964          ID     STAD           C48R       WT    WT     WT           WT   \n",
       "39965          ID     STAD             WT       WT    WT     WT           WT   \n",
       "39966          ID     STAD    I822I F229L       WT    WT     WT           WT   \n",
       "39967          ID     STAD  L1086L R1031*       WT  S32L  V135A           WT   \n",
       "\n",
       "               ABCA1          ABCA2          ABCA3  ...               ZNF292  \\\n",
       "0                 WT             WT             WT  ...                   WT   \n",
       "1                 WT             WT             WT  ...                   WT   \n",
       "2                 WT             WT             WT  ...                   WT   \n",
       "3                 WT             WT             WT  ...                   WT   \n",
       "4                 WT             WT             WT  ...                   WT   \n",
       "...              ...            ...            ...  ...                  ...   \n",
       "39963         L2012P             WT             WT  ...                   WT   \n",
       "39964             WT             WT         R1305W  ...                   WT   \n",
       "39965         K2231K             WT             WT  ...                   WT   \n",
       "39966  E2204D S1024N             WT    N553N L276F  ...        R1397* R2115*   \n",
       "39967  D1910V Q1543*  A1502V A1377V  D1676D A1669V  ...  G225C S1501I G1611D   \n",
       "\n",
       "        ZNF365      ZNF639 ZNF707    ZNFX1  ZNRF4         ZPBP   ZW10 ZWINT  \\\n",
       "0           WT          WT     WT       WT     WT           WT     WT    WT   \n",
       "1           WT          WT     WT       WT     WT           WT     WT    WT   \n",
       "2           WT          WT     WT       WT     WT           WT     WT    WT   \n",
       "3           WT          WT     WT       WT     WT           WT     WT    WT   \n",
       "4           WT          WT     WT       WT     WT           WT     WT    WT   \n",
       "...        ...         ...    ...      ...    ...          ...    ...   ...   \n",
       "39963       WT          WT     WT       WT     WT           WT     WT    WT   \n",
       "39964       WT          WT     WT       WT     WT           WT     WT    WT   \n",
       "39965       WT          WT     WT       WT  S354L           WT     WT    WT   \n",
       "39966       WT  K51T K174T     WT    L465I     WT  H335Y E275D  V450G    WT   \n",
       "39967  K399Sfs          WT   V57I  I1657fs     WT           WT     WT    WT   \n",
       "\n",
       "         ZYX  \n",
       "0         WT  \n",
       "1         WT  \n",
       "2         WT  \n",
       "3         WT  \n",
       "4         WT  \n",
       "...      ...  \n",
       "39963     WT  \n",
       "39964     WT  \n",
       "39965     WT  \n",
       "39966  V524A  \n",
       "39967     WT  \n",
       "\n",
       "[39968 rows x 4386 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.to_csv('1019_final_tr_1.csv', encoding='UTF-8-sig', index=False)\n",
    "# test.to_csv('1019_final_te_1.csv', encoding='UTF-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.fillna('WT')\n",
    "test = test.fillna('WT')\n",
    "\n",
    "train = train.applymap(convert_format)\n",
    "train = train.applymap(convert_deletion_format)\n",
    "\n",
    "test = test.applymap(convert_format)\n",
    "test = test.applymap(convert_deletion_format)\n",
    "\n",
    "train.to_csv('1019_final_tr_2.csv', encoding='UTF-8-sig', index=False)\n",
    "test.to_csv('1019_final_te_2.csv', encoding='UTF-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNTIL HERE"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
